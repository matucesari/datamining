{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Redes neuronales para predicción de COVID Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matucesari/datamining/blob/main/Redes_neuronales_para_predicci%C3%B3n_de_COVID_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAsnohpcxWAP"
      },
      "source": [
        "# **Laboratorio: Predicción de resultado de test para COVID 19 usando redes neuronales**\n",
        "\n",
        "En este proyecto aplicaremos redes neuronales del tipo feed-forward para intentar predecir el resultado de tests de para COVID 19 a partir de diversas variables de entrada, entre ellas una serie de valores típicos obtenidos a partir de estudios clínicos, sintomatología general y resultado de tests te otras enfermedades típicas. \n",
        "\n",
        "Entre otras cosas aprenderemos :\n",
        "* Como construir y entrenar una red neuronal multi-capa sencilla para realizar clasificación binaria\n",
        "* Como trabajar con un problema desbalanceado\n",
        "* Evaluar el desempeño de las redes en función de su profundidad para decidir cual es la mejor alternativa\n",
        "* Aplicar el mejor modelo obtenido sobre nuevos casos para obtener predicciones\n",
        "\n",
        "\n",
        "## Pre-requisitos\n",
        "* Tener una cuenta de Google (gmail)\n",
        "* Tener instalado el navegador Google Chrome\n",
        "* Contar con conectividad a Internet\n",
        "* Conocimientos básicos de programación con el lenguaje _python_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztuMvawM1T6C"
      },
      "source": [
        "## Entrega y uso del laboratorio\n",
        "\n",
        "### Uso\n",
        "\n",
        "* Antes de cualquier cosa, **cree una copia de este cuaderno:** \n",
        "  * `Click` en **File**\n",
        "  * luego **Save a Copy in Drive**\n",
        "  * Renombre el archivo con el siguiente formato: `APELLIDO_NOMBRE_LRNA.ipynb` \n",
        "* Use el _notebook_, complete las actividades y consignas que se requieran \n",
        "* Este laboratorio es una actividad *individual* pero se fomenta el intercambio de opiniones en clase\n",
        "\n",
        "### Entrega\n",
        "\n",
        "Una vez finalizado el laboratorio, complete [el formulario de entrega](...) indicando:\n",
        " * `link de su notebook`. El mismo se obtiene si realiza click en **Compartir/Share** (esquina superior derecha) y luego en **Obtener link para compartir/Get shareable link**  \n",
        " \n",
        "\n",
        "\n",
        "_______________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DhWQEhfpdWa"
      },
      "source": [
        "En primer lugar importamos todos los módulos que usaremos durante todo el proyecto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3H1yWwAplSB"
      },
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense,Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ysyusMvBdly"
      },
      "source": [
        "# Función para obtener cuenta de ejemplos con valores nulos por cada variable\n",
        "def get_null_counts(df):\n",
        "  null_columns=df.columns[df.isnull().any()]\n",
        "  nan_count_df = pd.DataFrame(data=df[null_columns].isnull().sum(),columns=['NaN count'])\n",
        "  nan_count_df.sort_values('NaN count',ascending=False,inplace=True)\n",
        "  return nan_count_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrleY3JqAE3n"
      },
      "source": [
        "## PARTE I : Pre-procesamiento y análisis inicial del dataset (EDA)\n",
        "\n",
        "\n",
        "Vamos a trabajar con un [dataset de pacientes recolectado en el Hospital Isrealita Albert Einstein en São Paulo, Brazil](https://www.kaggle.com/einsteindata4u/covid19#dataset.xlsx)\n",
        "\n",
        "Este dataset contiene datos anonimizados de los pacientes de los cuales se les realizó el test SARS-CoV-2 RT-PCR junto con otros tests de laboratorio adicionales que se les realizaron en su visita al hospital.\n",
        "\n",
        "Para trabajar más facilmente desde este notebook, este archivo está alojado en _github_ de manera de poder leerlo directamente. Lo primero que hacemos entonces es leerlo en un `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtBq4CR40Lma",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "82116ec4-69fe-43e1-f69c-04f3ae5483f0"
      },
      "source": [
        "url = 'https://github.com/matucesari/datamining/blob/main/brasil_einstein_covid_tests.xlsx?raw=true'\n",
        "raw_data_einstein = pd.read_excel(url)\n",
        "print(raw_data_einstein.shape)\n",
        "raw_data_einstein.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5644, 111)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient ID</th>\n",
              "      <th>Patient age quantile</th>\n",
              "      <th>SARS-Cov-2 exam result</th>\n",
              "      <th>Patient addmited to regular ward (1=yes, 0=no)</th>\n",
              "      <th>Patient addmited to semi-intensive unit (1=yes, 0=no)</th>\n",
              "      <th>Patient addmited to intensive care unit (1=yes, 0=no)</th>\n",
              "      <th>Hematocrit</th>\n",
              "      <th>Hemoglobin</th>\n",
              "      <th>Platelets</th>\n",
              "      <th>Mean platelet volume</th>\n",
              "      <th>Red blood Cells</th>\n",
              "      <th>Lymphocytes</th>\n",
              "      <th>Mean corpuscular hemoglobin concentration (MCHC)</th>\n",
              "      <th>Leukocytes</th>\n",
              "      <th>Basophils</th>\n",
              "      <th>Mean corpuscular hemoglobin (MCH)</th>\n",
              "      <th>Eosinophils</th>\n",
              "      <th>Mean corpuscular volume (MCV)</th>\n",
              "      <th>Monocytes</th>\n",
              "      <th>Red blood cell distribution width (RDW)</th>\n",
              "      <th>Serum Glucose</th>\n",
              "      <th>Respiratory Syncytial Virus</th>\n",
              "      <th>Influenza A</th>\n",
              "      <th>Influenza B</th>\n",
              "      <th>Parainfluenza 1</th>\n",
              "      <th>CoronavirusNL63</th>\n",
              "      <th>Rhinovirus/Enterovirus</th>\n",
              "      <th>Mycoplasma pneumoniae</th>\n",
              "      <th>Coronavirus HKU1</th>\n",
              "      <th>Parainfluenza 3</th>\n",
              "      <th>Chlamydophila pneumoniae</th>\n",
              "      <th>Adenovirus</th>\n",
              "      <th>Parainfluenza 4</th>\n",
              "      <th>Coronavirus229E</th>\n",
              "      <th>CoronavirusOC43</th>\n",
              "      <th>Inf A H1N1 2009</th>\n",
              "      <th>Bordetella pertussis</th>\n",
              "      <th>Metapneumovirus</th>\n",
              "      <th>Parainfluenza 2</th>\n",
              "      <th>Neutrophils</th>\n",
              "      <th>...</th>\n",
              "      <th>Urine - Esterase</th>\n",
              "      <th>Urine - Aspect</th>\n",
              "      <th>Urine - pH</th>\n",
              "      <th>Urine - Hemoglobin</th>\n",
              "      <th>Urine - Bile pigments</th>\n",
              "      <th>Urine - Ketone Bodies</th>\n",
              "      <th>Urine - Nitrite</th>\n",
              "      <th>Urine - Density</th>\n",
              "      <th>Urine - Urobilinogen</th>\n",
              "      <th>Urine - Protein</th>\n",
              "      <th>Urine - Sugar</th>\n",
              "      <th>Urine - Leukocytes</th>\n",
              "      <th>Urine - Crystals</th>\n",
              "      <th>Urine - Red blood cells</th>\n",
              "      <th>Urine - Hyaline cylinders</th>\n",
              "      <th>Urine - Granular cylinders</th>\n",
              "      <th>Urine - Yeasts</th>\n",
              "      <th>Urine - Color</th>\n",
              "      <th>Partial thromboplastin time (PTT)</th>\n",
              "      <th>Relationship (Patient/Normal)</th>\n",
              "      <th>International normalized ratio (INR)</th>\n",
              "      <th>Lactic Dehydrogenase</th>\n",
              "      <th>Prothrombin time (PT), Activity</th>\n",
              "      <th>Vitamin B12</th>\n",
              "      <th>Creatine phosphokinase (CPK)</th>\n",
              "      <th>Ferritin</th>\n",
              "      <th>Arterial Lactic Acid</th>\n",
              "      <th>Lipase dosage</th>\n",
              "      <th>D-Dimer</th>\n",
              "      <th>Albumin</th>\n",
              "      <th>Hb saturation (arterial blood gases)</th>\n",
              "      <th>pCO2 (arterial blood gas analysis)</th>\n",
              "      <th>Base excess (arterial blood gas analysis)</th>\n",
              "      <th>pH (arterial blood gas analysis)</th>\n",
              "      <th>Total CO2 (arterial blood gas analysis)</th>\n",
              "      <th>HCO3 (arterial blood gas analysis)</th>\n",
              "      <th>pO2 (arterial blood gas analysis)</th>\n",
              "      <th>Arteiral Fio2</th>\n",
              "      <th>Phosphor</th>\n",
              "      <th>ctO2 (arterial blood gas analysis)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>44477f75e8169d2</td>\n",
              "      <td>13</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>126e9dd13932f68</td>\n",
              "      <td>17</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.236515</td>\n",
              "      <td>-0.02234</td>\n",
              "      <td>-0.517413</td>\n",
              "      <td>0.010677</td>\n",
              "      <td>0.102004</td>\n",
              "      <td>0.318366</td>\n",
              "      <td>-0.95079</td>\n",
              "      <td>-0.09461</td>\n",
              "      <td>-0.223767</td>\n",
              "      <td>-0.292269</td>\n",
              "      <td>1.482158</td>\n",
              "      <td>0.166192</td>\n",
              "      <td>0.357547</td>\n",
              "      <td>-0.625073</td>\n",
              "      <td>-0.140648</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>detected</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>-0.619086</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a46b4402a0e5696</td>\n",
              "      <td>8</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f7d619a94f97c45</td>\n",
              "      <td>5</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d9e41465789c2b5</td>\n",
              "      <td>15</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>detected</td>\n",
              "      <td>NaN</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>not_detected</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 111 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Patient ID  ...  ctO2 (arterial blood gas analysis)\n",
              "0  44477f75e8169d2  ...                                 NaN\n",
              "1  126e9dd13932f68  ...                                 NaN\n",
              "2  a46b4402a0e5696  ...                                 NaN\n",
              "3  f7d619a94f97c45  ...                                 NaN\n",
              "4  d9e41465789c2b5  ...                                 NaN\n",
              "\n",
              "[5 rows x 111 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGrimrCc1BRA"
      },
      "source": [
        "En su versión más completa y cruda este conjunto de datos tiene ~5600 ejemplos y ~111 columnas. \n",
        "\n",
        "A continuación extraemos solo las variables numéricas que vamos a usar. Dejaremos afuera variables como el ID del paciente y algunas que producen error así como variables que no son numéricas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J28wearEDlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83016afb-a68a-4125-e906-057b166d5f62"
      },
      "source": [
        "\n",
        "# vamos a elminar estas columnas\n",
        "vars_to_drop = [\n",
        "  'Patient ID',\n",
        "  # numerics but failing\n",
        "  'Mean platelet volume', \n",
        "  'Mean corpuscular hemoglobin concentration (MCHC)',\n",
        "  'Gamma-glutamyltransferase',\n",
        "  'Ionized calcium',\n",
        "  'Creatine phosphokinase (CPK)',\n",
        "]\n",
        "\n",
        "# estas variables son numericas\n",
        "numeric_vars = [\n",
        "  'Patient age quantile',\n",
        "  'Hematocrit',\n",
        "  'Hemoglobin',\n",
        "  'Platelets',\n",
        "  'Red blood Cells',\n",
        "  'Lymphocytes',\n",
        "  'Eosinophils',\n",
        "  'Mean corpuscular volume (MCV)',\n",
        "  'Monocytes',\n",
        "  'Red blood cell distribution width (RDW)',\n",
        "  'Serum Glucose',\n",
        "  'Neutrophils',\n",
        "  'Urea',\n",
        "  'Proteina C reativa mg/dL',\n",
        "  'Creatinine',\n",
        "  'Potassium',\n",
        "  'Sodium',\n",
        "  'Alanine transaminase',\n",
        "  'Aspartate transaminase',\n",
        "  'Total Bilirubin',\n",
        "  'Direct Bilirubin',\n",
        "  'Indirect Bilirubin',\n",
        "  'Alkaline phosphatase',\n",
        "  'Magnesium',\n",
        "  'pCO2 (venous blood gas analysis)',\n",
        "  'Hb saturation (venous blood gas analysis)',\n",
        "  'Base excess (venous blood gas analysis)',\n",
        "  'pO2 (venous blood gas analysis)',\n",
        "  'Total CO2 (venous blood gas analysis)',\n",
        "  'pH (venous blood gas analysis)',\n",
        "  'HCO3 (venous blood gas analysis)',\n",
        "  'Rods #',\n",
        "  'Segmented',\n",
        "  'Promyelocytes',\n",
        "  'Metamyelocytes',\n",
        "  'Myelocytes',\n",
        "  'Urine - Density',\n",
        "  'Urine - Red blood cells',\n",
        "  'Relationship (Patient/Normal)',\n",
        "  'International normalized ratio (INR)',\n",
        "  'Lactic Dehydrogenase',\n",
        "  'Ferritin',\n",
        "  'Arterial Lactic Acid',\n",
        "]\n",
        "\n",
        "boolean_vars = [\n",
        "  # estas son booleanas y no las vamos a contemplar aqui\n",
        "  'Patient addmited to regular ward (1=yes, 0=no)',\n",
        "  'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n",
        "  'Patient addmited to intensive care unit (1=yes, 0=no)'\n",
        "]\n",
        "\n",
        "\n",
        "categorical_vars = [\n",
        "  # estas son categoricas y no las vamos a contemplar aqui\n",
        "  # solo el 25% del dataset tiene estas variables\n",
        "  'Respiratory Syncytial Virus',\n",
        "  'Influenza A',\n",
        "  'Influenza B',\n",
        "  'Parainfluenza 1',\n",
        "  'CoronavirusNL63',\n",
        "  'Rhinovirus/Enterovirus',\n",
        "  'Coronavirus HKU1',\n",
        "  'Parainfluenza 3',\n",
        "  'Chlamydophila pneumoniae',\n",
        "  'Adenovirus',\n",
        "  'Parainfluenza 4',\n",
        "  'Coronavirus229E',\n",
        "  'CoronavirusOC43',\n",
        "  'Inf A H1N1 2009',\n",
        "  'Bordetella pertussis',\n",
        "  'Metapneumovirus',\n",
        "  'Parainfluenza 2',\n",
        "  # para estas tenemos solo 15% de los casos con valor\n",
        "  'Influenza B, rapid test',\n",
        "  'Influenza A, rapid test',\n",
        "\n",
        "]\n",
        "\n",
        "# este es el nombre de la columna que contiene los resultados del test\n",
        "target_variable = 'SARS-Cov-2 exam result'\n",
        "\n",
        "# armamos una lista con las columnas que quedaran: el total menos las que queremos eliminar\n",
        "vars_to_work = list(set(raw_data_einstein.columns) - set(vars_to_drop) - set(boolean_vars) - set(categorical_vars))\n",
        "\n",
        "# a partir de ahora trabajaremos con el DataFrame working_data\n",
        "working_data = raw_data_einstein[vars_to_work]\n",
        "\n",
        "print('Tamaño del dataset')\n",
        "print(working_data.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamaño del dataset\n",
            "(5644, 88)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0dg6Si63VZ7"
      },
      "source": [
        "En primera instancia vemos la cantidad de ejemplos con valores nulos en el dataset por columnas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPpZ00CI3cIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3c14ca-19d4-4996-c600-c2d18dfd197a"
      },
      "source": [
        "null_count = get_null_counts(working_data)\n",
        "\n",
        "print('\\nCantidad de filas con Null por columna')\n",
        "print(null_count.to_string())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cantidad de filas con Null por columna\n",
            "                                                  NaN count\n",
            "Partial thromboplastin time (PTT)                      5644\n",
            "Prothrombin time (PT), Activity                        5644\n",
            "Mycoplasma pneumoniae                                  5644\n",
            "Urine - Sugar                                          5644\n",
            "D-Dimer                                                5644\n",
            "Fio2 (venous blood gas analysis)                       5643\n",
            "Urine - Nitrite                                        5643\n",
            "Vitamin B12                                            5641\n",
            "Lipase dosage                                          5636\n",
            "Albumin                                                5631\n",
            "Arteiral Fio2                                          5624\n",
            "Phosphor                                               5624\n",
            "Ferritin                                               5621\n",
            "Hb saturation (arterial blood gases)                   5617\n",
            "HCO3 (arterial blood gas analysis)                     5617\n",
            "ctO2 (arterial blood gas analysis)                     5617\n",
            "pH (arterial blood gas analysis)                       5617\n",
            "Arterial Lactic Acid                                   5617\n",
            "pO2 (arterial blood gas analysis)                      5617\n",
            "pCO2 (arterial blood gas analysis)                     5617\n",
            "Total CO2 (arterial blood gas analysis)                5617\n",
            "Base excess (arterial blood gas analysis)              5617\n",
            "Magnesium                                              5604\n",
            "Ionized calcium                                        5594\n",
            "Urine - Ketone Bodies                                  5587\n",
            "Urine - Protein                                        5584\n",
            "Urine - Esterase                                       5584\n",
            "Urine - Hyaline cylinders                              5577\n",
            "Urine - Urobilinogen                                   5575\n",
            "Urine - Granular cylinders                             5575\n",
            "Urine - Color                                          5574\n",
            "Urine - Bile pigments                                  5574\n",
            "Urine - Hemoglobin                                     5574\n",
            "Urine - Density                                        5574\n",
            "Urine - Yeasts                                         5574\n",
            "Urine - Crystals                                       5574\n",
            "Urine - Aspect                                         5574\n",
            "Urine - Leukocytes                                     5574\n",
            "Urine - pH                                             5574\n",
            "Urine - Red blood cells                                5574\n",
            "Relationship (Patient/Normal)                          5553\n",
            "Metamyelocytes                                         5547\n",
            "Myeloblasts                                            5547\n",
            "Rods #                                                 5547\n",
            "Segmented                                              5547\n",
            "Promyelocytes                                          5547\n",
            "Myelocytes                                             5547\n",
            "Lactic Dehydrogenase                                   5543\n",
            "Creatine phosphokinase (CPK)                           5540\n",
            "International normalized ratio (INR)                   5511\n",
            "HCO3 (venous blood gas analysis)                       5508\n",
            "pO2 (venous blood gas analysis)                        5508\n",
            "Base excess (venous blood gas analysis)                5508\n",
            "Total CO2 (venous blood gas analysis)                  5508\n",
            "Hb saturation (venous blood gas analysis)              5508\n",
            "pCO2 (venous blood gas analysis)                       5508\n",
            "pH (venous blood gas analysis)                         5508\n",
            "Alkaline phosphatase                                   5500\n",
            "Gamma-glutamyltransferase                              5491\n",
            "Total Bilirubin                                        5462\n",
            "Indirect Bilirubin                                     5462\n",
            "Direct Bilirubin                                       5462\n",
            "Serum Glucose                                          5436\n",
            "Alanine transaminase                                   5419\n",
            "Aspartate transaminase                                 5418\n",
            "Strepto A                                              5312\n",
            "Sodium                                                 5274\n",
            "Potassium                                              5273\n",
            "Urea                                                   5247\n",
            "Creatinine                                             5220\n",
            "Proteina C reativa mg/dL                               5138\n",
            "Neutrophils                                            5131\n",
            "Mean platelet volume                                   5045\n",
            "Monocytes                                              5043\n",
            "Leukocytes                                             5042\n",
            "Red blood Cells                                        5042\n",
            "Red blood cell distribution width (RDW)                5042\n",
            "Mean corpuscular hemoglobin (MCH)                      5042\n",
            "Basophils                                              5042\n",
            "Eosinophils                                            5042\n",
            "Platelets                                              5042\n",
            "Mean corpuscular volume (MCV)                          5042\n",
            "Mean corpuscular hemoglobin concentration (MCHC)       5042\n",
            "Lymphocytes                                            5042\n",
            "Hematocrit                                             5041\n",
            "Hemoglobin                                             5041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Okf5MHHNZ6"
      },
      "source": [
        "Para tener un dataset más limipio, también vamos a quitar del analisis aquellas columnas para las cuales todas las filas tienen valores nulos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsWCnKU9AG8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        },
        "outputId": "296a7c7e-2a22-4b44-a339-099d0fd5324d"
      },
      "source": [
        "nrows_limit = working_data.shape[0]\n",
        "\n",
        "cols_empty = null_count.loc[null_count['NaN count'] >= nrows_limit].index.values\n",
        "print('More columns to drop :\\n{}'.format(cols_empty))\n",
        "\n",
        "# armamos una lista con las columnas que quedaran: el total menos las que tienen todas las filas en nulo\n",
        "vars_to_work = list(set(working_data.columns) - set(cols_empty))\n",
        "\n",
        "# obtenemos solo las columnas de interés\n",
        "working_data = working_data[vars_to_work]\n",
        "print(working_data.shape)\n",
        "print(working_data.columns)\n",
        "working_data.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "More columns to drop :\n",
            "['Partial thromboplastin time\\xa0(PTT)\\xa0'\n",
            " 'Prothrombin time (PT), Activity' 'Mycoplasma pneumoniae' 'Urine - Sugar'\n",
            " 'D-Dimer']\n",
            "(5644, 83)\n",
            "Index(['Alanine transaminase', 'Monocytes', 'Ferritin', 'Myelocytes',\n",
            "       'Vitamin B12', 'Sodium', 'Neutrophils', 'Mean platelet volume ',\n",
            "       'Aspartate transaminase', 'Proteina C reativa mg/dL',\n",
            "       'pH (venous blood gas analysis)', 'Urine - Crystals', 'Urine - Protein',\n",
            "       'Alkaline phosphatase', 'Urine - Aspect', 'Urine - pH', 'Hemoglobin',\n",
            "       'Urine - Hyaline cylinders', 'ctO2 (arterial blood gas analysis)',\n",
            "       'Myeloblasts', 'International normalized ratio (INR)',\n",
            "       'Mean corpuscular hemoglobin concentration (MCHC)', 'Serum Glucose',\n",
            "       'Platelets', 'Hematocrit', 'Direct Bilirubin', 'Urine - Urobilinogen',\n",
            "       'Eosinophils', 'Strepto A', 'Hb saturation (arterial blood gases)',\n",
            "       'Urine - Red blood cells', 'Urine - Leukocytes', 'Leukocytes',\n",
            "       'SARS-Cov-2 exam result', 'Base excess (venous blood gas analysis)',\n",
            "       'Phosphor', 'HCO3 (venous blood gas analysis)',\n",
            "       'Gamma-glutamyltransferase ', 'Fio2 (venous blood gas analysis)',\n",
            "       'Potassium', 'Urine - Esterase', 'pCO2 (venous blood gas analysis)',\n",
            "       'Urine - Yeasts', 'Magnesium', 'Urine - Density', 'Creatinine',\n",
            "       'HCO3 (arterial blood gas analysis)', 'Mean corpuscular volume (MCV)',\n",
            "       'Arteiral Fio2', 'Lactic Dehydrogenase', 'Albumin', 'Total Bilirubin',\n",
            "       'pO2 (venous blood gas analysis)', 'Urine - Nitrite', 'Lymphocytes',\n",
            "       'Urine - Ketone Bodies', 'Rods #', 'Mean corpuscular hemoglobin (MCH)',\n",
            "       'Basophils', 'Lipase dosage', 'Arterial Lactic Acid', 'Segmented',\n",
            "       'Total CO2 (arterial blood gas analysis)', 'Urine - Hemoglobin',\n",
            "       'Urine - Color', 'Relationship (Patient/Normal)',\n",
            "       'Urine - Bile pigments', 'Total CO2 (venous blood gas analysis)',\n",
            "       'Urea', 'Indirect Bilirubin',\n",
            "       'Hb saturation (venous blood gas analysis)',\n",
            "       'pO2 (arterial blood gas analysis)', 'Metamyelocytes',\n",
            "       'Creatine phosphokinase (CPK) ',\n",
            "       'Base excess (arterial blood gas analysis)',\n",
            "       'Red blood cell distribution width (RDW)', 'Red blood Cells',\n",
            "       'Promyelocytes', 'pCO2 (arterial blood gas analysis)',\n",
            "       'Ionized calcium ', 'Urine - Granular cylinders',\n",
            "       'pH (arterial blood gas analysis)', 'Patient age quantile'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alanine transaminase</th>\n",
              "      <th>Monocytes</th>\n",
              "      <th>Ferritin</th>\n",
              "      <th>Myelocytes</th>\n",
              "      <th>Vitamin B12</th>\n",
              "      <th>Sodium</th>\n",
              "      <th>Neutrophils</th>\n",
              "      <th>Mean platelet volume</th>\n",
              "      <th>Aspartate transaminase</th>\n",
              "      <th>Proteina C reativa mg/dL</th>\n",
              "      <th>pH (venous blood gas analysis)</th>\n",
              "      <th>Urine - Crystals</th>\n",
              "      <th>Urine - Protein</th>\n",
              "      <th>Alkaline phosphatase</th>\n",
              "      <th>Urine - Aspect</th>\n",
              "      <th>Urine - pH</th>\n",
              "      <th>Hemoglobin</th>\n",
              "      <th>Urine - Hyaline cylinders</th>\n",
              "      <th>ctO2 (arterial blood gas analysis)</th>\n",
              "      <th>Myeloblasts</th>\n",
              "      <th>International normalized ratio (INR)</th>\n",
              "      <th>Mean corpuscular hemoglobin concentration (MCHC)</th>\n",
              "      <th>Serum Glucose</th>\n",
              "      <th>Platelets</th>\n",
              "      <th>Hematocrit</th>\n",
              "      <th>Direct Bilirubin</th>\n",
              "      <th>Urine - Urobilinogen</th>\n",
              "      <th>Eosinophils</th>\n",
              "      <th>Strepto A</th>\n",
              "      <th>Hb saturation (arterial blood gases)</th>\n",
              "      <th>Urine - Red blood cells</th>\n",
              "      <th>Urine - Leukocytes</th>\n",
              "      <th>Leukocytes</th>\n",
              "      <th>SARS-Cov-2 exam result</th>\n",
              "      <th>Base excess (venous blood gas analysis)</th>\n",
              "      <th>Phosphor</th>\n",
              "      <th>HCO3 (venous blood gas analysis)</th>\n",
              "      <th>Gamma-glutamyltransferase</th>\n",
              "      <th>Fio2 (venous blood gas analysis)</th>\n",
              "      <th>Potassium</th>\n",
              "      <th>...</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Urine - Density</th>\n",
              "      <th>Creatinine</th>\n",
              "      <th>HCO3 (arterial blood gas analysis)</th>\n",
              "      <th>Mean corpuscular volume (MCV)</th>\n",
              "      <th>Arteiral Fio2</th>\n",
              "      <th>Lactic Dehydrogenase</th>\n",
              "      <th>Albumin</th>\n",
              "      <th>Total Bilirubin</th>\n",
              "      <th>pO2 (venous blood gas analysis)</th>\n",
              "      <th>Urine - Nitrite</th>\n",
              "      <th>Lymphocytes</th>\n",
              "      <th>Urine - Ketone Bodies</th>\n",
              "      <th>Rods #</th>\n",
              "      <th>Mean corpuscular hemoglobin (MCH)</th>\n",
              "      <th>Basophils</th>\n",
              "      <th>Lipase dosage</th>\n",
              "      <th>Arterial Lactic Acid</th>\n",
              "      <th>Segmented</th>\n",
              "      <th>Total CO2 (arterial blood gas analysis)</th>\n",
              "      <th>Urine - Hemoglobin</th>\n",
              "      <th>Urine - Color</th>\n",
              "      <th>Relationship (Patient/Normal)</th>\n",
              "      <th>Urine - Bile pigments</th>\n",
              "      <th>Total CO2 (venous blood gas analysis)</th>\n",
              "      <th>Urea</th>\n",
              "      <th>Indirect Bilirubin</th>\n",
              "      <th>Hb saturation (venous blood gas analysis)</th>\n",
              "      <th>pO2 (arterial blood gas analysis)</th>\n",
              "      <th>Metamyelocytes</th>\n",
              "      <th>Creatine phosphokinase (CPK)</th>\n",
              "      <th>Base excess (arterial blood gas analysis)</th>\n",
              "      <th>Red blood cell distribution width (RDW)</th>\n",
              "      <th>Red blood Cells</th>\n",
              "      <th>Promyelocytes</th>\n",
              "      <th>pCO2 (arterial blood gas analysis)</th>\n",
              "      <th>Ionized calcium</th>\n",
              "      <th>Urine - Granular cylinders</th>\n",
              "      <th>pH (arterial blood gas analysis)</th>\n",
              "      <th>Patient age quantile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>negative</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.357547</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.862512</td>\n",
              "      <td>-0.619086</td>\n",
              "      <td>0.010677</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.147895</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.02234</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.95079</td>\n",
              "      <td>-0.140648</td>\n",
              "      <td>-0.517413</td>\n",
              "      <td>0.236515</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.482158</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.09461</td>\n",
              "      <td>negative</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.305787</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.089928</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.166192</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.318366</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.292269</td>\n",
              "      <td>-0.223767</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.198059</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.625073</td>\n",
              "      <td>0.102004</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>negative</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 83 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Alanine transaminase  ...  Patient age quantile\n",
              "0                   NaN  ...                    13\n",
              "1                   NaN  ...                    17\n",
              "2                   NaN  ...                     8\n",
              "\n",
              "[3 rows x 83 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgjrHfk64PHU"
      },
      "source": [
        "Para tener una idea del balance de clases del dataset, vemos la cuenta de valores únicos en esta columna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYY7Tp2wGk1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49305cac-8a32-4187-eabf-87b439934f22"
      },
      "source": [
        "# para tener una idea del balance de clases del dataset, vemos la cuenta de valores únicos en esta columna\n",
        "print(working_data[target_variable].value_counts())\n",
        "print(working_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative    5086\n",
            "positive     558\n",
            "Name: SARS-Cov-2 exam result, dtype: int64\n",
            "(5644, 83)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV7f_tFUP-Vs"
      },
      "source": [
        "Como era de esperarse la cantidad de ejempos con resultados `positive` es mucho menor la cantidad de ejemplos negativos, por lo que tendremos que considerar este desbalanceo más adelante para tener resultados más fiables. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkclXp2X1WjA"
      },
      "source": [
        "Además vamos a generar una variable numerica `y` la cual valdrá `1` para los datapoints con resultado de test `positive` y `0` para los `negative`. Esto es necesario por que la salida de la red debe ser numerica como sus entradas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KxfDp020-JS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91932f7d-5162-4880-d03a-c63104482a64"
      },
      "source": [
        "# iniciamos la columna con 0 por defecto\n",
        "working_data['y'] = 0\n",
        "# aquellas filas donde la variable objetivo es 'positive' los seteamos en 1\n",
        "working_data.loc[working_data[target_variable]=='positive',['y']] = 1\n",
        "\n",
        "# imprimimos algunos casos positivos y negativos\n",
        "print(working_data.loc[working_data.y == 1][[target_variable,'y']].sample(3))\n",
        "print(working_data.loc[working_data.y == 0][[target_variable,'y']].sample(3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     SARS-Cov-2 exam result  y\n",
            "5301               positive  1\n",
            "3640               positive  1\n",
            "1479               positive  1\n",
            "     SARS-Cov-2 exam result  y\n",
            "987                negative  0\n",
            "1311               negative  0\n",
            "4223               negative  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NP3EdUqnuWI"
      },
      "source": [
        "Como observamos anteriormente, este dataset está muy desbalanceado ya que hay muchisimas mas instancias de casos con resultado `negativo` que `positivo`. Para evitar problemas en perormance del modelo vamos a tomar una muestra de los casos positivos de tamaño igual a los negativos asi tenemos un dataset balanceado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgf2zw721pkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d30b3e8-cb28-4ca3-d845-cfac4ebdb4dd"
      },
      "source": [
        "# mostramos el count de clases antes de balancear\n",
        "print(working_data['y'].value_counts())\n",
        "\n",
        "# separamos el dataframe en su parte positiva y negativa\n",
        "working_data_pos = working_data.loc[working_data.y == 1]\n",
        "working_data_neg = working_data.loc[working_data.y == 0]\n",
        "\n",
        "# vamos a balancear tomando una muestra de la clase mayoritaria (negativa) del tamaño de la clase minoritaria (positivos)\n",
        "working_data = pd.concat([\n",
        "  working_data_pos,\n",
        "  working_data_neg.sample(working_data_pos.shape[0])\n",
        "])\n",
        "\n",
        "# volvemos a mostrar el count de clases\n",
        "print(working_data['y'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    5086\n",
            "1     558\n",
            "Name: y, dtype: int64\n",
            "1    558\n",
            "0    558\n",
            "Name: y, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHqbdT6kKpxl"
      },
      "source": [
        "Debido a que ahora hemos submuestreado el dataset, volvemos a repetir el proceso de eliminacion de columnas con todas las filas nulas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9dy2sovLCB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "0c7cade6-3b5f-4126-c2bc-7ddf537695da"
      },
      "source": [
        "print(working_data.shape)\n",
        "null_count = get_null_counts(working_data)\n",
        "\n",
        "print('\\nCantidad de filas con Null por columna')\n",
        "null_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1116, 84)\n",
            "\n",
            "Cantidad de filas con Null por columna\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NaN count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Vitamin B12</th>\n",
              "      <td>1116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fio2 (venous blood gas analysis)</th>\n",
              "      <td>1116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Urine - Nitrite</th>\n",
              "      <td>1116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Albumin</th>\n",
              "      <td>1115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Phosphor</th>\n",
              "      <td>1114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Basophils</th>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean corpuscular hemoglobin (MCH)</th>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lymphocytes</th>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Monocytes</th>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Platelets</th>\n",
              "      <td>985</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   NaN count\n",
              "Vitamin B12                             1116\n",
              "Fio2 (venous blood gas analysis)        1116\n",
              "Urine - Nitrite                         1116\n",
              "Albumin                                 1115\n",
              "Phosphor                                1114\n",
              "...                                      ...\n",
              "Basophils                                985\n",
              "Mean corpuscular hemoglobin (MCH)        985\n",
              "Lymphocytes                              985\n",
              "Monocytes                                985\n",
              "Platelets                                985\n",
              "\n",
              "[81 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXs26tdKpxo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "outputId": "b426145e-ce4a-4f0e-afbb-797cb4ae7ea4"
      },
      "source": [
        "nrows_limit = working_data.shape[0]\n",
        "\n",
        "cols_empty = null_count.loc[null_count['NaN count'] >= nrows_limit].index.values\n",
        "print('More columns to drop :\\n{}'.format(cols_empty))\n",
        "\n",
        "# armamos una lista con las columnas que quedaran: el total menos las que tienen todas las filas en nulo\n",
        "vars_to_work = list(set(working_data.columns) - set(cols_empty))\n",
        "\n",
        "# obtenemos solo las columnas de interés\n",
        "working_data = working_data[vars_to_work]\n",
        "print(working_data.shape)\n",
        "print(working_data.columns)\n",
        "working_data.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "More columns to drop :\n",
            "['Vitamin B12' 'Fio2 (venous blood gas analysis)' 'Urine - Nitrite']\n",
            "(1116, 81)\n",
            "Index(['Alanine transaminase', 'Monocytes', 'Ferritin', 'Myelocytes', 'Sodium',\n",
            "       'Neutrophils', 'y', 'Mean platelet volume ', 'Aspartate transaminase',\n",
            "       'Proteina C reativa mg/dL', 'pH (venous blood gas analysis)',\n",
            "       'Urine - Crystals', 'Urine - Protein', 'Alkaline phosphatase',\n",
            "       'Urine - Aspect', 'Urine - pH', 'Hemoglobin',\n",
            "       'Urine - Hyaline cylinders', 'ctO2 (arterial blood gas analysis)',\n",
            "       'Myeloblasts', 'International normalized ratio (INR)',\n",
            "       'Mean corpuscular hemoglobin concentration (MCHC)', 'Serum Glucose',\n",
            "       'Platelets', 'Hematocrit', 'Direct Bilirubin', 'Urine - Urobilinogen',\n",
            "       'Eosinophils', 'Strepto A', 'Hb saturation (arterial blood gases)',\n",
            "       'Urine - Red blood cells', 'Urine - Leukocytes', 'Leukocytes',\n",
            "       'SARS-Cov-2 exam result', 'Base excess (venous blood gas analysis)',\n",
            "       'Phosphor', 'HCO3 (venous blood gas analysis)',\n",
            "       'Gamma-glutamyltransferase ', 'Potassium', 'Urine - Esterase',\n",
            "       'pCO2 (venous blood gas analysis)', 'Urine - Yeasts', 'Magnesium',\n",
            "       'Urine - Density', 'Creatinine', 'HCO3 (arterial blood gas analysis)',\n",
            "       'Mean corpuscular volume (MCV)', 'Arteiral Fio2',\n",
            "       'Lactic Dehydrogenase', 'Albumin', 'Total Bilirubin',\n",
            "       'pO2 (venous blood gas analysis)', 'Lymphocytes',\n",
            "       'Urine - Ketone Bodies', 'Rods #', 'Mean corpuscular hemoglobin (MCH)',\n",
            "       'Basophils', 'Lipase dosage', 'Arterial Lactic Acid', 'Segmented',\n",
            "       'Total CO2 (arterial blood gas analysis)', 'Urine - Hemoglobin',\n",
            "       'Urine - Color', 'Relationship (Patient/Normal)',\n",
            "       'Urine - Bile pigments', 'Total CO2 (venous blood gas analysis)',\n",
            "       'Urea', 'Indirect Bilirubin',\n",
            "       'Hb saturation (venous blood gas analysis)',\n",
            "       'pO2 (arterial blood gas analysis)', 'Metamyelocytes',\n",
            "       'Creatine phosphokinase (CPK) ',\n",
            "       'Base excess (arterial blood gas analysis)',\n",
            "       'Red blood cell distribution width (RDW)', 'Red blood Cells',\n",
            "       'Promyelocytes', 'pCO2 (arterial blood gas analysis)',\n",
            "       'Ionized calcium ', 'Urine - Granular cylinders',\n",
            "       'pH (arterial blood gas analysis)', 'Patient age quantile'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alanine transaminase</th>\n",
              "      <th>Monocytes</th>\n",
              "      <th>Ferritin</th>\n",
              "      <th>Myelocytes</th>\n",
              "      <th>Sodium</th>\n",
              "      <th>Neutrophils</th>\n",
              "      <th>y</th>\n",
              "      <th>Mean platelet volume</th>\n",
              "      <th>Aspartate transaminase</th>\n",
              "      <th>Proteina C reativa mg/dL</th>\n",
              "      <th>pH (venous blood gas analysis)</th>\n",
              "      <th>Urine - Crystals</th>\n",
              "      <th>Urine - Protein</th>\n",
              "      <th>Alkaline phosphatase</th>\n",
              "      <th>Urine - Aspect</th>\n",
              "      <th>Urine - pH</th>\n",
              "      <th>Hemoglobin</th>\n",
              "      <th>Urine - Hyaline cylinders</th>\n",
              "      <th>ctO2 (arterial blood gas analysis)</th>\n",
              "      <th>Myeloblasts</th>\n",
              "      <th>International normalized ratio (INR)</th>\n",
              "      <th>Mean corpuscular hemoglobin concentration (MCHC)</th>\n",
              "      <th>Serum Glucose</th>\n",
              "      <th>Platelets</th>\n",
              "      <th>Hematocrit</th>\n",
              "      <th>Direct Bilirubin</th>\n",
              "      <th>Urine - Urobilinogen</th>\n",
              "      <th>Eosinophils</th>\n",
              "      <th>Strepto A</th>\n",
              "      <th>Hb saturation (arterial blood gases)</th>\n",
              "      <th>Urine - Red blood cells</th>\n",
              "      <th>Urine - Leukocytes</th>\n",
              "      <th>Leukocytes</th>\n",
              "      <th>SARS-Cov-2 exam result</th>\n",
              "      <th>Base excess (venous blood gas analysis)</th>\n",
              "      <th>Phosphor</th>\n",
              "      <th>HCO3 (venous blood gas analysis)</th>\n",
              "      <th>Gamma-glutamyltransferase</th>\n",
              "      <th>Potassium</th>\n",
              "      <th>Urine - Esterase</th>\n",
              "      <th>...</th>\n",
              "      <th>Urine - Yeasts</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Urine - Density</th>\n",
              "      <th>Creatinine</th>\n",
              "      <th>HCO3 (arterial blood gas analysis)</th>\n",
              "      <th>Mean corpuscular volume (MCV)</th>\n",
              "      <th>Arteiral Fio2</th>\n",
              "      <th>Lactic Dehydrogenase</th>\n",
              "      <th>Albumin</th>\n",
              "      <th>Total Bilirubin</th>\n",
              "      <th>pO2 (venous blood gas analysis)</th>\n",
              "      <th>Lymphocytes</th>\n",
              "      <th>Urine - Ketone Bodies</th>\n",
              "      <th>Rods #</th>\n",
              "      <th>Mean corpuscular hemoglobin (MCH)</th>\n",
              "      <th>Basophils</th>\n",
              "      <th>Lipase dosage</th>\n",
              "      <th>Arterial Lactic Acid</th>\n",
              "      <th>Segmented</th>\n",
              "      <th>Total CO2 (arterial blood gas analysis)</th>\n",
              "      <th>Urine - Hemoglobin</th>\n",
              "      <th>Urine - Color</th>\n",
              "      <th>Relationship (Patient/Normal)</th>\n",
              "      <th>Urine - Bile pigments</th>\n",
              "      <th>Total CO2 (venous blood gas analysis)</th>\n",
              "      <th>Urea</th>\n",
              "      <th>Indirect Bilirubin</th>\n",
              "      <th>Hb saturation (venous blood gas analysis)</th>\n",
              "      <th>pO2 (arterial blood gas analysis)</th>\n",
              "      <th>Metamyelocytes</th>\n",
              "      <th>Creatine phosphokinase (CPK)</th>\n",
              "      <th>Base excess (arterial blood gas analysis)</th>\n",
              "      <th>Red blood cell distribution width (RDW)</th>\n",
              "      <th>Red blood Cells</th>\n",
              "      <th>Promyelocytes</th>\n",
              "      <th>pCO2 (arterial blood gas analysis)</th>\n",
              "      <th>Ionized calcium</th>\n",
              "      <th>Urine - Granular cylinders</th>\n",
              "      <th>pH (arterial blood gas analysis)</th>\n",
              "      <th>Patient age quantile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Alanine transaminase  ...  Patient age quantile\n",
              "67                    NaN  ...                     7\n",
              "284                   NaN  ...                    16\n",
              "513                   NaN  ...                    10\n",
              "\n",
              "[3 rows x 81 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI6xRM7dWcrB"
      },
      "source": [
        "## PARTE II : Ingeniería de features y entrenamiento\n",
        "\n",
        "Ahora que tenemos el dataset mejor preparado para entrenar sobre el, vamos a comenzar la etapa de modelado.\n",
        "\n",
        "El primer paso es dividir el  total de datos en 3 conjuntos disjuntos:\n",
        "* `train-set`: será un `80%` del total de ejemplos y será usado para entrenar\n",
        "* `validation-set` : será un `10%` del total y será usado para elegir el mejor putno de corte en el entrenamiento y la mejor arquitectura. \n",
        "* `test-set` : será un `10%` del total de ejemplos y será usado para evaluar el modelo final en datos no antes visto durante el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q8_kVM-WigN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "01579e88-b25e-41df-e022-ecf57c95f270"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df,test_df= train_test_split(working_data,test_size = 0.1)\n",
        "train_df,val_df = train_test_split(train_df,test_size = 0.1)\n",
        "print(train_df.shape)\n",
        "train_df.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(903, 81)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alanine transaminase</th>\n",
              "      <th>Monocytes</th>\n",
              "      <th>Ferritin</th>\n",
              "      <th>Myelocytes</th>\n",
              "      <th>Sodium</th>\n",
              "      <th>Neutrophils</th>\n",
              "      <th>y</th>\n",
              "      <th>Mean platelet volume</th>\n",
              "      <th>Aspartate transaminase</th>\n",
              "      <th>Proteina C reativa mg/dL</th>\n",
              "      <th>pH (venous blood gas analysis)</th>\n",
              "      <th>Urine - Crystals</th>\n",
              "      <th>Urine - Protein</th>\n",
              "      <th>Alkaline phosphatase</th>\n",
              "      <th>Urine - Aspect</th>\n",
              "      <th>Urine - pH</th>\n",
              "      <th>Hemoglobin</th>\n",
              "      <th>Urine - Hyaline cylinders</th>\n",
              "      <th>ctO2 (arterial blood gas analysis)</th>\n",
              "      <th>Myeloblasts</th>\n",
              "      <th>International normalized ratio (INR)</th>\n",
              "      <th>Mean corpuscular hemoglobin concentration (MCHC)</th>\n",
              "      <th>Serum Glucose</th>\n",
              "      <th>Platelets</th>\n",
              "      <th>Hematocrit</th>\n",
              "      <th>Direct Bilirubin</th>\n",
              "      <th>Urine - Urobilinogen</th>\n",
              "      <th>Eosinophils</th>\n",
              "      <th>Strepto A</th>\n",
              "      <th>Hb saturation (arterial blood gases)</th>\n",
              "      <th>Urine - Red blood cells</th>\n",
              "      <th>Urine - Leukocytes</th>\n",
              "      <th>Leukocytes</th>\n",
              "      <th>SARS-Cov-2 exam result</th>\n",
              "      <th>Base excess (venous blood gas analysis)</th>\n",
              "      <th>Phosphor</th>\n",
              "      <th>HCO3 (venous blood gas analysis)</th>\n",
              "      <th>Gamma-glutamyltransferase</th>\n",
              "      <th>Potassium</th>\n",
              "      <th>Urine - Esterase</th>\n",
              "      <th>...</th>\n",
              "      <th>Urine - Yeasts</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Urine - Density</th>\n",
              "      <th>Creatinine</th>\n",
              "      <th>HCO3 (arterial blood gas analysis)</th>\n",
              "      <th>Mean corpuscular volume (MCV)</th>\n",
              "      <th>Arteiral Fio2</th>\n",
              "      <th>Lactic Dehydrogenase</th>\n",
              "      <th>Albumin</th>\n",
              "      <th>Total Bilirubin</th>\n",
              "      <th>pO2 (venous blood gas analysis)</th>\n",
              "      <th>Lymphocytes</th>\n",
              "      <th>Urine - Ketone Bodies</th>\n",
              "      <th>Rods #</th>\n",
              "      <th>Mean corpuscular hemoglobin (MCH)</th>\n",
              "      <th>Basophils</th>\n",
              "      <th>Lipase dosage</th>\n",
              "      <th>Arterial Lactic Acid</th>\n",
              "      <th>Segmented</th>\n",
              "      <th>Total CO2 (arterial blood gas analysis)</th>\n",
              "      <th>Urine - Hemoglobin</th>\n",
              "      <th>Urine - Color</th>\n",
              "      <th>Relationship (Patient/Normal)</th>\n",
              "      <th>Urine - Bile pigments</th>\n",
              "      <th>Total CO2 (venous blood gas analysis)</th>\n",
              "      <th>Urea</th>\n",
              "      <th>Indirect Bilirubin</th>\n",
              "      <th>Hb saturation (venous blood gas analysis)</th>\n",
              "      <th>pO2 (arterial blood gas analysis)</th>\n",
              "      <th>Metamyelocytes</th>\n",
              "      <th>Creatine phosphokinase (CPK)</th>\n",
              "      <th>Base excess (arterial blood gas analysis)</th>\n",
              "      <th>Red blood cell distribution width (RDW)</th>\n",
              "      <th>Red blood Cells</th>\n",
              "      <th>Promyelocytes</th>\n",
              "      <th>pCO2 (arterial blood gas analysis)</th>\n",
              "      <th>Ionized calcium</th>\n",
              "      <th>Urine - Granular cylinders</th>\n",
              "      <th>pH (arterial blood gas analysis)</th>\n",
              "      <th>Patient age quantile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>negative</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5534</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5627</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Alanine transaminase  ...  Patient age quantile\n",
              "672                    NaN  ...                    19\n",
              "5534                   NaN  ...                     9\n",
              "5627                   NaN  ...                     8\n",
              "\n",
              "[3 rows x 81 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ZNM0xn1EkL"
      },
      "source": [
        "Verificamos que el balance se mantenga en los conjuntos de `train`, `test` y `validation`, usando la función `get_class_weights`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODsJBp_bdOX1"
      },
      "source": [
        "'''\n",
        "  Función para calcular proporciones de clases y obtener pesos para equilibrarlas\n",
        "'''\n",
        "def get_class_weights(df,target_col='y'):\n",
        "  # función para obtener pesos acorde al desbalanceo de clases en caso que sea necesario\n",
        "  print(df[target_col].value_counts())\n",
        "  neg, pos = np.bincount(df[target_col])\n",
        "  total = neg + pos\n",
        "  print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))\n",
        "  # Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "  # The sum of the weights of all examples stays the same.\n",
        "  weight_for_0 = (1 / neg)*(total)/2.0 \n",
        "  weight_for_1 = (1 / pos)*(total)/2.0\n",
        "\n",
        "  class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "  print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "  print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
        "  return class_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SEqaRSI05v4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d259e50f-0c96-4f20-b086-69fdafe693c8"
      },
      "source": [
        "print('\\nTrain class weghting')\n",
        "class_wt = get_class_weights(train_df,'y')\n",
        "print(class_wt)\n",
        "print('\\nValidation class weghting')\n",
        "get_class_weights(val_df,'y')\n",
        "print('\\nTest class weghting')\n",
        "get_class_weights(test_df,'y')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train class weghting\n",
            "1    458\n",
            "0    445\n",
            "Name: y, dtype: int64\n",
            "Examples:\n",
            "    Total: 903\n",
            "    Positive: 458 (50.72% of total)\n",
            "\n",
            "Weight for class 0: 1.01\n",
            "Weight for class 1: 0.99\n",
            "{0: 1.0146067415730338, 1: 0.9858078602620087}\n",
            "\n",
            "Validation class weghting\n",
            "0    54\n",
            "1    47\n",
            "Name: y, dtype: int64\n",
            "Examples:\n",
            "    Total: 101\n",
            "    Positive: 47 (46.53% of total)\n",
            "\n",
            "Weight for class 0: 0.94\n",
            "Weight for class 1: 1.07\n",
            "\n",
            "Test class weghting\n",
            "0    59\n",
            "1    53\n",
            "Name: y, dtype: int64\n",
            "Examples:\n",
            "    Total: 112\n",
            "    Positive: 53 (47.32% of total)\n",
            "\n",
            "Weight for class 0: 0.95\n",
            "Weight for class 1: 1.06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.9491525423728814, 1: 1.0566037735849056}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXTcQ95Mh7N0"
      },
      "source": [
        "Debido a que tenemos los datos en formato de dataframe, idealmente necesitamos extraerlos en matrices para despues poder transformar todo a numérico. \n",
        "\n",
        "Para esta extracción definimos la función `get_arrays_from_data_frame`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeLeEmGhwbl_"
      },
      "source": [
        "'''\n",
        "  Función para transformar un dataset en formato DataFrame en matriz así como la \n",
        "  columna de la variable objetivo etiquetada\n",
        "'''\n",
        "def get_arrays_from_data_frame(\n",
        "    df, # DataFrame de entrada\n",
        "    numeric_vars, # lista de columnas que contienen variables numéricas \n",
        "    y_col = 'y' # nombre de la columna que contiene la variable objetivo etiquetada\n",
        "    ):\n",
        "  # extraemos los valores de la porción del DataFrame que contiene valores numéricos en forma de matriz\n",
        "  X_numeric = df[numeric_vars].values\n",
        "  print(X_numeric.shape)\n",
        "  # extraemos la columna que contiene la variable objetivo etiquetada en formato de arreglo\n",
        "  y = df[y_col].values\n",
        "  print(y.shape)\n",
        "  # retornamos eso en una tupla de 4 elementos\n",
        "  return X_numeric,y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiCm6ROWwfYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e938f49-c974-44b3-9c19-be6469e9eb03"
      },
      "source": [
        "# repetimos el proceso para los tres dataframe\n",
        "# vamos a obtemer matrices numericas y el arreglo de etiquetas\n",
        "train_X_numeric,train_y = get_arrays_from_data_frame(train_df,numeric_vars,'y')\n",
        "val_X_numeric,val_y = get_arrays_from_data_frame(val_df,numeric_vars,'y')\n",
        "test_X_numeric,test_y = get_arrays_from_data_frame(test_df,numeric_vars,'y')\n",
        "\n",
        "print('Etiquetas del train-set :\\n{}'.format(train_y[:3]))\n",
        "print('Matriz numerica del train-set :\\n{}'.format(train_X_numeric[:3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(903, 43)\n",
            "(903,)\n",
            "(101, 43)\n",
            "(101,)\n",
            "(112, 43)\n",
            "(112,)\n",
            "Etiquetas del train-set :\n",
            "[0 1 1]\n",
            "Matriz numerica del train-set :\n",
            "[[19. nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan]\n",
            " [ 9. nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan]\n",
            " [ 8. nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM_ceu_O8kiC"
      },
      "source": [
        "Como se ve el dataset tiene muchas variables con valores no asignados o desconocidos. Vamos a imputar los valores faltantes asignando para ellos la `mediana` de la columna correspondiente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reA_AyFmr11F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "548c6148-680d-4e57-f56a-5a2652ff5e1a"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "imputer.fit(train_X_numeric)\n",
        "\n",
        "train_X_numeric = imputer.transform(train_X_numeric)\n",
        "print(train_X_numeric)\n",
        "\n",
        "test_X_numeric = imputer.transform(test_X_numeric)\n",
        "print(test_X_numeric)\n",
        "\n",
        "val_X_numeric= imputer.transform(val_X_numeric)\n",
        "print(val_X_numeric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[19.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [ 9.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [ 8.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " ...\n",
            " [15.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [15.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [10.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]]\n",
            "[[15.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [ 6.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [ 5.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " ...\n",
            " [ 2.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [14.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [ 5.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]]\n",
            "[[ 5.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [14.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [15.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " ...\n",
            " [10.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [ 3.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]\n",
            " [19.          0.17929401  0.22828403 ... -0.40989739  0.03415509\n",
            "  -0.62870961]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2G4zEHa9tlz"
      },
      "source": [
        "Es conveniente que todas las variables de entrada de la red tengan escalas similares de valores por lo que cada matríz será escalada para tener valores entre **-1 y 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMwLErJ--ZiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4110f002-f018-472f-c35f-76000415567b"
      },
      "source": [
        "#Pre-processing the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scaling numeric variables with MinMax\n",
        "scaler_numeric = MinMaxScaler((-1,1))\n",
        "\n",
        "\n",
        "# ajustamos el Scaler solo sobre el trainset\n",
        "X_train = scaler_numeric.fit_transform(train_X_numeric)\n",
        "\n",
        "# sobre el validation y test set solo aplicamos la transformación\n",
        "X_val = scaler_numeric.transform(val_X_numeric)\n",
        "X_test = scaler_numeric.transform(test_X_numeric)\n",
        "\n",
        "\n",
        "print('Matriz numerica del train-set :\\n{}'.format(X_train[:3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz numerica del train-set :\n",
            "[[ 1.00000000e+00  3.77880216e-01  2.80000080e-01 -7.47562294e-01\n",
            "   9.52381043e-02 -1.19914322e-01 -8.65168530e-01  3.02030465e-01\n",
            "  -2.30414769e-01 -6.38888867e-01 -6.12403112e-01  8.74751545e-02\n",
            "  -3.87755114e-01 -8.64440053e-01 -2.42424305e-01 -2.22222178e-01\n",
            "   9.09091019e-02 -9.06614778e-01 -8.88888890e-01 -7.14285651e-01\n",
            "  -3.33333353e-01 -4.99999955e-01 -8.28125001e-01 -1.99999806e-01\n",
            "  -2.32954457e-01  3.10846600e-01  3.75999980e-01 -2.83630460e-01\n",
            "   4.62184864e-01  6.42856583e-02  4.78261032e-01 -8.33333338e-01\n",
            "   6.58536615e-01 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
            "   1.42857165e-01 -9.80198030e-01  3.05555470e-01 -7.01492424e-01\n",
            "  -6.49635038e-01  6.93889390e-17 -8.33333318e-01]\n",
            " [-5.26315789e-02  3.77880216e-01  2.80000080e-01 -7.47562294e-01\n",
            "   9.52381043e-02 -1.19914322e-01 -8.65168530e-01  3.02030465e-01\n",
            "  -2.30414769e-01 -6.38888867e-01 -6.12403112e-01  8.74751545e-02\n",
            "  -3.87755114e-01 -8.64440053e-01 -2.42424305e-01 -2.22222178e-01\n",
            "   9.09091019e-02 -9.06614778e-01 -8.88888890e-01 -7.14285651e-01\n",
            "  -3.33333353e-01 -4.99999955e-01 -8.28125001e-01 -1.99999806e-01\n",
            "  -2.32954457e-01  3.10846600e-01  3.75999980e-01 -2.83630460e-01\n",
            "   4.62184864e-01  6.42856583e-02  4.78261032e-01 -8.33333338e-01\n",
            "   6.58536615e-01 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
            "   1.42857165e-01 -9.80198030e-01  3.05555470e-01 -7.01492424e-01\n",
            "  -6.49635038e-01  6.93889390e-17 -8.33333318e-01]\n",
            " [-1.57894737e-01  3.77880216e-01  2.80000080e-01 -7.47562294e-01\n",
            "   9.52381043e-02 -1.19914322e-01 -8.65168530e-01  3.02030465e-01\n",
            "  -2.30414769e-01 -6.38888867e-01 -6.12403112e-01  8.74751545e-02\n",
            "  -3.87755114e-01 -8.64440053e-01 -2.42424305e-01 -2.22222178e-01\n",
            "   9.09091019e-02 -9.06614778e-01 -8.88888890e-01 -7.14285651e-01\n",
            "  -3.33333353e-01 -4.99999955e-01 -8.28125001e-01 -1.99999806e-01\n",
            "  -2.32954457e-01  3.10846600e-01  3.75999980e-01 -2.83630460e-01\n",
            "   4.62184864e-01  6.42856583e-02  4.78261032e-01 -8.33333338e-01\n",
            "   6.58536615e-01 -1.00000000e+00 -1.00000000e+00 -1.00000000e+00\n",
            "   1.42857165e-01 -9.80198030e-01  3.05555470e-01 -7.01492424e-01\n",
            "  -6.49635038e-01  6.93889390e-17 -8.33333318e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAySHZ0WkJS6"
      },
      "source": [
        "Implementamos la función `get_nnet_model` la cual:\n",
        "\n",
        " * Construye una red neuronal clásica a partir de la especificación de cantidad de capas intermedias y cantidad de neuronas para cada una\n",
        " * La entrena sobre el dataset que le proveamos durante el tiempo que indiquemos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHfEHKr4kEki"
      },
      "source": [
        "\n",
        "'''\n",
        "  Función para construir y entrenar una red neuronal feed forward a partir de un conjunto de datos y profundidad\n",
        "'''\n",
        "def get_nnet_model(\n",
        "    X_train , # matriz con las variables de entrada de entrenamiento de tamaño (n_ejemplos,n_variables)\n",
        "    y_train , # arreglo con las variables objetivo de salida etiquetadas de tamaño (n_ejemplos,)\n",
        "    X_val , y_val , # matrices de validación para usar en el proceso de optimización y entrenamiento \n",
        "    class_wt, # parametro para indicar desbalance de clase\n",
        "    mid_layers_mults = [0.5], # arreglo de multiplicadores de cada capa, la salida de la capa anterior se multiplicará por cada uno de estos valores \n",
        "                              # para obtener la cantidad de neuronas de cada capa intermedia\n",
        "    epochs=300, # un epoch es cuando el dataset entero pasa hacia adelante y atras por la red UNA vez \n",
        "    batch_size=500, # número total de ejemplos de entrenamiento que se pasan en conjunto\n",
        "    early_stop_patience = 10, # cantidad de epochs que espera sin mejoras antes de detener el entrenamiento\n",
        "    monitor_metric = 'val_precision',\n",
        "    monitor_mode = 'max'\n",
        "    ):\n",
        "\n",
        "  # tendremos tantas neuronas de entrada como columnas (variables) en X_train\n",
        "  input_size = int(X_train.shape[1])\n",
        "  # la salida será siempre 1 sola neurona , el resultado del test\n",
        "  output_size = 1\n",
        "  # construiremos la red, agregando capas secuencialmente\n",
        "  nnet = Sequential()\n",
        "  # tendremos una nueva capa por cada multiplicador en el arreglo  mid_layers_mults\n",
        "  # todas las capas serán densas (totalmente conectadas) y usaran como función de activación 'relu'\n",
        "  for idx, mult in enumerate(mid_layers_mults):\n",
        "    if idx == 0 :\n",
        "      # solo para la primera capa debemos especifica el parametro input_dim\n",
        "      nnet.add(Dense( int(input_size * mult) , input_dim=int(input_size), activation='relu'))\n",
        "    else:\n",
        "      nnet.add(Dense( int(input_size * mult) , activation='relu'))\n",
        "    # siempre después de cada capa densa intermedia agregamos una Dropout para evitar el overfitting\n",
        "    nnet.add(Dropout(rate=0.5))\n",
        "\n",
        "  # agregamos la capa final con una función de acivación sigmoide, lo normalmente usado para clasificación binaria\n",
        "  nnet.add(Dense(int(output_size), activation='sigmoid')) \n",
        "\n",
        "  # vamos a proveer estas metricas para poder evaluar como se desempeñan despues\n",
        "  metrics = [\n",
        "    keras.metrics.TruePositives(name='tp'),\n",
        "    keras.metrics.FalsePositives(name='fp'),\n",
        "    keras.metrics.TrueNegatives(name='tn'),\n",
        "    keras.metrics.FalseNegatives(name='fn'), \n",
        "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall'),\n",
        "    keras.metrics.AUC(name='auc'),\n",
        "  ]\n",
        "\n",
        "  # usaremos Stocastich gradient descent como algoritmo para optimizar la red\n",
        "  optimizer = keras.optimizers.SGD(learning_rate=0.1)  \n",
        "  # además la medida a optimizar (loss) será la BinaryCrossEntropy\n",
        "  nnet.compile(optimizer=optimizer,loss=keras.losses.BinaryCrossentropy(),metrics=metrics)\n",
        "  \n",
        "  # guardaremos el modelo con este nombre\n",
        "  out_model_name = 'nnet_{}_mid_layers.model'.format(len(mid_layers_mults))\n",
        "  # cada vez que el modelo mejore la loss sobre el set de validación, se sobre escribirá el ultimo gurdado\n",
        "  \n",
        "  \n",
        "  checkpoint = ModelCheckpoint(out_model_name,save_best_only=True,monitor=monitor_metric,mode=monitor_mode,verbose=1)\n",
        "  # cuando no hayan mejoras luego de 10 epochs se detiene el entrenamiento\n",
        "  early_stop = EarlyStopping(monitor=monitor_metric,mode = monitor_mode, patience=early_stop_patience)\n",
        "\n",
        "  # ejecutamos la optimización real aqui y guardamos la evuloción para usarla luego\n",
        "  history = nnet.fit(X_train, y_train,validation_data = (X_val,y_val), epochs=epochs, batch_size=batch_size,class_weight=class_wt, callbacks=[checkpoint,early_stop])\n",
        "\n",
        "  # leemos el mejor modelo que se entrenó para devolverlo\n",
        "  nnet = load_model(out_model_name)\n",
        "\n",
        "  return nnet,history\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koZMEvEnkzJA"
      },
      "source": [
        "Además definimos la función `get_best_epoch` para poder saber cual fue el epoch donde se obtuvo el mejor valor para una métrica indicada. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SakAh4p1kzjj"
      },
      "source": [
        "'''\n",
        "  Función accesoria para saber cual fue el epochs en el que se logró la mejor medida de una metrica en un historial de entrenamiento \n",
        "'''\n",
        "def get_best_epoch(history,metric = 'val_loss', mode = 'min'):\n",
        "  if mode == 'min':\n",
        "    best_epoch = np.argmin(history.history[metric])\n",
        "  elif mode == 'max':\n",
        "    best_epoch = np.argmax(history.history[metric])\n",
        "  best_metric = history.history[metric][best_epoch]\n",
        "  print('Best {} at best epoch {} was {} '.format(metric,best_epoch,best_metric))\n",
        "  return best_epoch,best_metric\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7iFvltTBwh4"
      },
      "source": [
        "\n",
        "Debido a que podemos parametrizar la cantidad de capas intermedias y cuantas neuronas en cada una, vamos a evaluar como se comportan distintos modelos (redes) de varias profundidades, empezando por uno de una sola capa y aumentando incrementalmente.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ8VKUujG85S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c7ee54-9404-4858-8ef1-c4c637ab3ed6"
      },
      "source": [
        "# este tamaño de batch asegura que en cada pasada podamos tener ejemplos de las distintas clases en grandes cantidades\n",
        "batch_size = X_train.shape[0] # 200\n",
        "\n",
        "epochs = 500\n",
        "\n",
        "early_stop_patience = 100\n",
        "\n",
        "#monitor_metric = 'val_loss'\n",
        "#monitor_mode = 'min'\n",
        "\n",
        "monitor_metric = 'val_accuracy'\n",
        "monitor_mode = 'max'\n",
        "\n",
        "# listas para almacenar los resultados de los distintos modelos en base a su profundidad\n",
        "n_layers = []\n",
        "models = []\n",
        "histories = []\n",
        "best_epochs = []\n",
        "best_metrics = []\n",
        "\n",
        "train_loss = []\n",
        "train_pres = []\n",
        "train_recs = []\n",
        "train_accs = []\n",
        "\n",
        "test_loss = []\n",
        "test_pres = []\n",
        "test_recs = []\n",
        "test_accs = []\n",
        "\n",
        "\n",
        "# el primer modelo tiene solo una capa intermedia que tiene la mitad de las neuronas de entrada como su cantidad de neuronas\n",
        "layers = [1/2]\n",
        "model,history = get_nnet_model(X_train,train_y,X_val,val_y,None,layers,epochs,batch_size,early_stop_patience,monitor_metric,monitor_mode)\n",
        "\n",
        "\n",
        "# guardamos todos los resultados en las listas para despues comparar\n",
        "n_layers.append(len(layers))\n",
        "models.append(model)\n",
        "histories.append(history)\n",
        "\n",
        "best_epoch,best_metric = get_best_epoch(history,monitor_metric,monitor_mode)\n",
        "best_epochs.append(best_epoch)\n",
        "best_metrics.append(best_metric)\n",
        "print(X_train.shape)\n",
        "# mostramos un resumen del modelo \n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.7436 - tp: 353.0000 - fp: 337.0000 - tn: 108.0000 - fn: 105.0000 - accuracy: 0.5105 - precision: 0.5116 - recall: 0.7707 - auc: 0.4963 - val_loss: 0.7136 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5802\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.45545, saving model to nnet_1_mid_layers.model\n",
            "INFO:tensorflow:Assets written to: nnet_1_mid_layers.model/assets\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7269 - tp: 317.0000 - fp: 307.0000 - tn: 138.0000 - fn: 141.0000 - accuracy: 0.5039 - precision: 0.5080 - recall: 0.6921 - auc: 0.5023 - val_loss: 0.7064 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5597\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.45545\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7166 - tp: 273.0000 - fp: 268.0000 - tn: 177.0000 - fn: 185.0000 - accuracy: 0.4983 - precision: 0.5046 - recall: 0.5961 - auc: 0.5053 - val_loss: 0.7033 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5349\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.45545\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6928 - tp: 287.0000 - fp: 243.0000 - tn: 202.0000 - fn: 171.0000 - accuracy: 0.5415 - precision: 0.5415 - recall: 0.6266 - auc: 0.5590 - val_loss: 0.7008 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5181\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.45545\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.7032 - tp: 249.0000 - fp: 214.0000 - tn: 231.0000 - fn: 209.0000 - accuracy: 0.5316 - precision: 0.5378 - recall: 0.5437 - auc: 0.5274 - val_loss: 0.7004 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5181\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.45545\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7122 - tp: 245.0000 - fp: 242.0000 - tn: 203.0000 - fn: 213.0000 - accuracy: 0.4961 - precision: 0.5031 - recall: 0.5349 - auc: 0.4962 - val_loss: 0.6997 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5100\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.45545\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7072 - tp: 253.0000 - fp: 232.0000 - tn: 213.0000 - fn: 205.0000 - accuracy: 0.5161 - precision: 0.5216 - recall: 0.5524 - auc: 0.5137 - val_loss: 0.6992 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5181\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.45545\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6982 - tp: 251.0000 - fp: 239.0000 - tn: 206.0000 - fn: 207.0000 - accuracy: 0.5061 - precision: 0.5122 - recall: 0.5480 - auc: 0.5296 - val_loss: 0.6985 - val_tp: 44.0000 - val_fp: 44.0000 - val_tn: 10.0000 - val_fn: 3.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9362 - val_auc: 0.4653\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.45545 to 0.53465, saving model to nnet_1_mid_layers.model\n",
            "INFO:tensorflow:Assets written to: nnet_1_mid_layers.model/assets\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7058 - tp: 226.0000 - fp: 233.0000 - tn: 212.0000 - fn: 232.0000 - accuracy: 0.4850 - precision: 0.4924 - recall: 0.4934 - auc: 0.4873 - val_loss: 0.6979 - val_tp: 17.0000 - val_fp: 23.0000 - val_tn: 31.0000 - val_fn: 30.0000 - val_accuracy: 0.4752 - val_precision: 0.4250 - val_recall: 0.3617 - val_auc: 0.5727\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.53465\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6946 - tp: 256.0000 - fp: 215.0000 - tn: 230.0000 - fn: 202.0000 - accuracy: 0.5382 - precision: 0.5435 - recall: 0.5590 - auc: 0.5418 - val_loss: 0.6973 - val_tp: 4.0000 - val_fp: 2.0000 - val_tn: 52.0000 - val_fn: 43.0000 - val_accuracy: 0.5545 - val_precision: 0.6667 - val_recall: 0.0851 - val_auc: 0.4653\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.53465 to 0.55446, saving model to nnet_1_mid_layers.model\n",
            "INFO:tensorflow:Assets written to: nnet_1_mid_layers.model/assets\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.7035 - tp: 211.0000 - fp: 203.0000 - tn: 242.0000 - fn: 247.0000 - accuracy: 0.5017 - precision: 0.5097 - recall: 0.4607 - auc: 0.5034 - val_loss: 0.6973 - val_tp: 6.0000 - val_fp: 4.0000 - val_tn: 50.0000 - val_fn: 41.0000 - val_accuracy: 0.5545 - val_precision: 0.6000 - val_recall: 0.1277 - val_auc: 0.4649\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.55446\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7012 - tp: 230.0000 - fp: 210.0000 - tn: 235.0000 - fn: 228.0000 - accuracy: 0.5150 - precision: 0.5227 - recall: 0.5022 - auc: 0.5141 - val_loss: 0.6979 - val_tp: 31.0000 - val_fp: 33.0000 - val_tn: 21.0000 - val_fn: 16.0000 - val_accuracy: 0.5149 - val_precision: 0.4844 - val_recall: 0.6596 - val_auc: 0.4752\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.55446\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6963 - tp: 252.0000 - fp: 234.0000 - tn: 211.0000 - fn: 206.0000 - accuracy: 0.5127 - precision: 0.5185 - recall: 0.5502 - auc: 0.5186 - val_loss: 0.6971 - val_tp: 9.0000 - val_fp: 7.0000 - val_tn: 47.0000 - val_fn: 38.0000 - val_accuracy: 0.5545 - val_precision: 0.5625 - val_recall: 0.1915 - val_auc: 0.4807\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.55446\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6982 - tp: 235.0000 - fp: 206.0000 - tn: 239.0000 - fn: 223.0000 - accuracy: 0.5249 - precision: 0.5329 - recall: 0.5131 - auc: 0.5196 - val_loss: 0.6968 - val_tp: 4.0000 - val_fp: 2.0000 - val_tn: 52.0000 - val_fn: 43.0000 - val_accuracy: 0.5545 - val_precision: 0.6667 - val_recall: 0.0851 - val_auc: 0.4771\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.55446\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7035 - tp: 225.0000 - fp: 211.0000 - tn: 234.0000 - fn: 233.0000 - accuracy: 0.5083 - precision: 0.5161 - recall: 0.4913 - auc: 0.4932 - val_loss: 0.6968 - val_tp: 9.0000 - val_fp: 10.0000 - val_tn: 44.0000 - val_fn: 38.0000 - val_accuracy: 0.5248 - val_precision: 0.4737 - val_recall: 0.1915 - val_auc: 0.5144\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.55446\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6977 - tp: 222.0000 - fp: 204.0000 - tn: 241.0000 - fn: 236.0000 - accuracy: 0.5127 - precision: 0.5211 - recall: 0.4847 - auc: 0.5206 - val_loss: 0.6965 - val_tp: 6.0000 - val_fp: 4.0000 - val_tn: 50.0000 - val_fn: 41.0000 - val_accuracy: 0.5545 - val_precision: 0.6000 - val_recall: 0.1277 - val_auc: 0.4569\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.55446\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7021 - tp: 225.0000 - fp: 227.0000 - tn: 218.0000 - fn: 233.0000 - accuracy: 0.4906 - precision: 0.4978 - recall: 0.4913 - auc: 0.5010 - val_loss: 0.6971 - val_tp: 18.0000 - val_fp: 25.0000 - val_tn: 29.0000 - val_fn: 29.0000 - val_accuracy: 0.4653 - val_precision: 0.4186 - val_recall: 0.3830 - val_auc: 0.5055\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.55446\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6945 - tp: 226.0000 - fp: 211.0000 - tn: 234.0000 - fn: 232.0000 - accuracy: 0.5094 - precision: 0.5172 - recall: 0.4934 - auc: 0.5267 - val_loss: 0.6975 - val_tp: 31.0000 - val_fp: 33.0000 - val_tn: 21.0000 - val_fn: 16.0000 - val_accuracy: 0.5149 - val_precision: 0.4844 - val_recall: 0.6596 - val_auc: 0.4827\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.55446\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6971 - tp: 251.0000 - fp: 229.0000 - tn: 216.0000 - fn: 207.0000 - accuracy: 0.5172 - precision: 0.5229 - recall: 0.5480 - auc: 0.5167 - val_loss: 0.6970 - val_tp: 18.0000 - val_fp: 25.0000 - val_tn: 29.0000 - val_fn: 29.0000 - val_accuracy: 0.4653 - val_precision: 0.4186 - val_recall: 0.3830 - val_auc: 0.5012\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.55446\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6987 - tp: 232.0000 - fp: 230.0000 - tn: 215.0000 - fn: 226.0000 - accuracy: 0.4950 - precision: 0.5022 - recall: 0.5066 - auc: 0.5054 - val_loss: 0.6967 - val_tp: 17.0000 - val_fp: 23.0000 - val_tn: 31.0000 - val_fn: 30.0000 - val_accuracy: 0.4752 - val_precision: 0.4250 - val_recall: 0.3617 - val_auc: 0.4984\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.55446\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7003 - tp: 215.0000 - fp: 199.0000 - tn: 246.0000 - fn: 243.0000 - accuracy: 0.5105 - precision: 0.5193 - recall: 0.4694 - auc: 0.5006 - val_loss: 0.6968 - val_tp: 23.0000 - val_fp: 28.0000 - val_tn: 26.0000 - val_fn: 24.0000 - val_accuracy: 0.4851 - val_precision: 0.4510 - val_recall: 0.4894 - val_auc: 0.5032\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.55446\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6953 - tp: 235.0000 - fp: 221.0000 - tn: 224.0000 - fn: 223.0000 - accuracy: 0.5083 - precision: 0.5154 - recall: 0.5131 - auc: 0.5191 - val_loss: 0.6964 - val_tp: 17.0000 - val_fp: 23.0000 - val_tn: 31.0000 - val_fn: 30.0000 - val_accuracy: 0.4752 - val_precision: 0.4250 - val_recall: 0.3617 - val_auc: 0.4756\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.55446\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6942 - tp: 224.0000 - fp: 203.0000 - tn: 242.0000 - fn: 234.0000 - accuracy: 0.5161 - precision: 0.5246 - recall: 0.4891 - auc: 0.5254 - val_loss: 0.6964 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4756\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.55446\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6960 - tp: 226.0000 - fp: 231.0000 - tn: 214.0000 - fn: 232.0000 - accuracy: 0.4873 - precision: 0.4945 - recall: 0.4934 - auc: 0.5034 - val_loss: 0.6964 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4704\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.55446\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6975 - tp: 229.0000 - fp: 225.0000 - tn: 220.0000 - fn: 229.0000 - accuracy: 0.4972 - precision: 0.5044 - recall: 0.5000 - auc: 0.4940 - val_loss: 0.6958 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5177\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.55446\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6983 - tp: 224.0000 - fp: 220.0000 - tn: 225.0000 - fn: 234.0000 - accuracy: 0.4972 - precision: 0.5045 - recall: 0.4891 - auc: 0.4947 - val_loss: 0.6956 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5075\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.55446\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6959 - tp: 216.0000 - fp: 204.0000 - tn: 241.0000 - fn: 242.0000 - accuracy: 0.5061 - precision: 0.5143 - recall: 0.4716 - auc: 0.5025 - val_loss: 0.6957 - val_tp: 14.0000 - val_fp: 22.0000 - val_tn: 32.0000 - val_fn: 33.0000 - val_accuracy: 0.4554 - val_precision: 0.3889 - val_recall: 0.2979 - val_auc: 0.4742\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.55446\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6966 - tp: 214.0000 - fp: 201.0000 - tn: 244.0000 - fn: 244.0000 - accuracy: 0.5072 - precision: 0.5157 - recall: 0.4672 - auc: 0.5022 - val_loss: 0.6959 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4915\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.55446\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6942 - tp: 227.0000 - fp: 223.0000 - tn: 222.0000 - fn: 231.0000 - accuracy: 0.4972 - precision: 0.5044 - recall: 0.4956 - auc: 0.5112 - val_loss: 0.6959 - val_tp: 18.0000 - val_fp: 26.0000 - val_tn: 28.0000 - val_fn: 29.0000 - val_accuracy: 0.4554 - val_precision: 0.4091 - val_recall: 0.3830 - val_auc: 0.4803\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.55446\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6959 - tp: 247.0000 - fp: 244.0000 - tn: 201.0000 - fn: 211.0000 - accuracy: 0.4961 - precision: 0.5031 - recall: 0.5393 - auc: 0.4937 - val_loss: 0.6959 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4866\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.55446\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6929 - tp: 232.0000 - fp: 231.0000 - tn: 214.0000 - fn: 226.0000 - accuracy: 0.4939 - precision: 0.5011 - recall: 0.5066 - auc: 0.5006 - val_loss: 0.6960 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4874\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.55446\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6925 - tp: 237.0000 - fp: 223.0000 - tn: 222.0000 - fn: 221.0000 - accuracy: 0.5083 - precision: 0.5152 - recall: 0.5175 - auc: 0.5212 - val_loss: 0.6961 - val_tp: 18.0000 - val_fp: 26.0000 - val_tn: 28.0000 - val_fn: 29.0000 - val_accuracy: 0.4554 - val_precision: 0.4091 - val_recall: 0.3830 - val_auc: 0.4842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.55446\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6937 - tp: 241.0000 - fp: 226.0000 - tn: 219.0000 - fn: 217.0000 - accuracy: 0.5094 - precision: 0.5161 - recall: 0.5262 - auc: 0.5168 - val_loss: 0.6959 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4874\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.55446\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6941 - tp: 246.0000 - fp: 224.0000 - tn: 221.0000 - fn: 212.0000 - accuracy: 0.5172 - precision: 0.5234 - recall: 0.5371 - auc: 0.5166 - val_loss: 0.6959 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4972\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.55446\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6929 - tp: 240.0000 - fp: 215.0000 - tn: 230.0000 - fn: 218.0000 - accuracy: 0.5205 - precision: 0.5275 - recall: 0.5240 - auc: 0.5236 - val_loss: 0.6957 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4868\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.55446\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6953 - tp: 220.0000 - fp: 228.0000 - tn: 217.0000 - fn: 238.0000 - accuracy: 0.4839 - precision: 0.4911 - recall: 0.4803 - auc: 0.4933 - val_loss: 0.6954 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4817\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.55446\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6930 - tp: 223.0000 - fp: 210.0000 - tn: 235.0000 - fn: 235.0000 - accuracy: 0.5072 - precision: 0.5150 - recall: 0.4869 - auc: 0.5107 - val_loss: 0.6952 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4917\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.55446\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6922 - tp: 231.0000 - fp: 207.0000 - tn: 238.0000 - fn: 227.0000 - accuracy: 0.5194 - precision: 0.5274 - recall: 0.5044 - auc: 0.5229 - val_loss: 0.6951 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4888\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.55446\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6942 - tp: 210.0000 - fp: 218.0000 - tn: 227.0000 - fn: 248.0000 - accuracy: 0.4839 - precision: 0.4907 - recall: 0.4585 - auc: 0.4939 - val_loss: 0.6950 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4939\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.55446\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6905 - tp: 243.0000 - fp: 230.0000 - tn: 215.0000 - fn: 215.0000 - accuracy: 0.5072 - precision: 0.5137 - recall: 0.5306 - auc: 0.5169 - val_loss: 0.6947 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.4833\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.55446\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6869 - tp: 251.0000 - fp: 202.0000 - tn: 243.0000 - fn: 207.0000 - accuracy: 0.5471 - precision: 0.5541 - recall: 0.5480 - auc: 0.5571 - val_loss: 0.6945 - val_tp: 17.0000 - val_fp: 24.0000 - val_tn: 30.0000 - val_fn: 30.0000 - val_accuracy: 0.4653 - val_precision: 0.4146 - val_recall: 0.3617 - val_auc: 0.5016\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.55446\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6941 - tp: 229.0000 - fp: 235.0000 - tn: 210.0000 - fn: 229.0000 - accuracy: 0.4862 - precision: 0.4935 - recall: 0.5000 - auc: 0.4988 - val_loss: 0.6944 - val_tp: 18.0000 - val_fp: 26.0000 - val_tn: 28.0000 - val_fn: 29.0000 - val_accuracy: 0.4554 - val_precision: 0.4091 - val_recall: 0.3830 - val_auc: 0.4953\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.55446\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6963 - tp: 209.0000 - fp: 233.0000 - tn: 212.0000 - fn: 249.0000 - accuracy: 0.4662 - precision: 0.4729 - recall: 0.4563 - auc: 0.4859 - val_loss: 0.6942 - val_tp: 18.0000 - val_fp: 26.0000 - val_tn: 28.0000 - val_fn: 29.0000 - val_accuracy: 0.4554 - val_precision: 0.4091 - val_recall: 0.3830 - val_auc: 0.5146\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.55446\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6898 - tp: 227.0000 - fp: 210.0000 - tn: 235.0000 - fn: 231.0000 - accuracy: 0.5116 - precision: 0.5195 - recall: 0.4956 - auc: 0.5300 - val_loss: 0.6942 - val_tp: 18.0000 - val_fp: 26.0000 - val_tn: 28.0000 - val_fn: 29.0000 - val_accuracy: 0.4554 - val_precision: 0.4091 - val_recall: 0.3830 - val_auc: 0.5242\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.55446\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6914 - tp: 237.0000 - fp: 203.0000 - tn: 242.0000 - fn: 221.0000 - accuracy: 0.5305 - precision: 0.5386 - recall: 0.5175 - auc: 0.5299 - val_loss: 0.6942 - val_tp: 25.0000 - val_fp: 31.0000 - val_tn: 23.0000 - val_fn: 22.0000 - val_accuracy: 0.4752 - val_precision: 0.4464 - val_recall: 0.5319 - val_auc: 0.5008\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.55446\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6923 - tp: 255.0000 - fp: 232.0000 - tn: 213.0000 - fn: 203.0000 - accuracy: 0.5183 - precision: 0.5236 - recall: 0.5568 - auc: 0.5266 - val_loss: 0.6939 - val_tp: 23.0000 - val_fp: 28.0000 - val_tn: 26.0000 - val_fn: 24.0000 - val_accuracy: 0.4851 - val_precision: 0.4510 - val_recall: 0.4894 - val_auc: 0.5187\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.55446\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6907 - tp: 220.0000 - fp: 203.0000 - tn: 242.0000 - fn: 238.0000 - accuracy: 0.5116 - precision: 0.5201 - recall: 0.4803 - auc: 0.5368 - val_loss: 0.6940 - val_tp: 25.0000 - val_fp: 31.0000 - val_tn: 23.0000 - val_fn: 22.0000 - val_accuracy: 0.4752 - val_precision: 0.4464 - val_recall: 0.5319 - val_auc: 0.5028\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.55446\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6923 - tp: 247.0000 - fp: 214.0000 - tn: 231.0000 - fn: 211.0000 - accuracy: 0.5293 - precision: 0.5358 - recall: 0.5393 - auc: 0.5367 - val_loss: 0.6938 - val_tp: 25.0000 - val_fp: 31.0000 - val_tn: 23.0000 - val_fn: 22.0000 - val_accuracy: 0.4752 - val_precision: 0.4464 - val_recall: 0.5319 - val_auc: 0.5355\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.55446\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6940 - tp: 227.0000 - fp: 228.0000 - tn: 217.0000 - fn: 231.0000 - accuracy: 0.4917 - precision: 0.4989 - recall: 0.4956 - auc: 0.5113 - val_loss: 0.6938 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5102\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.55446\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6897 - tp: 252.0000 - fp: 222.0000 - tn: 223.0000 - fn: 206.0000 - accuracy: 0.5260 - precision: 0.5316 - recall: 0.5502 - auc: 0.5448 - val_loss: 0.6940 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5067\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.55446\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6907 - tp: 251.0000 - fp: 243.0000 - tn: 202.0000 - fn: 207.0000 - accuracy: 0.5017 - precision: 0.5081 - recall: 0.5480 - auc: 0.5148 - val_loss: 0.6938 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5309\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.55446\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6922 - tp: 255.0000 - fp: 260.0000 - tn: 185.0000 - fn: 203.0000 - accuracy: 0.4873 - precision: 0.4951 - recall: 0.5568 - auc: 0.5100 - val_loss: 0.6937 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5394\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.55446\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6919 - tp: 244.0000 - fp: 237.0000 - tn: 208.0000 - fn: 214.0000 - accuracy: 0.5006 - precision: 0.5073 - recall: 0.5328 - auc: 0.5189 - val_loss: 0.6937 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5329\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.55446\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6908 - tp: 252.0000 - fp: 239.0000 - tn: 206.0000 - fn: 206.0000 - accuracy: 0.5072 - precision: 0.5132 - recall: 0.5502 - auc: 0.5147 - val_loss: 0.6934 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5366\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.55446\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6895 - tp: 270.0000 - fp: 245.0000 - tn: 200.0000 - fn: 188.0000 - accuracy: 0.5205 - precision: 0.5243 - recall: 0.5895 - auc: 0.5327 - val_loss: 0.6932 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5349\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.55446\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6916 - tp: 259.0000 - fp: 222.0000 - tn: 223.0000 - fn: 199.0000 - accuracy: 0.5338 - precision: 0.5385 - recall: 0.5655 - auc: 0.5265 - val_loss: 0.6928 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5325\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.55446\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6885 - tp: 274.0000 - fp: 232.0000 - tn: 213.0000 - fn: 184.0000 - accuracy: 0.5393 - precision: 0.5415 - recall: 0.5983 - auc: 0.5587 - val_loss: 0.6925 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5167\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.55446\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6911 - tp: 251.0000 - fp: 234.0000 - tn: 211.0000 - fn: 207.0000 - accuracy: 0.5116 - precision: 0.5175 - recall: 0.5480 - auc: 0.5158 - val_loss: 0.6926 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5368\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.55446\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6905 - tp: 260.0000 - fp: 237.0000 - tn: 208.0000 - fn: 198.0000 - accuracy: 0.5183 - precision: 0.5231 - recall: 0.5677 - auc: 0.5316 - val_loss: 0.6925 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5368\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.55446\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6885 - tp: 257.0000 - fp: 239.0000 - tn: 206.0000 - fn: 201.0000 - accuracy: 0.5127 - precision: 0.5181 - recall: 0.5611 - auc: 0.5439 - val_loss: 0.6927 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5325\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.55446\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6892 - tp: 269.0000 - fp: 246.0000 - tn: 199.0000 - fn: 189.0000 - accuracy: 0.5183 - precision: 0.5223 - recall: 0.5873 - auc: 0.5414 - val_loss: 0.6927 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5325\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.55446\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6876 - tp: 273.0000 - fp: 242.0000 - tn: 203.0000 - fn: 185.0000 - accuracy: 0.5271 - precision: 0.5301 - recall: 0.5961 - auc: 0.5464 - val_loss: 0.6925 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5368\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.55446\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6900 - tp: 254.0000 - fp: 230.0000 - tn: 215.0000 - fn: 204.0000 - accuracy: 0.5194 - precision: 0.5248 - recall: 0.5546 - auc: 0.5308 - val_loss: 0.6925 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5368\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.55446\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6887 - tp: 249.0000 - fp: 219.0000 - tn: 226.0000 - fn: 209.0000 - accuracy: 0.5260 - precision: 0.5321 - recall: 0.5437 - auc: 0.5378 - val_loss: 0.6928 - val_tp: 31.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 16.0000 - val_accuracy: 0.4653 - val_precision: 0.4493 - val_recall: 0.6596 - val_auc: 0.5361\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.55446\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6893 - tp: 276.0000 - fp: 247.0000 - tn: 198.0000 - fn: 182.0000 - accuracy: 0.5249 - precision: 0.5277 - recall: 0.6026 - auc: 0.5391 - val_loss: 0.6927 - val_tp: 31.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 16.0000 - val_accuracy: 0.4653 - val_precision: 0.4493 - val_recall: 0.6596 - val_auc: 0.5355\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.55446\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6897 - tp: 280.0000 - fp: 247.0000 - tn: 198.0000 - fn: 178.0000 - accuracy: 0.5293 - precision: 0.5313 - recall: 0.6114 - auc: 0.5443 - val_loss: 0.6923 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5376\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.55446\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6874 - tp: 260.0000 - fp: 225.0000 - tn: 220.0000 - fn: 198.0000 - accuracy: 0.5316 - precision: 0.5361 - recall: 0.5677 - auc: 0.5442 - val_loss: 0.6923 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5378\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.55446\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6890 - tp: 254.0000 - fp: 233.0000 - tn: 212.0000 - fn: 204.0000 - accuracy: 0.5161 - precision: 0.5216 - recall: 0.5546 - auc: 0.5335 - val_loss: 0.6923 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5394\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.55446\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6857 - tp: 261.0000 - fp: 226.0000 - tn: 219.0000 - fn: 197.0000 - accuracy: 0.5316 - precision: 0.5359 - recall: 0.5699 - auc: 0.5630 - val_loss: 0.6922 - val_tp: 31.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 16.0000 - val_accuracy: 0.5050 - val_precision: 0.4769 - val_recall: 0.6596 - val_auc: 0.5394\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.55446\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6883 - tp: 277.0000 - fp: 234.0000 - tn: 211.0000 - fn: 181.0000 - accuracy: 0.5404 - precision: 0.5421 - recall: 0.6048 - auc: 0.5549 - val_loss: 0.6926 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5837\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.55446\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6907 - tp: 274.0000 - fp: 238.0000 - tn: 207.0000 - fn: 184.0000 - accuracy: 0.5327 - precision: 0.5352 - recall: 0.5983 - auc: 0.5367 - val_loss: 0.6926 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5583\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.55446\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6878 - tp: 280.0000 - fp: 249.0000 - tn: 196.0000 - fn: 178.0000 - accuracy: 0.5271 - precision: 0.5293 - recall: 0.6114 - auc: 0.5464 - val_loss: 0.6925 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5398\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.55446\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6928 - tp: 256.0000 - fp: 250.0000 - tn: 195.0000 - fn: 202.0000 - accuracy: 0.4994 - precision: 0.5059 - recall: 0.5590 - auc: 0.5129 - val_loss: 0.6924 - val_tp: 32.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 15.0000 - val_accuracy: 0.5149 - val_precision: 0.4848 - val_recall: 0.6809 - val_auc: 0.5357\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.55446\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6893 - tp: 273.0000 - fp: 247.0000 - tn: 198.0000 - fn: 185.0000 - accuracy: 0.5216 - precision: 0.5250 - recall: 0.5961 - auc: 0.5207 - val_loss: 0.6925 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5420\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.55446\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6894 - tp: 268.0000 - fp: 254.0000 - tn: 191.0000 - fn: 190.0000 - accuracy: 0.5083 - precision: 0.5134 - recall: 0.5852 - auc: 0.5361 - val_loss: 0.6923 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5420\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.55446\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6876 - tp: 282.0000 - fp: 233.0000 - tn: 212.0000 - fn: 176.0000 - accuracy: 0.5471 - precision: 0.5476 - recall: 0.6157 - auc: 0.5503 - val_loss: 0.6926 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5384\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.55446\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6879 - tp: 292.0000 - fp: 265.0000 - tn: 180.0000 - fn: 166.0000 - accuracy: 0.5227 - precision: 0.5242 - recall: 0.6376 - auc: 0.5509 - val_loss: 0.6926 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5361\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.55446\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6859 - tp: 287.0000 - fp: 248.0000 - tn: 197.0000 - fn: 171.0000 - accuracy: 0.5360 - precision: 0.5364 - recall: 0.6266 - auc: 0.5549 - val_loss: 0.6925 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5625\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.55446\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6925 - tp: 287.0000 - fp: 253.0000 - tn: 192.0000 - fn: 171.0000 - accuracy: 0.5305 - precision: 0.5315 - recall: 0.6266 - auc: 0.5323 - val_loss: 0.6925 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5705\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.55446\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6864 - tp: 265.0000 - fp: 249.0000 - tn: 196.0000 - fn: 193.0000 - accuracy: 0.5105 - precision: 0.5156 - recall: 0.5786 - auc: 0.5378 - val_loss: 0.6922 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5422\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.55446\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6903 - tp: 271.0000 - fp: 228.0000 - tn: 217.0000 - fn: 187.0000 - accuracy: 0.5404 - precision: 0.5431 - recall: 0.5917 - auc: 0.5613 - val_loss: 0.6926 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5361\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.55446\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6869 - tp: 302.0000 - fp: 269.0000 - tn: 176.0000 - fn: 156.0000 - accuracy: 0.5293 - precision: 0.5289 - recall: 0.6594 - auc: 0.5519 - val_loss: 0.6926 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5408\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.55446\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6867 - tp: 293.0000 - fp: 268.0000 - tn: 177.0000 - fn: 165.0000 - accuracy: 0.5205 - precision: 0.5223 - recall: 0.6397 - auc: 0.5525 - val_loss: 0.6928 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5406\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.55446\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6874 - tp: 312.0000 - fp: 261.0000 - tn: 184.0000 - fn: 146.0000 - accuracy: 0.5493 - precision: 0.5445 - recall: 0.6812 - auc: 0.5681 - val_loss: 0.6923 - val_tp: 32.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 15.0000 - val_accuracy: 0.5149 - val_precision: 0.4848 - val_recall: 0.6809 - val_auc: 0.5177\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.55446\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6912 - tp: 282.0000 - fp: 245.0000 - tn: 200.0000 - fn: 176.0000 - accuracy: 0.5338 - precision: 0.5351 - recall: 0.6157 - auc: 0.5451 - val_loss: 0.6927 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5449\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.55446\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6888 - tp: 309.0000 - fp: 275.0000 - tn: 170.0000 - fn: 149.0000 - accuracy: 0.5305 - precision: 0.5291 - recall: 0.6747 - auc: 0.5463 - val_loss: 0.6924 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5410\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.55446\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6897 - tp: 294.0000 - fp: 277.0000 - tn: 168.0000 - fn: 164.0000 - accuracy: 0.5116 - precision: 0.5149 - recall: 0.6419 - auc: 0.5349 - val_loss: 0.6922 - val_tp: 32.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 15.0000 - val_accuracy: 0.5149 - val_precision: 0.4848 - val_recall: 0.6809 - val_auc: 0.5211\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.55446\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6880 - tp: 279.0000 - fp: 251.0000 - tn: 194.0000 - fn: 179.0000 - accuracy: 0.5238 - precision: 0.5264 - recall: 0.6092 - auc: 0.5437 - val_loss: 0.6924 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5467\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.55446\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6896 - tp: 295.0000 - fp: 284.0000 - tn: 161.0000 - fn: 163.0000 - accuracy: 0.5050 - precision: 0.5095 - recall: 0.6441 - auc: 0.5332 - val_loss: 0.6926 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5451\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.55446\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6913 - tp: 298.0000 - fp: 278.0000 - tn: 167.0000 - fn: 160.0000 - accuracy: 0.5150 - precision: 0.5174 - recall: 0.6507 - auc: 0.5280 - val_loss: 0.6922 - val_tp: 32.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 15.0000 - val_accuracy: 0.5149 - val_precision: 0.4848 - val_recall: 0.6809 - val_auc: 0.5211\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.55446\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6879 - tp: 282.0000 - fp: 235.0000 - tn: 210.0000 - fn: 176.0000 - accuracy: 0.5449 - precision: 0.5455 - recall: 0.6157 - auc: 0.5451 - val_loss: 0.6925 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5475\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.55446\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6863 - tp: 309.0000 - fp: 259.0000 - tn: 186.0000 - fn: 149.0000 - accuracy: 0.5482 - precision: 0.5440 - recall: 0.6747 - auc: 0.5764 - val_loss: 0.6924 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5370\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.55446\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6868 - tp: 305.0000 - fp: 249.0000 - tn: 196.0000 - fn: 153.0000 - accuracy: 0.5548 - precision: 0.5505 - recall: 0.6659 - auc: 0.5553 - val_loss: 0.6923 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5400\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.55446\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6888 - tp: 304.0000 - fp: 259.0000 - tn: 186.0000 - fn: 154.0000 - accuracy: 0.5426 - precision: 0.5400 - recall: 0.6638 - auc: 0.5582 - val_loss: 0.6924 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5420\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.55446\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6909 - tp: 302.0000 - fp: 281.0000 - tn: 164.0000 - fn: 156.0000 - accuracy: 0.5161 - precision: 0.5180 - recall: 0.6594 - auc: 0.5395 - val_loss: 0.6923 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5457\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.55446\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6879 - tp: 290.0000 - fp: 256.0000 - tn: 189.0000 - fn: 168.0000 - accuracy: 0.5305 - precision: 0.5311 - recall: 0.6332 - auc: 0.5399 - val_loss: 0.6925 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5453\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.55446\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6901 - tp: 297.0000 - fp: 264.0000 - tn: 181.0000 - fn: 161.0000 - accuracy: 0.5293 - precision: 0.5294 - recall: 0.6485 - auc: 0.5411 - val_loss: 0.6928 - val_tp: 36.0000 - val_fp: 39.0000 - val_tn: 15.0000 - val_fn: 11.0000 - val_accuracy: 0.5050 - val_precision: 0.4800 - val_recall: 0.7660 - val_auc: 0.5112\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.55446\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6865 - tp: 325.0000 - fp: 274.0000 - tn: 171.0000 - fn: 133.0000 - accuracy: 0.5493 - precision: 0.5426 - recall: 0.7096 - auc: 0.5684 - val_loss: 0.6928 - val_tp: 36.0000 - val_fp: 39.0000 - val_tn: 15.0000 - val_fn: 11.0000 - val_accuracy: 0.5050 - val_precision: 0.4800 - val_recall: 0.7660 - val_auc: 0.5461\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.55446\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6863 - tp: 333.0000 - fp: 286.0000 - tn: 159.0000 - fn: 125.0000 - accuracy: 0.5449 - precision: 0.5380 - recall: 0.7271 - auc: 0.5784 - val_loss: 0.6927 - val_tp: 36.0000 - val_fp: 39.0000 - val_tn: 15.0000 - val_fn: 11.0000 - val_accuracy: 0.5050 - val_precision: 0.4800 - val_recall: 0.7660 - val_auc: 0.5276\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.55446\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6874 - tp: 334.0000 - fp: 292.0000 - tn: 153.0000 - fn: 124.0000 - accuracy: 0.5393 - precision: 0.5335 - recall: 0.7293 - auc: 0.5621 - val_loss: 0.6928 - val_tp: 36.0000 - val_fp: 39.0000 - val_tn: 15.0000 - val_fn: 11.0000 - val_accuracy: 0.5050 - val_precision: 0.4800 - val_recall: 0.7660 - val_auc: 0.5248\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.55446\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6901 - tp: 320.0000 - fp: 297.0000 - tn: 148.0000 - fn: 138.0000 - accuracy: 0.5183 - precision: 0.5186 - recall: 0.6987 - auc: 0.5231 - val_loss: 0.6925 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5353\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.55446\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6897 - tp: 296.0000 - fp: 283.0000 - tn: 162.0000 - fn: 162.0000 - accuracy: 0.5072 - precision: 0.5112 - recall: 0.6463 - auc: 0.5328 - val_loss: 0.6924 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5071\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.55446\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6840 - tp: 324.0000 - fp: 259.0000 - tn: 186.0000 - fn: 134.0000 - accuracy: 0.5648 - precision: 0.5557 - recall: 0.7074 - auc: 0.5838 - val_loss: 0.6923 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5132\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.55446\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6892 - tp: 300.0000 - fp: 273.0000 - tn: 172.0000 - fn: 158.0000 - accuracy: 0.5227 - precision: 0.5236 - recall: 0.6550 - auc: 0.5264 - val_loss: 0.6920 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5536\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.55446\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6896 - tp: 303.0000 - fp: 263.0000 - tn: 182.0000 - fn: 155.0000 - accuracy: 0.5371 - precision: 0.5353 - recall: 0.6616 - auc: 0.5567 - val_loss: 0.6920 - val_tp: 32.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 15.0000 - val_accuracy: 0.5149 - val_precision: 0.4848 - val_recall: 0.6809 - val_auc: 0.5282\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.55446\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6875 - tp: 287.0000 - fp: 243.0000 - tn: 202.0000 - fn: 171.0000 - accuracy: 0.5415 - precision: 0.5415 - recall: 0.6266 - auc: 0.5585 - val_loss: 0.6920 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5510\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.55446\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6892 - tp: 290.0000 - fp: 274.0000 - tn: 171.0000 - fn: 168.0000 - accuracy: 0.5105 - precision: 0.5142 - recall: 0.6332 - auc: 0.5329 - val_loss: 0.6920 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5502\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.55446\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6896 - tp: 289.0000 - fp: 266.0000 - tn: 179.0000 - fn: 169.0000 - accuracy: 0.5183 - precision: 0.5207 - recall: 0.6310 - auc: 0.5354 - val_loss: 0.6916 - val_tp: 32.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 15.0000 - val_accuracy: 0.5149 - val_precision: 0.4848 - val_recall: 0.6809 - val_auc: 0.5110\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.55446\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6901 - tp: 280.0000 - fp: 252.0000 - tn: 193.0000 - fn: 178.0000 - accuracy: 0.5238 - precision: 0.5263 - recall: 0.6114 - auc: 0.5380 - val_loss: 0.6918 - val_tp: 32.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 15.0000 - val_accuracy: 0.5149 - val_precision: 0.4848 - val_recall: 0.6809 - val_auc: 0.5335\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.55446\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6848 - tp: 283.0000 - fp: 248.0000 - tn: 197.0000 - fn: 175.0000 - accuracy: 0.5316 - precision: 0.5330 - recall: 0.6179 - auc: 0.5614 - val_loss: 0.6918 - val_tp: 32.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 15.0000 - val_accuracy: 0.4752 - val_precision: 0.4571 - val_recall: 0.6809 - val_auc: 0.5426\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.55446\n",
            "Best val_accuracy at best epoch 9 was 0.5544554591178894 \n",
            "(903, 43)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 21)                924       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 21)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 22        \n",
            "=================================================================\n",
            "Total params: 946\n",
            "Trainable params: 946\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvvlRMCPmHcP"
      },
      "source": [
        "### Algunas preguntas sobre este primer modelo :\n",
        "#### 1) Si la capa de entrada tiene 43 neuronas (43 variables de entrada), ¿ Cuantás neuronas tiene la capa intermedia?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaxreBXCn0BY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492c6b25-222f-4b2c-faa2-5c8e73de9b4c"
      },
      "source": [
        "n_neuronas_mid_layer_1 = 21\n",
        "print('La capa intermedia tiene {} neuronas'.format(n_neuronas_mid_layer_1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "La capa intermedia tiene 21 neuronas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpbT0WDen0Oj"
      },
      "source": [
        "#### 2) Dado que la capa intermedia es densa, ¿Cuantos pesos tenemos que ajustar aproximadamente?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k9imvWZoCUb"
      },
      "source": [
        "n_pesos_mid_layer_1 = ?\n",
        "print('La capa intermedia tiene {} pesos por optimizar'.format(n_pesos_mid_layer_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnij7s82oCoy"
      },
      "source": [
        "#### 3) La salida obviamente tiene solo una neurona, pero también está densamente conectada con la capa intermedia, ¿Cuantos pesos tenemos que optimizar en esta capa? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrWORlIfoPOg"
      },
      "source": [
        "n_pesos_last_layer_1 = ?\n",
        "print('La capa final tiene {} pesos por optimizar'.format(n_pesos_last_layer_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcMvGLdGDPQH"
      },
      "source": [
        "Graficamos la loss de entrenamiento y validacion usando la función `plot_metric` implementada a continuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFYUI2esqqPv"
      },
      "source": [
        "'''\n",
        "  Función para graficar una sola metrica del objeto history\n",
        "'''\n",
        "def plot_metric(history,metric='loss',mode='min'):\n",
        "\n",
        "  val_metric = 'val_'+metric\n",
        "  best_epoch, best_val_metric = get_best_epoch(history,val_metric,mode)\n",
        "  \n",
        "  name = metric.replace(\"_\",\" \").capitalize()\n",
        "  plt.plot(history.epoch,  history.history[metric], label='Train')\n",
        "  plt.plot(history.epoch, history.history[val_metric], linestyle=\"--\", label='Val')\n",
        "  plt.plot([best_epoch], [best_val_metric], 'r*',label='Best {} epoch'.format(val_metric))\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(name)\n",
        "  if metric == 'loss':\n",
        "    plt.ylim([0, plt.ylim()[1]])\n",
        "  elif metric == 'auc':\n",
        "    plt.ylim([0.8,1])\n",
        "  else:\n",
        "    plt.ylim([0,1])\n",
        "\n",
        "  plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQoFZICErqA9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "outputId": "556fb845-0eae-4414-849d-dca03e4ee8b8"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plot_metric(histories[0],'accuracy',monitor_mode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best val_accuracy at best epoch 9 was 0.5544554591178894 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYhs3ExOrdf2"
      },
      "source": [
        "Vemos que en el epoch **`completar aquí`** se obtuvo la mejor loss de validación (asterisco rojo). Teoricamente, a partír de ahi la `loss` de validación empieza a alejarse de la de entrenamiento y se da el overfitting, por eso decidimos usar el modelo de ese epoch para ser evaluado. \n",
        "\n",
        "Notesé además que debido al mecanismo de corte temprano, no llegamos a los `500` epochs que seteamos al principio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6ybjtFGsYKH"
      },
      "source": [
        "Ahora vamos a evaluar el modelo final sobre el `train` y `test` set para ver como se comportan sus medidas al momento de generalizar. Además veremos su matríz de confusión "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaNR4Hmmsijc"
      },
      "source": [
        "'''\n",
        "  Función para graficar e imprimir la matriz de confusión y medidas comparando un vector de predicciones vs su ground truth\n",
        "'''\n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('True Negatives: ', cm[0][0])\n",
        "  print('False Positives: ', cm[0][1])\n",
        "  print('False Negatives: ', cm[1][0])\n",
        "  print('True Positives: ', cm[1][1])\n",
        "  print('Total : ', np.sum(cm[1]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fY_n5l2uB4s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "23c0f65f-eb11-4530-f476-80ff3b1abf91"
      },
      "source": [
        "test_predictions = model.predict(X_test, batch_size=batch_size)\n",
        "\n",
        "print('Train results')\n",
        "print()\n",
        "results = model.evaluate(X_train, train_y,batch_size=batch_size, verbose=0)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(name, ': ', value)\n",
        "  if name == 'loss':\n",
        "    train_loss.append(value)\n",
        "  elif name == 'precision':\n",
        "    train_pres.append(value)\n",
        "  elif name == 'recall':\n",
        "    train_recs.append(value)\n",
        "  elif name == 'accuracy': \n",
        "    train_accs.append(value)\n",
        "print()\n",
        "\n",
        "print('Test results')\n",
        "print()\n",
        "results = model.evaluate(X_test, test_y,batch_size=batch_size, verbose=0)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(name, ': ', value)\n",
        "  if name == 'loss':\n",
        "    test_loss.append(value)\n",
        "  elif name == 'precision':\n",
        "    test_pres.append(value)\n",
        "  elif name == 'recall':\n",
        "    test_recs.append(value)\n",
        "  elif name == 'accuracy': \n",
        "    test_accs.append(value)\n",
        "print()\n",
        "\n",
        "print('Confusion matrix')\n",
        "print()\n",
        "plot_cm(test_y, test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train results\n",
            "\n",
            "loss :  0.6921249628067017\n",
            "tp :  43.0\n",
            "fp :  23.0\n",
            "tn :  422.0\n",
            "fn :  415.0\n",
            "accuracy :  0.514950156211853\n",
            "precision :  0.6515151262283325\n",
            "recall :  0.09388646483421326\n",
            "auc :  0.5040553212165833\n",
            "\n",
            "Test results\n",
            "\n",
            "loss :  0.6960626840591431\n",
            "tp :  3.0\n",
            "fp :  5.0\n",
            "tn :  54.0\n",
            "fn :  50.0\n",
            "accuracy :  0.5089285969734192\n",
            "precision :  0.375\n",
            "recall :  0.056603774428367615\n",
            "auc :  0.4245283007621765\n",
            "\n",
            "Confusion matrix\n",
            "\n",
            "True Negatives:  54\n",
            "False Positives:  5\n",
            "False Negatives:  50\n",
            "True Positives:  3\n",
            "Total :  53\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN62yn3Nso_a"
      },
      "source": [
        "Recordamos que una  matriz de confusión reporta en la celda `(i,j)` la cantidad de casos con la etiqueta `i`, pero que fueron clasificados como `j`. En este problema particular las celdas reportan:\n",
        "* `(0,0)` : `True negatives`\n",
        "* `(1,1)` : `True positives`\n",
        "* `(1,0)` : `False negatives`\n",
        "* `(0,1)` : `False positives`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2np8wxDvFHU"
      },
      "source": [
        "#### Observando la matriz, ¿Cual es la proporción de casos positivos que se encontraron correctamente?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezUk_Ss0vcND",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "7a47c397-55dd-48d5-e44a-3fec5ff93318"
      },
      "source": [
        "positivos_encontrados_1 = ?\n",
        "print('Positivos encontrados {} % '.format(positivos_encontrados_1 * 100.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-6e05cdaf7648>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    positivos_encontrados_1 = ?\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZYRwCYlwLe1"
      },
      "source": [
        "#### ¿Con qué metrica se corresponde este valor?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzd9YuptwTjp"
      },
      "source": [
        "metrica_positivos_encontrados_1 = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eoNzJzNvckD"
      },
      "source": [
        "#### De los casos clasificados como positivos, ¿Cual es la proporción de ellos que realmente fueron positivos? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1uEFrVivoQe"
      },
      "source": [
        "positivos_correctos_1 = ?\n",
        "print('Positivos correctos {} % '.format(positivos_correctos_1 * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzPeJgUzwuqx"
      },
      "source": [
        "#### ¿Con qué metrica se corresponde este valor?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ew26cEKwyBR"
      },
      "source": [
        "metrica_positivos_correctos_1 = ? "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UybfNAZsxA1M"
      },
      "source": [
        "#### ¿Cuales son las ventajas de este modelo y cuales son sus desventajas?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up85aX8zxGmX"
      },
      "source": [
        "ventajas_desventajas_1 = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIvuxKruEFwN"
      },
      "source": [
        "A continuación, probamos un modelo mas profundo, con 2 capas. Para esto llamamos de nuevo a la función `get_nnet_model` pero usando una lista de 2 elementos para parametrizar las capas intermedias.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCnx2Nc0klwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6700964e-a8fa-4400-bf19-5febdebb2eb1"
      },
      "source": [
        "layers = [1/2,1/4]\n",
        "model,history = get_nnet_model(X_train,train_y,X_val,val_y,None,layers,epochs,batch_size,early_stop_patience,monitor_metric,monitor_mode)\n",
        "\n",
        "n_layers.append(len(layers))\n",
        "models.append(model)\n",
        "histories.append(history)\n",
        "\n",
        "best_epoch,best_metric = get_best_epoch(history,monitor_metric,monitor_mode)\n",
        "best_epochs.append(best_epoch)\n",
        "best_metrics.append(best_metric)\n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7871 - tp: 254.0000 - fp: 263.0000 - tn: 182.0000 - fn: 204.0000 - accuracy: 0.4828 - precision: 0.4913 - recall: 0.5546 - auc: 0.4828 - val_loss: 0.6975 - val_tp: 47.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.4851 - val_precision: 0.4747 - val_recall: 1.0000 - val_auc: 0.4963\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.48515, saving model to nnet_2_mid_layers.model\n",
            "INFO:tensorflow:Assets written to: nnet_2_mid_layers.model/assets\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.7676 - tp: 275.0000 - fp: 260.0000 - tn: 185.0000 - fn: 183.0000 - accuracy: 0.5094 - precision: 0.5140 - recall: 0.6004 - auc: 0.5061 - val_loss: 0.6964 - val_tp: 45.0000 - val_fp: 42.0000 - val_tn: 12.0000 - val_fn: 2.0000 - val_accuracy: 0.5644 - val_precision: 0.5172 - val_recall: 0.9574 - val_auc: 0.4909\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.48515 to 0.56436, saving model to nnet_2_mid_layers.model\n",
            "INFO:tensorflow:Assets written to: nnet_2_mid_layers.model/assets\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.7415 - tp: 295.0000 - fp: 257.0000 - tn: 188.0000 - fn: 163.0000 - accuracy: 0.5349 - precision: 0.5344 - recall: 0.6441 - auc: 0.5306 - val_loss: 0.6956 - val_tp: 32.0000 - val_fp: 33.0000 - val_tn: 21.0000 - val_fn: 15.0000 - val_accuracy: 0.5248 - val_precision: 0.4923 - val_recall: 0.6809 - val_auc: 0.5083\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.56436\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7402 - tp: 298.0000 - fp: 290.0000 - tn: 155.0000 - fn: 160.0000 - accuracy: 0.5017 - precision: 0.5068 - recall: 0.6507 - auc: 0.5059 - val_loss: 0.6944 - val_tp: 19.0000 - val_fp: 25.0000 - val_tn: 29.0000 - val_fn: 28.0000 - val_accuracy: 0.4752 - val_precision: 0.4318 - val_recall: 0.4043 - val_auc: 0.4990\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.56436\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.7468 - tp: 270.0000 - fp: 286.0000 - tn: 159.0000 - fn: 188.0000 - accuracy: 0.4751 - precision: 0.4856 - recall: 0.5895 - auc: 0.4777 - val_loss: 0.6943 - val_tp: 17.0000 - val_fp: 23.0000 - val_tn: 31.0000 - val_fn: 30.0000 - val_accuracy: 0.4752 - val_precision: 0.4250 - val_recall: 0.3617 - val_auc: 0.5014\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.56436\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.7371 - tp: 275.0000 - fp: 278.0000 - tn: 167.0000 - fn: 183.0000 - accuracy: 0.4895 - precision: 0.4973 - recall: 0.6004 - auc: 0.4870 - val_loss: 0.6937 - val_tp: 11.0000 - val_fp: 19.0000 - val_tn: 35.0000 - val_fn: 36.0000 - val_accuracy: 0.4554 - val_precision: 0.3667 - val_recall: 0.2340 - val_auc: 0.4742\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.56436\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7252 - tp: 269.0000 - fp: 268.0000 - tn: 177.0000 - fn: 189.0000 - accuracy: 0.4939 - precision: 0.5009 - recall: 0.5873 - auc: 0.4930 - val_loss: 0.6935 - val_tp: 13.0000 - val_fp: 21.0000 - val_tn: 33.0000 - val_fn: 34.0000 - val_accuracy: 0.4554 - val_precision: 0.3824 - val_recall: 0.2766 - val_auc: 0.4972\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.56436\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.7273 - tp: 263.0000 - fp: 266.0000 - tn: 179.0000 - fn: 195.0000 - accuracy: 0.4895 - precision: 0.4972 - recall: 0.5742 - auc: 0.4763 - val_loss: 0.6935 - val_tp: 17.0000 - val_fp: 23.0000 - val_tn: 31.0000 - val_fn: 30.0000 - val_accuracy: 0.4752 - val_precision: 0.4250 - val_recall: 0.3617 - val_auc: 0.4959\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.56436\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7104 - tp: 283.0000 - fp: 257.0000 - tn: 188.0000 - fn: 175.0000 - accuracy: 0.5216 - precision: 0.5241 - recall: 0.6179 - auc: 0.5180 - val_loss: 0.6933 - val_tp: 12.0000 - val_fp: 19.0000 - val_tn: 35.0000 - val_fn: 35.0000 - val_accuracy: 0.4653 - val_precision: 0.3871 - val_recall: 0.2553 - val_auc: 0.4710\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.56436\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7169 - tp: 264.0000 - fp: 262.0000 - tn: 183.0000 - fn: 194.0000 - accuracy: 0.4950 - precision: 0.5019 - recall: 0.5764 - auc: 0.4890 - val_loss: 0.6936 - val_tp: 17.0000 - val_fp: 23.0000 - val_tn: 31.0000 - val_fn: 30.0000 - val_accuracy: 0.4752 - val_precision: 0.4250 - val_recall: 0.3617 - val_auc: 0.5122\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.56436\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7114 - tp: 268.0000 - fp: 272.0000 - tn: 173.0000 - fn: 190.0000 - accuracy: 0.4884 - precision: 0.4963 - recall: 0.5852 - auc: 0.5062 - val_loss: 0.6937 - val_tp: 29.0000 - val_fp: 34.0000 - val_tn: 20.0000 - val_fn: 18.0000 - val_accuracy: 0.4851 - val_precision: 0.4603 - val_recall: 0.6170 - val_auc: 0.5175\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.56436\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.7146 - tp: 283.0000 - fp: 280.0000 - tn: 165.0000 - fn: 175.0000 - accuracy: 0.4961 - precision: 0.5027 - recall: 0.6179 - auc: 0.4857 - val_loss: 0.6939 - val_tp: 36.0000 - val_fp: 40.0000 - val_tn: 14.0000 - val_fn: 11.0000 - val_accuracy: 0.4950 - val_precision: 0.4737 - val_recall: 0.7660 - val_auc: 0.5057\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.56436\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.7053 - tp: 278.0000 - fp: 240.0000 - tn: 205.0000 - fn: 180.0000 - accuracy: 0.5349 - precision: 0.5367 - recall: 0.6070 - auc: 0.5148 - val_loss: 0.6939 - val_tp: 44.0000 - val_fp: 44.0000 - val_tn: 10.0000 - val_fn: 3.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9362 - val_auc: 0.4915\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.56436\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7049 - tp: 274.0000 - fp: 253.0000 - tn: 192.0000 - fn: 184.0000 - accuracy: 0.5161 - precision: 0.5199 - recall: 0.5983 - auc: 0.5108 - val_loss: 0.6938 - val_tp: 44.0000 - val_fp: 44.0000 - val_tn: 10.0000 - val_fn: 3.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9362 - val_auc: 0.4848\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.56436\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6988 - tp: 293.0000 - fp: 276.0000 - tn: 169.0000 - fn: 165.0000 - accuracy: 0.5116 - precision: 0.5149 - recall: 0.6397 - auc: 0.5126 - val_loss: 0.6936 - val_tp: 43.0000 - val_fp: 42.0000 - val_tn: 12.0000 - val_fn: 4.0000 - val_accuracy: 0.5446 - val_precision: 0.5059 - val_recall: 0.9149 - val_auc: 0.4850\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.56436\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.7242 - tp: 238.0000 - fp: 276.0000 - tn: 169.0000 - fn: 220.0000 - accuracy: 0.4507 - precision: 0.4630 - recall: 0.5197 - auc: 0.4348 - val_loss: 0.6941 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.4982\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.56436\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7002 - tp: 279.0000 - fp: 271.0000 - tn: 174.0000 - fn: 179.0000 - accuracy: 0.5017 - precision: 0.5073 - recall: 0.6092 - auc: 0.5103 - val_loss: 0.6938 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.5597\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.56436\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6984 - tp: 281.0000 - fp: 270.0000 - tn: 175.0000 - fn: 177.0000 - accuracy: 0.5050 - precision: 0.5100 - recall: 0.6135 - auc: 0.5178 - val_loss: 0.6940 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.5219\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.56436\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7099 - tp: 279.0000 - fp: 272.0000 - tn: 173.0000 - fn: 179.0000 - accuracy: 0.5006 - precision: 0.5064 - recall: 0.6092 - auc: 0.4736 - val_loss: 0.6943 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.4860\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.56436\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6946 - tp: 291.0000 - fp: 254.0000 - tn: 191.0000 - fn: 167.0000 - accuracy: 0.5338 - precision: 0.5339 - recall: 0.6354 - auc: 0.5354 - val_loss: 0.6942 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.4866\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.56436\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6978 - tp: 280.0000 - fp: 272.0000 - tn: 173.0000 - fn: 178.0000 - accuracy: 0.5017 - precision: 0.5072 - recall: 0.6114 - auc: 0.5125 - val_loss: 0.6943 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.5122\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.56436\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6970 - tp: 289.0000 - fp: 266.0000 - tn: 179.0000 - fn: 169.0000 - accuracy: 0.5183 - precision: 0.5207 - recall: 0.6310 - auc: 0.5214 - val_loss: 0.6942 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.5126\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.56436\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7038 - tp: 274.0000 - fp: 275.0000 - tn: 170.0000 - fn: 184.0000 - accuracy: 0.4917 - precision: 0.4991 - recall: 0.5983 - auc: 0.4697 - val_loss: 0.6945 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.4968\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.56436\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6990 - tp: 269.0000 - fp: 283.0000 - tn: 162.0000 - fn: 189.0000 - accuracy: 0.4773 - precision: 0.4873 - recall: 0.5873 - auc: 0.5039 - val_loss: 0.6947 - val_tp: 45.0000 - val_fp: 52.0000 - val_tn: 2.0000 - val_fn: 2.0000 - val_accuracy: 0.4653 - val_precision: 0.4639 - val_recall: 0.9574 - val_auc: 0.4990\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.56436\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7025 - tp: 298.0000 - fp: 297.0000 - tn: 148.0000 - fn: 160.0000 - accuracy: 0.4939 - precision: 0.5008 - recall: 0.6507 - auc: 0.4820 - val_loss: 0.6949 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5201\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.56436\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7041 - tp: 288.0000 - fp: 291.0000 - tn: 154.0000 - fn: 170.0000 - accuracy: 0.4895 - precision: 0.4974 - recall: 0.6288 - auc: 0.4788 - val_loss: 0.6947 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.4921\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.56436\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6961 - tp: 281.0000 - fp: 265.0000 - tn: 180.0000 - fn: 177.0000 - accuracy: 0.5105 - precision: 0.5147 - recall: 0.6135 - auc: 0.5181 - val_loss: 0.6947 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5292\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.56436\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6971 - tp: 290.0000 - fp: 289.0000 - tn: 156.0000 - fn: 168.0000 - accuracy: 0.4939 - precision: 0.5009 - recall: 0.6332 - auc: 0.5022 - val_loss: 0.6950 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5229\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.56436\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6937 - tp: 307.0000 - fp: 279.0000 - tn: 166.0000 - fn: 151.0000 - accuracy: 0.5238 - precision: 0.5239 - recall: 0.6703 - auc: 0.5253 - val_loss: 0.6952 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.4878\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.56436\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6953 - tp: 297.0000 - fp: 277.0000 - tn: 168.0000 - fn: 161.0000 - accuracy: 0.5150 - precision: 0.5174 - recall: 0.6485 - auc: 0.5178 - val_loss: 0.6951 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5138\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.56436\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6964 - tp: 288.0000 - fp: 265.0000 - tn: 180.0000 - fn: 170.0000 - accuracy: 0.5183 - precision: 0.5208 - recall: 0.6288 - auc: 0.5208 - val_loss: 0.6953 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.4856\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.56436\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6987 - tp: 284.0000 - fp: 280.0000 - tn: 165.0000 - fn: 174.0000 - accuracy: 0.4972 - precision: 0.5035 - recall: 0.6201 - auc: 0.4907 - val_loss: 0.6948 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5286\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.56436\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6969 - tp: 287.0000 - fp: 268.0000 - tn: 177.0000 - fn: 171.0000 - accuracy: 0.5138 - precision: 0.5171 - recall: 0.6266 - auc: 0.4965 - val_loss: 0.6947 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5268\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.56436\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6997 - tp: 280.0000 - fp: 288.0000 - tn: 157.0000 - fn: 178.0000 - accuracy: 0.4839 - precision: 0.4930 - recall: 0.6114 - auc: 0.4825 - val_loss: 0.6946 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.4844\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.56436\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6992 - tp: 284.0000 - fp: 278.0000 - tn: 167.0000 - fn: 174.0000 - accuracy: 0.4994 - precision: 0.5053 - recall: 0.6201 - auc: 0.4897 - val_loss: 0.6944 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5278\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.56436\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6983 - tp: 281.0000 - fp: 281.0000 - tn: 164.0000 - fn: 177.0000 - accuracy: 0.4928 - precision: 0.5000 - recall: 0.6135 - auc: 0.4980 - val_loss: 0.6942 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5487\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.56436\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6943 - tp: 297.0000 - fp: 265.0000 - tn: 180.0000 - fn: 161.0000 - accuracy: 0.5282 - precision: 0.5285 - recall: 0.6485 - auc: 0.5198 - val_loss: 0.6943 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5418\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.56436\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6976 - tp: 285.0000 - fp: 279.0000 - tn: 166.0000 - fn: 173.0000 - accuracy: 0.4994 - precision: 0.5053 - recall: 0.6223 - auc: 0.4946 - val_loss: 0.6942 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5069\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.56436\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6996 - tp: 287.0000 - fp: 270.0000 - tn: 175.0000 - fn: 171.0000 - accuracy: 0.5116 - precision: 0.5153 - recall: 0.6266 - auc: 0.5018 - val_loss: 0.6939 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.4567\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.56436\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6977 - tp: 269.0000 - fp: 280.0000 - tn: 165.0000 - fn: 189.0000 - accuracy: 0.4806 - precision: 0.4900 - recall: 0.5873 - auc: 0.4938 - val_loss: 0.6938 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5102\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.56436\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6931 - tp: 284.0000 - fp: 262.0000 - tn: 183.0000 - fn: 174.0000 - accuracy: 0.5172 - precision: 0.5201 - recall: 0.6201 - auc: 0.5119 - val_loss: 0.6937 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5370\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.56436\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6948 - tp: 290.0000 - fp: 268.0000 - tn: 177.0000 - fn: 168.0000 - accuracy: 0.5172 - precision: 0.5197 - recall: 0.6332 - auc: 0.5039 - val_loss: 0.6936 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5370\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.56436\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6955 - tp: 285.0000 - fp: 277.0000 - tn: 168.0000 - fn: 173.0000 - accuracy: 0.5017 - precision: 0.5071 - recall: 0.6223 - auc: 0.5045 - val_loss: 0.6937 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5087\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.56436\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6929 - tp: 288.0000 - fp: 261.0000 - tn: 184.0000 - fn: 170.0000 - accuracy: 0.5227 - precision: 0.5246 - recall: 0.6288 - auc: 0.5319 - val_loss: 0.6937 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5441\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.56436\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6921 - tp: 305.0000 - fp: 284.0000 - tn: 161.0000 - fn: 153.0000 - accuracy: 0.5161 - precision: 0.5178 - recall: 0.6659 - auc: 0.5256 - val_loss: 0.6937 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5370\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.56436\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6952 - tp: 274.0000 - fp: 268.0000 - tn: 177.0000 - fn: 184.0000 - accuracy: 0.4994 - precision: 0.5055 - recall: 0.5983 - auc: 0.4896 - val_loss: 0.6937 - val_tp: 45.0000 - val_fp: 53.0000 - val_tn: 1.0000 - val_fn: 2.0000 - val_accuracy: 0.4554 - val_precision: 0.4592 - val_recall: 0.9574 - val_auc: 0.5372\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.56436\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6966 - tp: 292.0000 - fp: 287.0000 - tn: 158.0000 - fn: 166.0000 - accuracy: 0.4983 - precision: 0.5043 - recall: 0.6376 - auc: 0.4951 - val_loss: 0.6936 - val_tp: 45.0000 - val_fp: 54.0000 - val_tn: 0.0000e+00 - val_fn: 2.0000 - val_accuracy: 0.4455 - val_precision: 0.4545 - val_recall: 0.9574 - val_auc: 0.5085\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.56436\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6926 - tp: 301.0000 - fp: 285.0000 - tn: 160.0000 - fn: 157.0000 - accuracy: 0.5105 - precision: 0.5137 - recall: 0.6572 - auc: 0.5085 - val_loss: 0.6936 - val_tp: 44.0000 - val_fp: 49.0000 - val_tn: 5.0000 - val_fn: 3.0000 - val_accuracy: 0.4851 - val_precision: 0.4731 - val_recall: 0.9362 - val_auc: 0.5439\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.56436\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6934 - tp: 284.0000 - fp: 245.0000 - tn: 200.0000 - fn: 174.0000 - accuracy: 0.5360 - precision: 0.5369 - recall: 0.6201 - auc: 0.5303 - val_loss: 0.6935 - val_tp: 44.0000 - val_fp: 45.0000 - val_tn: 9.0000 - val_fn: 3.0000 - val_accuracy: 0.5248 - val_precision: 0.4944 - val_recall: 0.9362 - val_auc: 0.5426\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.56436\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6966 - tp: 268.0000 - fp: 265.0000 - tn: 180.0000 - fn: 190.0000 - accuracy: 0.4961 - precision: 0.5028 - recall: 0.5852 - auc: 0.5013 - val_loss: 0.6935 - val_tp: 44.0000 - val_fp: 49.0000 - val_tn: 5.0000 - val_fn: 3.0000 - val_accuracy: 0.4851 - val_precision: 0.4731 - val_recall: 0.9362 - val_auc: 0.5660\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.56436\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6972 - tp: 258.0000 - fp: 285.0000 - tn: 160.0000 - fn: 200.0000 - accuracy: 0.4629 - precision: 0.4751 - recall: 0.5633 - auc: 0.4674 - val_loss: 0.6933 - val_tp: 44.0000 - val_fp: 49.0000 - val_tn: 5.0000 - val_fn: 3.0000 - val_accuracy: 0.4851 - val_precision: 0.4731 - val_recall: 0.9362 - val_auc: 0.5644\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.56436\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6964 - tp: 276.0000 - fp: 278.0000 - tn: 167.0000 - fn: 182.0000 - accuracy: 0.4906 - precision: 0.4982 - recall: 0.6026 - auc: 0.5001 - val_loss: 0.6933 - val_tp: 44.0000 - val_fp: 49.0000 - val_tn: 5.0000 - val_fn: 3.0000 - val_accuracy: 0.4851 - val_precision: 0.4731 - val_recall: 0.9362 - val_auc: 0.5236\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.56436\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6903 - tp: 288.0000 - fp: 256.0000 - tn: 189.0000 - fn: 170.0000 - accuracy: 0.5282 - precision: 0.5294 - recall: 0.6288 - auc: 0.5302 - val_loss: 0.6933 - val_tp: 44.0000 - val_fp: 45.0000 - val_tn: 9.0000 - val_fn: 3.0000 - val_accuracy: 0.5248 - val_precision: 0.4944 - val_recall: 0.9362 - val_auc: 0.4927\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.56436\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6914 - tp: 280.0000 - fp: 263.0000 - tn: 182.0000 - fn: 178.0000 - accuracy: 0.5116 - precision: 0.5157 - recall: 0.6114 - auc: 0.5157 - val_loss: 0.6932 - val_tp: 44.0000 - val_fp: 44.0000 - val_tn: 10.0000 - val_fn: 3.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9362 - val_auc: 0.4415\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.56436\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6941 - tp: 278.0000 - fp: 269.0000 - tn: 176.0000 - fn: 180.0000 - accuracy: 0.5028 - precision: 0.5082 - recall: 0.6070 - auc: 0.5044 - val_loss: 0.6929 - val_tp: 44.0000 - val_fp: 44.0000 - val_tn: 10.0000 - val_fn: 3.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9362 - val_auc: 0.4710\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.56436\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6927 - tp: 279.0000 - fp: 257.0000 - tn: 188.0000 - fn: 179.0000 - accuracy: 0.5172 - precision: 0.5205 - recall: 0.6092 - auc: 0.5125 - val_loss: 0.6929 - val_tp: 43.0000 - val_fp: 43.0000 - val_tn: 11.0000 - val_fn: 4.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9149 - val_auc: 0.5065\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.56436\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6963 - tp: 266.0000 - fp: 268.0000 - tn: 177.0000 - fn: 192.0000 - accuracy: 0.4906 - precision: 0.4981 - recall: 0.5808 - auc: 0.4934 - val_loss: 0.6929 - val_tp: 43.0000 - val_fp: 43.0000 - val_tn: 11.0000 - val_fn: 4.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9149 - val_auc: 0.5093\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.56436\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6923 - tp: 276.0000 - fp: 244.0000 - tn: 201.0000 - fn: 182.0000 - accuracy: 0.5282 - precision: 0.5308 - recall: 0.6026 - auc: 0.5292 - val_loss: 0.6930 - val_tp: 43.0000 - val_fp: 43.0000 - val_tn: 11.0000 - val_fn: 4.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9149 - val_auc: 0.5089\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.56436\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6957 - tp: 274.0000 - fp: 256.0000 - tn: 189.0000 - fn: 184.0000 - accuracy: 0.5127 - precision: 0.5170 - recall: 0.5983 - auc: 0.5096 - val_loss: 0.6929 - val_tp: 43.0000 - val_fp: 43.0000 - val_tn: 11.0000 - val_fn: 4.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9149 - val_auc: 0.5089\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.56436\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6934 - tp: 279.0000 - fp: 265.0000 - tn: 180.0000 - fn: 179.0000 - accuracy: 0.5083 - precision: 0.5129 - recall: 0.6092 - auc: 0.5191 - val_loss: 0.6928 - val_tp: 39.0000 - val_fp: 39.0000 - val_tn: 15.0000 - val_fn: 8.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.8298 - val_auc: 0.5376\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.56436\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6957 - tp: 267.0000 - fp: 286.0000 - tn: 159.0000 - fn: 191.0000 - accuracy: 0.4718 - precision: 0.4828 - recall: 0.5830 - auc: 0.4865 - val_loss: 0.6928 - val_tp: 36.0000 - val_fp: 38.0000 - val_tn: 16.0000 - val_fn: 11.0000 - val_accuracy: 0.5149 - val_precision: 0.4865 - val_recall: 0.7660 - val_auc: 0.5449\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.56436\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6947 - tp: 268.0000 - fp: 260.0000 - tn: 185.0000 - fn: 190.0000 - accuracy: 0.5017 - precision: 0.5076 - recall: 0.5852 - auc: 0.4997 - val_loss: 0.6927 - val_tp: 17.0000 - val_fp: 23.0000 - val_tn: 31.0000 - val_fn: 30.0000 - val_accuracy: 0.4752 - val_precision: 0.4250 - val_recall: 0.3617 - val_auc: 0.5431\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.56436\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6967 - tp: 267.0000 - fp: 256.0000 - tn: 189.0000 - fn: 191.0000 - accuracy: 0.5050 - precision: 0.5105 - recall: 0.5830 - auc: 0.4870 - val_loss: 0.6927 - val_tp: 17.0000 - val_fp: 23.0000 - val_tn: 31.0000 - val_fn: 30.0000 - val_accuracy: 0.4752 - val_precision: 0.4250 - val_recall: 0.3617 - val_auc: 0.5449\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.56436\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6909 - tp: 262.0000 - fp: 239.0000 - tn: 206.0000 - fn: 196.0000 - accuracy: 0.5183 - precision: 0.5230 - recall: 0.5721 - auc: 0.5386 - val_loss: 0.6927 - val_tp: 24.0000 - val_fp: 29.0000 - val_tn: 25.0000 - val_fn: 23.0000 - val_accuracy: 0.4851 - val_precision: 0.4528 - val_recall: 0.5106 - val_auc: 0.5376\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.56436\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6953 - tp: 274.0000 - fp: 275.0000 - tn: 170.0000 - fn: 184.0000 - accuracy: 0.4917 - precision: 0.4991 - recall: 0.5983 - auc: 0.5179 - val_loss: 0.6928 - val_tp: 43.0000 - val_fp: 43.0000 - val_tn: 11.0000 - val_fn: 4.0000 - val_accuracy: 0.5347 - val_precision: 0.5000 - val_recall: 0.9149 - val_auc: 0.5372\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.56436\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6962 - tp: 266.0000 - fp: 263.0000 - tn: 182.0000 - fn: 192.0000 - accuracy: 0.4961 - precision: 0.5028 - recall: 0.5808 - auc: 0.4945 - val_loss: 0.6927 - val_tp: 38.0000 - val_fp: 39.0000 - val_tn: 15.0000 - val_fn: 9.0000 - val_accuracy: 0.5248 - val_precision: 0.4935 - val_recall: 0.8085 - val_auc: 0.5372\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.56436\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6920 - tp: 276.0000 - fp: 266.0000 - tn: 179.0000 - fn: 182.0000 - accuracy: 0.5039 - precision: 0.5092 - recall: 0.6026 - auc: 0.5067 - val_loss: 0.6927 - val_tp: 22.0000 - val_fp: 25.0000 - val_tn: 29.0000 - val_fn: 25.0000 - val_accuracy: 0.5050 - val_precision: 0.4681 - val_recall: 0.4681 - val_auc: 0.5372\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.56436\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6956 - tp: 261.0000 - fp: 258.0000 - tn: 187.0000 - fn: 197.0000 - accuracy: 0.4961 - precision: 0.5029 - recall: 0.5699 - auc: 0.4897 - val_loss: 0.6929 - val_tp: 24.0000 - val_fp: 29.0000 - val_tn: 25.0000 - val_fn: 23.0000 - val_accuracy: 0.4851 - val_precision: 0.4528 - val_recall: 0.5106 - val_auc: 0.5372\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.56436\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6927 - tp: 277.0000 - fp: 255.0000 - tn: 190.0000 - fn: 181.0000 - accuracy: 0.5172 - precision: 0.5207 - recall: 0.6048 - auc: 0.5198 - val_loss: 0.6928 - val_tp: 24.0000 - val_fp: 29.0000 - val_tn: 25.0000 - val_fn: 23.0000 - val_accuracy: 0.4851 - val_precision: 0.4528 - val_recall: 0.5106 - val_auc: 0.5457\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.56436\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6929 - tp: 270.0000 - fp: 274.0000 - tn: 171.0000 - fn: 188.0000 - accuracy: 0.4884 - precision: 0.4963 - recall: 0.5895 - auc: 0.5054 - val_loss: 0.6926 - val_tp: 11.0000 - val_fp: 18.0000 - val_tn: 36.0000 - val_fn: 36.0000 - val_accuracy: 0.4653 - val_precision: 0.3793 - val_recall: 0.2340 - val_auc: 0.5528\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.56436\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6962 - tp: 269.0000 - fp: 268.0000 - tn: 177.0000 - fn: 189.0000 - accuracy: 0.4939 - precision: 0.5009 - recall: 0.5873 - auc: 0.4932 - val_loss: 0.6925 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5449\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.56436\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6940 - tp: 271.0000 - fp: 271.0000 - tn: 174.0000 - fn: 187.0000 - accuracy: 0.4928 - precision: 0.5000 - recall: 0.5917 - auc: 0.5026 - val_loss: 0.6925 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5534\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.56436\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6922 - tp: 262.0000 - fp: 233.0000 - tn: 212.0000 - fn: 196.0000 - accuracy: 0.5249 - precision: 0.5293 - recall: 0.5721 - auc: 0.5317 - val_loss: 0.6924 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5674\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.56436\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6898 - tp: 299.0000 - fp: 254.0000 - tn: 191.0000 - fn: 159.0000 - accuracy: 0.5426 - precision: 0.5407 - recall: 0.6528 - auc: 0.5456 - val_loss: 0.6923 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5691\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.56436\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6920 - tp: 273.0000 - fp: 241.0000 - tn: 204.0000 - fn: 185.0000 - accuracy: 0.5282 - precision: 0.5311 - recall: 0.5961 - auc: 0.5177 - val_loss: 0.6924 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5691\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.56436\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6900 - tp: 269.0000 - fp: 241.0000 - tn: 204.0000 - fn: 189.0000 - accuracy: 0.5238 - precision: 0.5275 - recall: 0.5873 - auc: 0.5370 - val_loss: 0.6923 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5532\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.56436\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6960 - tp: 254.0000 - fp: 238.0000 - tn: 207.0000 - fn: 204.0000 - accuracy: 0.5105 - precision: 0.5163 - recall: 0.5546 - auc: 0.5025 - val_loss: 0.6923 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5532\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.56436\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6936 - tp: 272.0000 - fp: 250.0000 - tn: 195.0000 - fn: 186.0000 - accuracy: 0.5172 - precision: 0.5211 - recall: 0.5939 - auc: 0.5244 - val_loss: 0.6923 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5764\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.56436\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6940 - tp: 258.0000 - fp: 247.0000 - tn: 198.0000 - fn: 200.0000 - accuracy: 0.5050 - precision: 0.5109 - recall: 0.5633 - auc: 0.5053 - val_loss: 0.6923 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5764\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.56436\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6924 - tp: 279.0000 - fp: 250.0000 - tn: 195.0000 - fn: 179.0000 - accuracy: 0.5249 - precision: 0.5274 - recall: 0.6092 - auc: 0.5307 - val_loss: 0.6922 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5768\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.56436\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6962 - tp: 245.0000 - fp: 247.0000 - tn: 198.0000 - fn: 213.0000 - accuracy: 0.4906 - precision: 0.4980 - recall: 0.5349 - auc: 0.4998 - val_loss: 0.6921 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5404\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.56436\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6950 - tp: 267.0000 - fp: 242.0000 - tn: 203.0000 - fn: 191.0000 - accuracy: 0.5205 - precision: 0.5246 - recall: 0.5830 - auc: 0.5141 - val_loss: 0.6920 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5390\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.56436\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6935 - tp: 265.0000 - fp: 240.0000 - tn: 205.0000 - fn: 193.0000 - accuracy: 0.5205 - precision: 0.5248 - recall: 0.5786 - auc: 0.5044 - val_loss: 0.6921 - val_tp: 10.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 37.0000 - val_accuracy: 0.4752 - val_precision: 0.3846 - val_recall: 0.2128 - val_auc: 0.5402\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.56436\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6913 - tp: 263.0000 - fp: 232.0000 - tn: 213.0000 - fn: 195.0000 - accuracy: 0.5271 - precision: 0.5313 - recall: 0.5742 - auc: 0.5272 - val_loss: 0.6921 - val_tp: 11.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 36.0000 - val_accuracy: 0.4851 - val_precision: 0.4074 - val_recall: 0.2340 - val_auc: 0.5398\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.56436\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6932 - tp: 265.0000 - fp: 247.0000 - tn: 198.0000 - fn: 193.0000 - accuracy: 0.5127 - precision: 0.5176 - recall: 0.5786 - auc: 0.5282 - val_loss: 0.6921 - val_tp: 10.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 37.0000 - val_accuracy: 0.4752 - val_precision: 0.3846 - val_recall: 0.2128 - val_auc: 0.5390\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.56436\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6915 - tp: 272.0000 - fp: 237.0000 - tn: 208.0000 - fn: 186.0000 - accuracy: 0.5316 - precision: 0.5344 - recall: 0.5939 - auc: 0.5299 - val_loss: 0.6922 - val_tp: 12.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 35.0000 - val_accuracy: 0.4554 - val_precision: 0.3750 - val_recall: 0.2553 - val_auc: 0.5402\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.56436\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6940 - tp: 257.0000 - fp: 261.0000 - tn: 184.0000 - fn: 201.0000 - accuracy: 0.4884 - precision: 0.4961 - recall: 0.5611 - auc: 0.4908 - val_loss: 0.6922 - val_tp: 13.0000 - val_fp: 20.0000 - val_tn: 34.0000 - val_fn: 34.0000 - val_accuracy: 0.4653 - val_precision: 0.3939 - val_recall: 0.2766 - val_auc: 0.5329\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.56436\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6935 - tp: 268.0000 - fp: 242.0000 - tn: 203.0000 - fn: 190.0000 - accuracy: 0.5216 - precision: 0.5255 - recall: 0.5852 - auc: 0.5028 - val_loss: 0.6922 - val_tp: 11.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 36.0000 - val_accuracy: 0.4851 - val_precision: 0.4074 - val_recall: 0.2340 - val_auc: 0.5317\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.56436\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6958 - tp: 237.0000 - fp: 260.0000 - tn: 185.0000 - fn: 221.0000 - accuracy: 0.4673 - precision: 0.4769 - recall: 0.5175 - auc: 0.4754 - val_loss: 0.6920 - val_tp: 11.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 36.0000 - val_accuracy: 0.4851 - val_precision: 0.4074 - val_recall: 0.2340 - val_auc: 0.5528\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.56436\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6944 - tp: 250.0000 - fp: 234.0000 - tn: 211.0000 - fn: 208.0000 - accuracy: 0.5105 - precision: 0.5165 - recall: 0.5459 - auc: 0.5004 - val_loss: 0.6919 - val_tp: 11.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 36.0000 - val_accuracy: 0.4851 - val_precision: 0.4074 - val_recall: 0.2340 - val_auc: 0.5528\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.56436\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6943 - tp: 267.0000 - fp: 254.0000 - tn: 191.0000 - fn: 191.0000 - accuracy: 0.5072 - precision: 0.5125 - recall: 0.5830 - auc: 0.5124 - val_loss: 0.6920 - val_tp: 11.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 36.0000 - val_accuracy: 0.4851 - val_precision: 0.4074 - val_recall: 0.2340 - val_auc: 0.5449\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.56436\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6932 - tp: 244.0000 - fp: 254.0000 - tn: 191.0000 - fn: 214.0000 - accuracy: 0.4817 - precision: 0.4900 - recall: 0.5328 - auc: 0.4910 - val_loss: 0.6919 - val_tp: 11.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 36.0000 - val_accuracy: 0.4851 - val_precision: 0.4074 - val_recall: 0.2340 - val_auc: 0.5504\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.56436\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6929 - tp: 250.0000 - fp: 243.0000 - tn: 202.0000 - fn: 208.0000 - accuracy: 0.5006 - precision: 0.5071 - recall: 0.5459 - auc: 0.5080 - val_loss: 0.6919 - val_tp: 10.0000 - val_fp: 11.0000 - val_tn: 43.0000 - val_fn: 37.0000 - val_accuracy: 0.5248 - val_precision: 0.4762 - val_recall: 0.2128 - val_auc: 0.5583\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.56436\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6944 - tp: 243.0000 - fp: 239.0000 - tn: 206.0000 - fn: 215.0000 - accuracy: 0.4972 - precision: 0.5041 - recall: 0.5306 - auc: 0.5049 - val_loss: 0.6919 - val_tp: 11.0000 - val_fp: 16.0000 - val_tn: 38.0000 - val_fn: 36.0000 - val_accuracy: 0.4851 - val_precision: 0.4074 - val_recall: 0.2340 - val_auc: 0.5504\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.56436\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6940 - tp: 254.0000 - fp: 248.0000 - tn: 197.0000 - fn: 204.0000 - accuracy: 0.4994 - precision: 0.5060 - recall: 0.5546 - auc: 0.5260 - val_loss: 0.6918 - val_tp: 10.0000 - val_fp: 11.0000 - val_tn: 43.0000 - val_fn: 37.0000 - val_accuracy: 0.5248 - val_precision: 0.4762 - val_recall: 0.2128 - val_auc: 0.5522\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.56436\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6948 - tp: 253.0000 - fp: 232.0000 - tn: 213.0000 - fn: 205.0000 - accuracy: 0.5161 - precision: 0.5216 - recall: 0.5524 - auc: 0.5121 - val_loss: 0.6919 - val_tp: 10.0000 - val_fp: 11.0000 - val_tn: 43.0000 - val_fn: 37.0000 - val_accuracy: 0.5248 - val_precision: 0.4762 - val_recall: 0.2128 - val_auc: 0.5449\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.56436\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6962 - tp: 243.0000 - fp: 248.0000 - tn: 197.0000 - fn: 215.0000 - accuracy: 0.4873 - precision: 0.4949 - recall: 0.5306 - auc: 0.4832 - val_loss: 0.6919 - val_tp: 10.0000 - val_fp: 11.0000 - val_tn: 43.0000 - val_fn: 37.0000 - val_accuracy: 0.5248 - val_precision: 0.4762 - val_recall: 0.2128 - val_auc: 0.5524\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.56436\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6909 - tp: 257.0000 - fp: 254.0000 - tn: 191.0000 - fn: 201.0000 - accuracy: 0.4961 - precision: 0.5029 - recall: 0.5611 - auc: 0.5142 - val_loss: 0.6918 - val_tp: 10.0000 - val_fp: 11.0000 - val_tn: 43.0000 - val_fn: 37.0000 - val_accuracy: 0.5248 - val_precision: 0.4762 - val_recall: 0.2128 - val_auc: 0.5522\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.56436\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6949 - tp: 253.0000 - fp: 250.0000 - tn: 195.0000 - fn: 205.0000 - accuracy: 0.4961 - precision: 0.5030 - recall: 0.5524 - auc: 0.4918 - val_loss: 0.6918 - val_tp: 10.0000 - val_fp: 11.0000 - val_tn: 43.0000 - val_fn: 37.0000 - val_accuracy: 0.5248 - val_precision: 0.4762 - val_recall: 0.2128 - val_auc: 0.5449\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.56436\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6940 - tp: 237.0000 - fp: 240.0000 - tn: 205.0000 - fn: 221.0000 - accuracy: 0.4895 - precision: 0.4969 - recall: 0.5175 - auc: 0.4879 - val_loss: 0.6919 - val_tp: 10.0000 - val_fp: 11.0000 - val_tn: 43.0000 - val_fn: 37.0000 - val_accuracy: 0.5248 - val_precision: 0.4762 - val_recall: 0.2128 - val_auc: 0.5164\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.56436\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6913 - tp: 272.0000 - fp: 252.0000 - tn: 193.0000 - fn: 186.0000 - accuracy: 0.5150 - precision: 0.5191 - recall: 0.5939 - auc: 0.5222 - val_loss: 0.6918 - val_tp: 10.0000 - val_fp: 11.0000 - val_tn: 43.0000 - val_fn: 37.0000 - val_accuracy: 0.5248 - val_precision: 0.4762 - val_recall: 0.2128 - val_auc: 0.5429\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.56436\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6950 - tp: 241.0000 - fp: 258.0000 - tn: 187.0000 - fn: 217.0000 - accuracy: 0.4740 - precision: 0.4830 - recall: 0.5262 - auc: 0.4705 - val_loss: 0.6918 - val_tp: 10.0000 - val_fp: 8.0000 - val_tn: 46.0000 - val_fn: 37.0000 - val_accuracy: 0.5545 - val_precision: 0.5556 - val_recall: 0.2128 - val_auc: 0.4758\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.56436\n",
            "Best val_accuracy at best epoch 1 was 0.5643564462661743 \n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 21)                924       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 21)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                220       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 1,155\n",
            "Trainable params: 1,155\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H5VHOzGHyTc"
      },
      "source": [
        "### Algunas preguntas sobre este modelo :\n",
        "#### 1) Si la capa de entrada tiene 43 neuronas (43 variables de entrada), ¿ Cuantás neuronas tiene la **SEGUNDA** capa densa intermedia?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-FB8DFoHyTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6835d180-01d7-4464-8f75-1fc171a57e22"
      },
      "source": [
        "n_neuronas_2mid_layer_2 = ?\n",
        "print('La SEGUNDA capa intermedia tiene {} neuronas'.format(n_neuronas_2mid_layer_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-bc5cc743c910>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    n_neuronas_2mid_layer_2 = ?\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygwVtnP9HyTu"
      },
      "source": [
        "#### 2) Dado que la **SEGUNDA** capa intermedia es densa, ¿Cuantos pesos tenemos que ajustar aproximadamente?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uD9PFEnHyTw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "8ece41b8-53c6-452d-f781-ae28e6549a72"
      },
      "source": [
        "n_pesos_2mid_layer_2 = ?\n",
        "print('La capa intermedia tiene aproximadamente {} pesos por optimizar'.format(n_pesos_2mid_layer_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-1900f9c98096>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    n_pesos_2mid_layer_2 = ?\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPs9uHBuHyT1"
      },
      "source": [
        "#### 3) La salida obviamente tiene solo una neurona, pero también está densamente conectada con la capa capa anterior (**SEGUNDA** capa intermedia), ¿Cuantos pesos tenemos que optimizar en esta capa para este modelo? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pwcL5i6HyT2"
      },
      "source": [
        "n_pesos_last_layer_2 = ?\n",
        "print('La capa final tiene {} pesos por optimizar'.format(n_pesos_last_layer_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtnLtZSAJhSz"
      },
      "source": [
        "Nuevamente graficamos la historia de la _loss_ de entrenamiento y validación para este modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1cibiHzwAKH"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plot_metric(histories[1],'accuracy','max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gixWKJuNKCP4"
      },
      "source": [
        "Ahora vamos a evaluar el modelo final sobre el `train` y `test` set para ver como se comportan sus medidas al momento de generalizar. Además veremos su matríz de confusión "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE2SPQEy8WxT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "047aa45b-ee7f-4b88-a313-a3f648011295"
      },
      "source": [
        "test_predictions = model.predict(X_test, batch_size=batch_size)\n",
        "\n",
        "print('Train results')\n",
        "print()\n",
        "results = model.evaluate(X_train, train_y,batch_size=batch_size, verbose=0)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(name, ': ', value)\n",
        "  if name == 'loss':\n",
        "    train_loss.append(value)\n",
        "  elif name == 'precision':\n",
        "    train_pres.append(value)\n",
        "  elif name == 'recall':\n",
        "    train_recs.append(value)\n",
        "  elif name == 'accuracy': \n",
        "    train_accs.append(value)\n",
        "print()\n",
        "\n",
        "print('Test results')\n",
        "print()\n",
        "results = model.evaluate(X_test, test_y,batch_size=batch_size, verbose=0)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(name, ': ', value)\n",
        "  if name == 'loss':\n",
        "    test_loss.append(value)\n",
        "  elif name == 'precision':\n",
        "    test_pres.append(value)\n",
        "  elif name == 'recall':\n",
        "    test_recs.append(value)\n",
        "  elif name == 'accuracy': \n",
        "    test_accs.append(value)\n",
        "print()\n",
        "\n",
        "print('Confusion matrix')\n",
        "print()\n",
        "plot_cm(test_y, test_predictions)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train results\n",
            "\n",
            "loss :  0.694230854511261\n",
            "tp :  423.0\n",
            "fp :  360.0\n",
            "tn :  85.0\n",
            "fn :  35.0\n",
            "accuracy :  0.5625692009925842\n",
            "precision :  0.540229856967926\n",
            "recall :  0.9235807657241821\n",
            "auc :  0.5194592475891113\n",
            "\n",
            "Test results\n",
            "\n",
            "loss :  0.6988270878791809\n",
            "tp :  49.0\n",
            "fp :  50.0\n",
            "tn :  9.0\n",
            "fn :  4.0\n",
            "accuracy :  0.5178571343421936\n",
            "precision :  0.49494948983192444\n",
            "recall :  0.9245283007621765\n",
            "auc :  0.494403600692749\n",
            "\n",
            "Confusion matrix\n",
            "\n",
            "True Negatives:  9\n",
            "False Positives:  50\n",
            "False Negatives:  4\n",
            "True Positives:  49\n",
            "Total :  53\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUcBIGcsMPZQ"
      },
      "source": [
        "#### Observando esta matriz, ¿Cual es la _recall_ obtenida por este modelo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiE92F57MPZR"
      },
      "source": [
        "positivos_encontrados_2 = ?\n",
        "print('Positivos encontrados {} % '.format(positivos_encontrados_2 * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URVRTHJCMPZX"
      },
      "source": [
        "#### ¿Es este modelo mejor para capturar casos positivos?\n",
        "(use `True` o `False` para esta respuesta) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_OOBZvTMPZY"
      },
      "source": [
        "recall_2_better_1 = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll1WEImrMPZd"
      },
      "source": [
        "#### ¿Cual es la precisión de este modelo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUO-PW5gMPZf"
      },
      "source": [
        "positivos_correctos_2 = ?\n",
        "print('Positivos correctos {} % '.format(positivos_correctos_2 * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sbbr15fMPZk"
      },
      "source": [
        "#### Observando la precisión, ¿Cual es la contra de tener un modelo bueno para capturar casos positivos?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTs285XGMPZl"
      },
      "source": [
        "ventajas_desventajas_2 = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfhjeL7XHNtk"
      },
      "source": [
        "Probamos con un modelo más profundo aun, de 4 capas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijGG9DgQHTv0"
      },
      "source": [
        "layers = [1/2,1/4,1/8,1/16]\n",
        "model,history = get_nnet_model(X_train,train_y,X_val,val_y,None,layers,epochs,batch_size,early_stop_patience,monitor_metric,monitor_mode)\n",
        "\n",
        "n_layers.append(len(layers))\n",
        "models.append(model)\n",
        "histories.append(history)\n",
        "\n",
        "best_epoch,best_metric = get_best_epoch(history,monitor_metric,monitor_mode)\n",
        "best_epochs.append(best_epoch)\n",
        "best_metrics.append(best_metric)\n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekAM6dvzTZWO"
      },
      "source": [
        "Graficamos la evolución del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qu_uCjLIZyg"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plot_metric(histories[2],'accuracy','max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLGDmz_UUL-"
      },
      "source": [
        "Evaluamos este nuevo modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2cPs-XqIlks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "ecf52855-8bdf-4874-c43c-b8cb47135ddb"
      },
      "source": [
        "test_predictions = model.predict(X_test, batch_size=batch_size)\n",
        "\n",
        "print('Train results')\n",
        "print()\n",
        "results = model.evaluate(X_train, train_y,batch_size=batch_size, verbose=0)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(name, ': ', value)\n",
        "  if name == 'loss':\n",
        "    train_loss.append(value)\n",
        "  elif name == 'precision':\n",
        "    train_pres.append(value)\n",
        "  elif name == 'recall':\n",
        "    train_recs.append(value)\n",
        "  elif name == 'accuracy': \n",
        "    train_accs.append(value)\n",
        "print()\n",
        "\n",
        "print('Test results')\n",
        "print()\n",
        "results = model.evaluate(X_test, test_y,batch_size=batch_size, verbose=0)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(name, ': ', value)\n",
        "  if name == 'loss':\n",
        "    test_loss.append(value)\n",
        "  elif name == 'precision':\n",
        "    test_pres.append(value)\n",
        "  elif name == 'recall':\n",
        "    test_recs.append(value)\n",
        "  elif name == 'accuracy': \n",
        "    test_accs.append(value)\n",
        "print()\n",
        "\n",
        "print('Confusion matrix')\n",
        "print()\n",
        "plot_cm(test_y, test_predictions)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train results\n",
            "\n",
            "loss :  0.694230854511261\n",
            "tp :  423.0\n",
            "fp :  360.0\n",
            "tn :  85.0\n",
            "fn :  35.0\n",
            "accuracy :  0.5625692009925842\n",
            "precision :  0.540229856967926\n",
            "recall :  0.9235807657241821\n",
            "auc :  0.5194592475891113\n",
            "\n",
            "Test results\n",
            "\n",
            "loss :  0.6988270878791809\n",
            "tp :  49.0\n",
            "fp :  50.0\n",
            "tn :  9.0\n",
            "fn :  4.0\n",
            "accuracy :  0.5178571343421936\n",
            "precision :  0.49494948983192444\n",
            "recall :  0.9245283007621765\n",
            "auc :  0.494403600692749\n",
            "\n",
            "Confusion matrix\n",
            "\n",
            "True Negatives:  9\n",
            "False Positives:  50\n",
            "False Negatives:  4\n",
            "True Positives:  49\n",
            "Total :  53\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAFNCAYAAAB/p8gbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdX0lEQVR4nO3dd5xddZ3/8dc7CZBQEtIJTUCCFAus/FiaLH0pugFFFJFfXIEorCDFFUQXacsCFqRZQo2gQFzQIEoTQcLSEopAQpXiAgmBFEihJDOf/eN8J1yGmXvv3Jw799457yeP85h7T/mez8wwn3y+3+855yoiMDPr6/o1OgAzs97gZGdmheBkZ2aF4GRnZoXgZGdmheBkZ2aF4GRnZoXgZNeEJA2S9HtJb0j6zQq0c7CkW/OMrVEkfUrSU42Ow1qXk90KkPQlSdMlLZI0S9JNknbMoekDgNHA8Ij4fK2NRMSvImLPHOKpK0khaeNy+0TE1Ij4yAqeZ8/0j8hsSa9JulvSVyX167TfMEm/lbRY0ouSvlSmzVMkLU3/D3QsG5Vs31LSg5KWpK9brsj3YLVzsquRpOOAnwBnkiWm9YGfAuNyaP5DwNMRsSyHtlqepAE5tHEO2e/qEmBTYC3gG8CuwI2SVinZ/SLgXbLf68HAzyRtUab5ayNi9ZLluXTOlYEpwFXAUGASMCWtt94WEV56uABDgEXA58vsswpZMnwlLT8BVknbdgZeAo4H5gCzgH9N204l+0Nbms5xKHAKcFVJ2xsAAQxI778CPAcsBJ4HDi5Zf3fJcdsD04A30tftS7bdCZwO/E9q51ZgRDffW0f83y6Jfz9gH+BpYB5wUsn+2wD3AgvSvhcCK6dtd6XvZXH6fr9Q0v4JwGzgyo516ZgPp3P8Q3q/NvAasHM38f7/9P2s0s32HwAnp9erpZ//JiXbrwTO6ubY9/1uOm3bE3gZUMm6vwN7Nfr/4SIuDQ+gFRdgL2BZR7LpZp/TgPuAUcBI4B7g9LRt53T8acBKKUksAYam7Z2TW7fJLv1xvgl8JG0bA2yRXi9PdsAwYD5wSDruoPR+eNp+J/A3YBNgUHrf3R94R/wnp/gPT8nm18AawBbAW8CGaf9PAtum824APAEcU9JeABt30f7ZZP9oDCpNdmmfw4GZwKrALcAPy/wungHWS6/PJkugDwHnpp/HIOBvaftWwJJOx38L+H03bZ9C9o/HPGAGcETJtmOBmzrtfyNwfKP/Hy7i4m5sbYYDr0f5bubBwGkRMSciXiOr2A4p2b40bV8aEX8kq2pqHZNqBz4qaVBEzIqIGV3ssy/wTERcGRHLIuJq4EngMyX7XB4RT0fEW8BkoNz40lLgPyNiKXANMAI4LyIWpvPPBD4BEBEPRsR96bwvAL8A/qmK7+n7EfFOiud9IuJi4FngfrIE/92uGkljga9ExP9K2hvYG/g42T9YuwH9U/vzJI0AVif7x6PUG2RJvCuTgc3I/kE7HDhZ0kFp2+rp2GrbsjpysqvNXGBEhbGktYEXS96/mNYtb6NTslxC9sfRIxGxmKzr93VglqQ/SNq0ing6Ylqn5P3sHsQzNyLa0uuOZPRqyfa3Oo6XtImkG9PEwJtkY2cjyrQN8FpEvF1hn4uBjwIXRMQ73ewziqwrCfAx4Ob0D9Ac4OYUXz+yMbV5ZP/oDO7UxmCyrv0HRMTMiHglItoi4h7gPLIJJnraltWXk11t7gXeIRun6s4rZBMNHdZP62qxmKy71mGt0o0RcUtE7EFW4TxJlgQqxdMR08td7Ju3n5HFNTYiBgMnAapwTNlnj0lanWwc9FLgFEnDutn1dbKfC8BjwD9LGiVpFFl1txrwX8AfI6KdbMxxgKSxJW18gqyLWo3gve9tBvBxSaXf68d70JblyMmuBhHxBtl41UWS9pO0qqSVJO2dZv0Arga+J2lk6h6dTDYrV4tHgJ0krS9pCPCdjg2SRksaJ2k1sgS8iKwL2NkfgU3S5TIDJH0B2JxsDKne1iDrGi5KVecRnba/Cmz0gaPKOw+YHhGHAX8Aft7VThHxNLCepDERcRNZNfdX4AayyZEjyCqtb6X9FwPXA6dJWk3SDmQz7Fd21X762Q9VZhvgaLIZWMjGPduAoyWtIukbaf2fe/i9Wh4aPWjYygvZuNx0ssprNtkf3fZp20DgfLLZx1np9cC0bWdKBtvTuheA3dPrU+g0w0d2OcQCsnGqw3lvgmIM8BeysaAFZH9gm6djvsL7Z2N3BB5M+z4I7Fiy7U7gsJL37zu2Uyzviz/FEcAGJevuBr6cXu9EVtktAqaSTcyUxvX19DNaABzYzc9n+Tqy5PMyMCy9Xz39XA7uJt4J6XfzgQmlbtYNA36Xfq9/B75Usu1TwKKS91eTDWssSt/j0Z3a2ir9rN8imxTZqtH/3xZ1UfqFmPVpki4k646eTDYM0Y/s0pAzgH0jovN4pvUxTnZWGJL2B/6NNEtMdjnQ2ZFNLFgTk/QC2XBDG7AsIrZO47TXkl3O9AJwYETM77YNJzsza3Yp2W0dEa+XrDsHmBcRZ0k6kew61RO6a8MTFGbWqsaR3YJH+lru6ggnOzNrCQHcmh6mMCGtGx0Rs9Lr2WT3MndrhW+wrpfNRm3j/nWLenTmNY0OwVbASiM2qnQNZJeWvv5cTX+zK4/88NfIZsw7TIyIiZ122zEiXk7XR94m6cnSjRERksqev2mTnZkVQ0psnZNb531eTl/nSPot2cMlXk3XT86SNIbsoRTdcjfWzPLR3lbbUkG6uHuNjtdklww9TnZh+Pi023jeu5i7S67szCwf0dWNO7kYDfw23XU3APh1RNwsaRowWdKhZPd5H1iuESc7M8tHe32SXWQPQ/1EF+vnkj25pipOdmaWi6hfZZcLJzszy0edKru8ONmZWT5c2ZlZIVQxs9pITnZmlg9XdmZWCB6zM7Mi8GysmRWDKzszKwRXdmZWCJ6NNbNCcGVnZoXgMTszK4Qmr+z8PDszKwRXdmaWD3djzawIIjwba2ZF0ORjdk52ZpYPd2PNrBBc2ZlZIfgOCjMrBFd2ZlYIHrMzs0JwZWdmheDKzswKwcnOzIrAd1CYWTG4sjOzQvAEhZkVgis7MyuEJq/s/PBOMysEV3Zmlg93Y82sEJq8G+tkZ2b5cGVnZoXgZGdmheBurJkVgis7MysEV3ZmVgiu7MysEFzZmVkhuLIzs0JwsjOzQohodARlOdmZWT5c2ZlZITjZmVkheDbWzAqhySs7P7zTzArBlZ2Z5cOzsWZWCO7GmlkhtLfXtlRBUn9JD0u6Mb3fUNL9kp6VdK2klSu14WRnZvmI9tqW6nwTeKLk/dnAuRGxMTAfOLRSA052ZpaLaI+alkokrQvsC1yS3gvYFfjvtMskYL9K7XjMzszyUb8xu58A3wbWSO+HAwsiYll6/xKwTqVGXNmZWT5q7MZKmiBpeskyoaNJSZ8G5kTEgysanis7M8tHFV3SrkTERGBiN5t3AP5F0j7AQGAwcB6wpqQBqbpbF3i50nlc2ZlZPuowGxsR34mIdSNiA+CLwJ8j4mDgDuCAtNt4YEql8JzszCwfdbz0pAsnAMdJepZsDO/SSge4G9vLDjn8C3z+y/shid9c9Tt+OfGaRodkFez5ufGstuqq9OvXj/79+zP5svN5482FHP8f/8Urs19l7bVG86PTv8OQwWtUbqwvq/MdFBFxJ3Bnev0csE1Pjney60VjN92Iz395Pw7c6yssfXcZF197Hnfedjd/f/6lRodmFVx2wVkMXXPI8veXXDmZbbfeksMOOZBLrpzMpVdN5rgjK17q1bcV9Q4KSZtKOkHS+Wk5QdJm9TpfK9ho7IY8+tAM3n7rHdra2ph2z0Psse8ujQ7LanDH1HsZt/fuAIzbe3f+fNe9DY6oCbRHbUsvqUuyk3QCcA0g4IG0CLha0on1OGcreObJv/HJbbdkzaFDGDhoFXbafQfWWnt0o8OyCiQx4djvcuBXj+I3U/4IwNz5Cxg5YhgAI4YPZe78BY0MsTnU9w6KFVavbuyhwBYRsbR0paQfAzOAs+p03qb23DMvcMkFv+SSyefz1pK3efLxp2lva2t0WFbBL3/2Q0aPHMHc+Qs4/JiT2PBD671vuySyi/oLrhertFrUqxvbDqzdxfoxaVuXSi8uXPDWnDqF1ljX/foGDthjPIeM+xpvLHiTF577e6NDsgpGjxwBwPCha7LbTtvz2MynGD50TV57fR4Ar70+j2El43lFFe3tNS29pV7J7hjgdkk3SZqYlpuB28lu6O1SREyMiK0jYus1B42qU2iNNWzEUADGrDOaPfbdhRuvu6XBEVk5S956m8WLlyx/fc8DDzF2ow3YecdtmXLTnwCYctOf2OVT2zUyTKtCXbqxEXGzpE3IpoY77ll7GZgWEYXut5132dmsOXQwy5a1cfqJP2Dhm4saHZKVMXfefL550ukAtC1rY589d2bHbbfmo5ttwvH/cSbX33gLa681ih+dflKDI20CTd6NVTTp00U3G7VNcwZmFT0609cOtrKVRmxU0wDk4jO+XNPf7Grfu6pXBjx9nZ2Z5aPJKzsnOzPLR5NfVOxkZ2b5cGVnZoXgD8k2s0JwZWdmRdCbFwjXwsnOzPLhys7MCsHJzswKwRMUZlYIruzMrAiq+cDrRnKyM7N8ONmZWSH40hMzKwRXdmZWCE2e7Pwh2WZWCK7szCwXzfog4A5OdmaWjybvxjrZmVk+nOzMrAh8UbGZFYOTnZkVQnNfU+xkZ2b5cDfWzIrByc7MCsHdWDMrAndjzawYXNmZWRG4sjOzYnBlZ2ZF0OSft+NkZ2Y5cbIzsyJo9srOD+80s0JwZWdm+Wjyys7Jzsxy0ezdWCc7M8uFk52ZFULLJjtJC4GOS6KVvkZ6HRExuM6xmVkrCVXep4G6TXYRsUZvBmJmra1lK7tSknYExkbE5ZJGAGtExPP1Dc3MWkm0t2hl10HS94GtgY8AlwMrA1cBO9Q3NDNrJX2hstsf2Ap4CCAiXpHkLq6ZvU80+ZhdNXdQvBvZR30HgKTV6huSmbWiaK9tqUTSQEkPSPqrpBmSTk3rN5R0v6RnJV0raeVy7VST7CZL+gWwpqTDgT8BF1dxnJkVSLSrpqUK7wC7RsQngC2BvSRtC5wNnBsRGwPzgUPLNVIx2UXED4H/Bq4DNgFOjogLqonQzIojoralcrsREbEovV0pLQHsSpabACYB+5Vrp9qLih8DBqUTPFblMWZWIPWcjZXUH3gQ2Bi4CPgbsCAilqVdXgLWKddGxcpO0mHAA8BngQOA+yR9dQXiNrM+qNZurKQJkqaXLBM+0HZEW0RsCawLbANs2tP4qqns/h3YKiLmAkgaDtwDXNbTk5lZ31VNl7Tr42IiMLHKfRdIugPYjmweYUCq7tYFXi53bDUTFHOBhSXvF6Z1ZmbL1WuCQtJISWum14OAPYAngDvIepsA44Ep5dopd2/scenls8D9kqaQjdmNAx6tGKGZWT7GAJPSuF0/YHJE3ChpJnCNpDOAh4FLyzVSrhvbceHw39LSoWz2NLNiqtdFxRHxKNmNDZ3XP0c2fleVcg8COLW20MysiFr+djFJI4FvA1sAAzvWR8SudYzLzFpMex+4XexXwJPAhsCpwAvAtDrGZGYtKEI1Lb2lmmQ3PCIuBZZGxF8i4qtkVy6bmS1Xx9vFclHNdXZL09dZkvYFXgGG1S8kM2tFtV5n11uqSXZnSBoCHA9cAAwGjq1rVGbWclr+4Z0RcWN6+QawS33DMbNW1ewTFOUuKr6A9z5w5wMi4ui6RGRmLanZH95ZrrKb3mtRmFnLa9kxu4iY1JuBmFlra9lurJlZT7RyN9bMrGot241ttGcWlH00lTWxeHtxo0OwBmjZbqxnY82sJ1q5G+vZWDOrWstWdp6NNbO+pNpHPJ0AbI4f8WRm3Wjy+YmqH/H0BH7Ek5mV0R6qaektfsSTmeWi2Z9n50c8mVkumvyp7H7Ek5nlI2jR2dgOfsSTmVWjvclnKKqZjb2cLiZa0tidmRkA7a1e2QE3lrweCOxPNm5nZrZcX+jGXlf6XtLVwN11i8jMWlJfmKDobCwwKu9AzKy1tXxlJ2kh7x+zm012R4WZ2XItX9lFxBq9EYiZtbZmT3YV76CQdHs168ys2ALVtPSWcs+zGwisCoyQNBSWRzUYWKcXYjOzFtLkHxtbthv7NeAYYG3gQd5Ldm8CF9Y5LjNrMS17nV1EnAecJ+moiLigF2MysxbU5DdQVPXUk3ZJa3a8kTRU0pF1jMnMLHfVJLvDI2JBx5uImA8cXr+QzKwVtde49JZqLiruL0kR2QelSeoPrFzfsMys1bSrRcfsStwMXCvpF+n919I6M7Plmn3MrppkdwIwATgivb8NuLhuEZlZS2r5i4ojoj0ifh4RB0TEAcBMsod4mpkt167alt5S1YMAJG0FHAQcCDwPXF/PoMys9bTsdXaSNiFLcAcBrwPXAooIP63YzD6glcfsngSmAp+OiGcBJPmzJ8ysS81+u1i5MbvPArOAOyRdLGk3aPI61cwaptmvs+s22UXE7yLii8CmwB1k98mOkvQzSXv2VoBm1hqixqW3VDMbuzgifh0RnwHWBR7GD+80s06afTa2mtvFlouI+RExMSJ2q1dAZtaamr0bW8tnUJiZfUCzX1TsZGdmuYgmn750sjOzXLiyM7NCcLIzs0Jo9jsoejQba2bW2yStJ+kOSTMlzZD0zbR+mKTbJD2Tvg4t146TnZnloo7X2S0Djo+IzYFtgX+TtDlwInB7RIwFbk/vu+VkZ2a5qNd1dhExKyIeSq8XAk+QfZzrOGBS2m0SsF+5djxmZ2a56I0JCkkbAFsB9wOjI2JW2jQbGF3uWFd2ZpaLWu+NlTRB0vSSZUJX7UtaHbgOOCYi3nzfubPPyCk7R+LKzsxyUet9rhExEZhYbh9JK5Elul9FRMfDg1+VNCYiZkkaA8wp14YrOzPLRb3G7CQJuBR4IiJ+XLLpBmB8ej0emFKuHVd2ZpaLOl5ntwNwCPCYpEfSupOAs4DJkg4FXiT72IhuOdmZWS7a65TuIuJuun9wcNVPYHKyM7Nc+HYxMyuEZr9dzMnOzHLhys7MCqHZP13Myc7MclGvCYq8ONmZWS6aO9U52ZlZTjxmZ2aF0OzdWN8uZmaF4MrOzHLR3HWdk52Z5cRjdmZWCM0+ZudkZ2a5aO5U52RnZjlxN9bMCiGavLZzsjOzXLiyM7NC8ASFfUC/fv24/76beOXl2Yzbf3zlA6zh2tra+OKRJzBq+DAuOvMk7n/4MX7081+ydNkyNh+7Eaf++5EM6N+/0WE2VHOnOt9B0RBHH3UYTz75TKPDsB646vo/suH66wLQ3t7Od8++kHO+dyy/vfRcxoweyQ233NnYAJtAO1HT0luc7HrZOuuMYZ+9d+Oyy65udChWpdmvzWXq/Q/yuX2yjztY8OZCVhowgA3WWxuA7T75cW6bel8jQ2wK9fp0sbz0erKT9K+9fc5m8uMfncqJ3zmD9vZmH861DudcdDnHTjiEfsqeTjl0yGDa2tqY8dSzANx2133Mfm1uI0NsClHjf72lEZXdqQ04Z1PYd5/dmTPndR56+LFGh2JV+su90xk2dAhbbPLh5eskcc73juWcn17BQUeeyKqrDqJ/P3eSmr2yq8sEhaRHu9sEjC5z3ARgAoD6D6Ffv9XqEF3jbL/91nzm03uy9167MnDgKgwevAaTrjif8V85utGhWTcenvEUd9wzjan3P8Q77y5l8ZIlnHjmeZx10jeZdN4ZANwz/RFefOmVBkfaeM1+nZ0i8g9Q0qvAPwPzO28C7omItSu1MWDldZr7J7eC/mmn7Tju2K/3ydnYJc/d3OgQ6mLaI49zxeQbuOjMk5g7/w2GDx3Cu+8u5ciT/pPDD/4c/7jVxxodYi5WXvdjNX2axPgNPlfT3+ykF67rlU+vqNelJzcCq0fEI503SLqzTuc06zVXTJ7CX+57kGgPDvyXPftMolsR7XUonPJUl8ouD329suvL+mplVxS1VnaHfOizNf3NXvni9S1d2ZlZwTR7deJkZ2a58O1iZlYIzT4b62RnZrlo9svknezMLBfuxppZIbgba2aF4G6smRVCs16z28HJzsxy4TE7MysEd2PNrBA8QWFmheBurJkVgicozKwQPGZnZoXgMTszK4RmH7Pzp4SYWSG4sjOzXHiCwswKodm7sU52ZpYLT1CYWSE0+6eLOdmZWS6aO9U52ZlZTjxmZ2aF4GRnZoXQ7Jee+KJiM8tFO1HTUomkyyTNkfR4ybphkm6T9Ez6OrRSO052ZpaLqPG/KlwB7NVp3YnA7RExFrg9vS/Lyc7MchERNS1VtHsXMK/T6nHApPR6ErBfpXac7MwsF7V2YyVNkDS9ZJlQxelGR8Ss9Ho2MLrSAZ6gMLNc1DpBERETgYkrcN6QVPHkTnZmlotevvTkVUljImKWpDHAnEoHuBtrZrmo4wRFV24AxqfX44EplQ5wZWdmuajXvbGSrgZ2BkZIegn4PnAWMFnSocCLwIGV2nGyM7OmFhEHdbNpt56042RnZrnwI57MrBD8iCczKwRXdmZWCK7szKwQXNmZWSG4sjOzQnBlZ2aFENHe6BDKcrIzs1z4sexmVgjN/lh2Jzszy4UrOzMrBFd2ZlYIvvTEzArBl56YWSG4G2tmheAJCjMrhGav7PwZFGZWCK7szCwXno01s0Jo9m6sk52Z5cITFGZWCK7szKwQPGZnZoXgOyjMrBBc2ZlZIXjMzswKwd1YMysEV3ZmVghOdmZWCM2d6kDNno37KkkTImJio+Ow2vj313r81JPGmdDoAGyF+PfXYpzszKwQnOzMrBCc7BrH4z2tzb+/FuMJCjMrBFd2ZlYITnYNIGkvSU9JelbSiY2Ox6on6TJJcyQ93uhYrGec7HqZpP7ARcDewObAQZI2b2xU1gNXAHs1OgjrOSe73rcN8GxEPBcR7wLXAOMaHJNVKSLuAuY1Og7rOSe73rcO8L8l719K68ysjpzszKwQnOx638vAeiXv103rzKyOnOx63zRgrKQNJa0MfBG4ocExmfV5Tna9LCKWAd8AbgGeACZHxIzGRmXVknQ1cC/wEUkvSTq00TFZdXwHhZkVgis7MysEJzszKwQnOzMrBCc7MysEJzszKwQnuz5CUpukRyQ9Luk3klZdgbaukHRAen1JuQcVSNpZ0vY1nOMFSSOqXd9pn0U9PNcpkr7V0xitb3Gy6zveiogtI+KjwLvA10s3SqrpYzMj4rCImFlml52BHic7s97mZNc3TQU2TlXXVEk3ADMl9Zf0A0nTJD0q6WsAylyYnrH3J2BUR0OS7pS0dXq9l6SHJP1V0u2SNiBLqsemqvJTkkZKui6dY5qkHdKxwyXdKmmGpEsAVfomJP1O0oPpmAmdtp2b1t8uaWRa92FJN6djpkraNI8fpvUN/pDsPiZVcHsDN6dV/wB8NCKeTwnjjYj4f5JWAf5H0q3AVsBHyJ6vNxqYCVzWqd2RwMXATqmtYRExT9LPgUUR8cO036+BcyPibknrk90pshnwfeDuiDhN0r5ANXcefDWdYxAwTdJ1ETEXWA2YHhHHSjo5tf0Nss+F+HpEPCPpH4GfArvW8GO0PsjJru8YJOmR9HoqcClZ9/KBiHg+rd8T+HjHeBwwBBgL7ARcHRFtwCuS/txF+9sCd3W0FRHdPdNtd2BzaXnhNljS6ukcn03H/kHS/Cq+p6Ml7Z9er5dinQu0A9em9VcB16dzbA/8puTcq1RxDisIJ7u+462I2LJ0RfqjX1y6CjgqIm7ptN8+OcbRD9g2It7uIpaqSdqZLHFuFxFLJN0JDOxm90jnXdD5Z2DWwWN2xXILcISklQAkbSJpNeAu4AtpTG8MsEsXx94H7CRpw3TssLR+IbBGyX63Akd1vJHUkXzuAr6U1u0NDK0Q6xBgfkp0m5JVlh36AR3V6ZfIusdvAs9L+nw6hyR9osI5rECc7IrlErLxuIfSB8b8gqy6/y3wTNr2S7KnerxPRLwGTCDrMv6V97qRvwf275igAI4Gtk4TIDN5b1b4VLJkOYOsO/v3CrHeDAyQ9ARwFlmy7bAY2CZ9D7sCp6X1BwOHpvhm4MfdWwk/9cTMCsGVnZkVgpOdmRWCk52ZFYKTnZkVgpOdmRWCk52ZFYKTnZkVgpOdmRXC/wEp5lnaIXrMHAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ok7x71WamlC"
      },
      "source": [
        "#### ¿Que pasa si el modelo es más profundo aún?\n",
        "\n",
        "Implemente el codigo necesario para entrenar un modelo de 5 capas de manera de entendér que tenencias se ven al seguir complejizando las redes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NdBU7dBamlD"
      },
      "source": [
        "# 1 : construir y entrenar el modelo\n",
        "layers = [1/2,1/4,1/8,1/16,?]\n",
        "model,history = get_nnet_model(X_train,train_y,X_val,val_y,None,layers,epochs,batch_size,early_stop_patience,monitor_metric,monitor_mode)\n",
        "\n",
        "n_layers.append(len(layers))\n",
        "models.append(model)\n",
        "histories.append(history)\n",
        "\n",
        "best_epoch,best_metric = get_best_epoch(history,monitor_metric,monitor_mode)\n",
        "best_epochs.append(best_epoch)\n",
        "best_metrics.append(best_metric)\n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I69yzeZEcPGi"
      },
      "source": [
        "#### ¿Cual es la cantidad de parametros total de este nuevo modelo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1BiHrHbcVk1"
      },
      "source": [
        "total_pesos_4 = ?\n",
        "print('El total de pesos aproximados para este modelo es {}'.format(total_pesos_4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GWWKz9wciEM"
      },
      "source": [
        "Repetimos el segundo paso del flujo general, que es graficar la historia de entrenamiento para ver el epoch de corte. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax3fkeT8amlI"
      },
      "source": [
        "# 2 : plotear la loss del nuevo modelo \n",
        "plt.figure(figsize=(20,10))\n",
        "plot_metric(histories[?],'precision','max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRAIZGUXdC6V"
      },
      "source": [
        "#### ¿ Cual fué el mejor `epoch` y cual fué la `loss` en ese epoch?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NsBDMHddLef"
      },
      "source": [
        "best_epoch_4 = ?\n",
        "best_loss_4 = ?\n",
        "print('La mejor loss fue {} y se logró en el epoch {}'.format(best_loss_4,best_epoch_4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndWgRSe3amlL"
      },
      "source": [
        "# 3 : evaluación de metricas\n",
        "test_predictions = model.predict(X_test, batch_size=batch_size)\n",
        "\n",
        "print('Train results')\n",
        "print()\n",
        "results = model.evaluate(X_train, train_y,batch_size=batch_size, verbose=0)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(name, ': ', value)\n",
        "  if name == 'loss':\n",
        "    train_loss.append(value)\n",
        "  elif name == 'precision':\n",
        "    train_pres.append(value)\n",
        "  elif name == 'recall':\n",
        "    train_recs.append(value)\n",
        "  elif name == 'accuracy': \n",
        "    train_accs.append(value)\n",
        "print()\n",
        "\n",
        "print('Test results')\n",
        "print()\n",
        "results = model.evaluate(X_test, test_y,batch_size=batch_size, verbose=0)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(name, ': ', value)\n",
        "  if name == 'loss':\n",
        "    test_loss.append(value)\n",
        "  elif name == 'precision':\n",
        "    test_pres.append(value)\n",
        "  elif name == 'recall':\n",
        "    test_recs.append(value)\n",
        "  elif name == 'accuracy': \n",
        "    test_accs.append(value)\n",
        "print()\n",
        "\n",
        "print('Confusion matrix')\n",
        "print()\n",
        "plot_cm(test_y, test_predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhuuO1G9gNoX"
      },
      "source": [
        "#### Observando estos resultados, ¿Cual es la _recall_ obtenida por este modelo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke35M3XWgNoZ"
      },
      "source": [
        "positivos_encontrados_4 = ?\n",
        "print('Positivos encontrados {} % '.format(positivos_encontrados_4 * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVDpZWPrgNo5"
      },
      "source": [
        "#### ¿Cual es la precisión de este modelo?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ_koD2sgNo6"
      },
      "source": [
        "positivos_correctos_4 = ?\n",
        "print('Positivos correctos {} % '.format(positivos_correctos_2 * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHUn3JQ1O6B_"
      },
      "source": [
        "Vamos a construir una tabla con los resultados obtenidos para cada modelo y así poder comparar cual de ellos se comportó mejor para elegir uno y continuar el analisis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHblxbLYEV44"
      },
      "source": [
        "experiment_results = pd.DataFrame(\n",
        "  {\n",
        "    'n_layers' : n_layers,\n",
        "    'best_epochs' : best_epochs,\n",
        "    'best_metrics' : best_metrics,\n",
        "    'train_loss' : train_loss,\n",
        "    'test_loss' : test_loss,\n",
        "    'train_precs' : train_pres,\n",
        "    'test_precs' : test_pres,\n",
        "    'train_recs' : train_recs,\n",
        "    'test_recs' : test_recs,\n",
        "    'train_accs' : train_accs,\n",
        "    'test_accs' : test_accs\n",
        "  }\n",
        ")\n",
        "\n",
        "experiment_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS0SasbhPKSF"
      },
      "source": [
        "Ahora vamos a graficar algunas medidas de interés en función de la cantidad de capas del modelo. \n",
        "\n",
        "Nos vamos a enfocar en :\n",
        "* **Recall** : recordemos que es el porcentaje de casos positivos encontramos \n",
        "* **Precision** : recordemos que es la proporcion de los detectados como positivos, que lo es realmente\n",
        "* **Accuracy** : proporcion del total de ejemplos que se classifico correctamente ( `TP + TN / Total` ) \n",
        "* **Loss** : medida utilizada para ajustar el modelo, en este caso la [`Binary Cross Entropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzdJUsdLAj-"
      },
      "source": [
        "\n",
        "fig,axs = plt.subplots(2,2,figsize=(20,10))\n",
        "\n",
        "pre_ax = axs[0,0]\n",
        "sns.lineplot(data=experiment_results,x='n_layers',y='train_precs',label='Train Precision',ax=pre_ax)\n",
        "sns.lineplot(data=experiment_results,x='n_layers',y='test_precs',label='Test Precision',ax=pre_ax)\n",
        "pre_ax.set_ylabel('Precision')\n",
        "\n",
        "rec_ax = axs[0,1]\n",
        "sns.lineplot(data=experiment_results,x='n_layers',y='train_recs',label='Train Recall',ax = rec_ax)\n",
        "sns.lineplot(data=experiment_results,x='n_layers',y='test_recs',label='Test Recall',ax = rec_ax)\n",
        "rec_ax.set_ylabel('Recall')\n",
        "\n",
        "accs_ax = axs[1,0]\n",
        "sns.lineplot(data=experiment_results,x='n_layers',y='train_accs',label='Train Accuracy',ax = accs_ax )\n",
        "sns.lineplot(data=experiment_results,x='n_layers',y='test_accs',label='Test Accuracy',ax = accs_ax )\n",
        "accs_ax.set_ylabel('Accuracies')\n",
        "\n",
        "\n",
        "\n",
        "loss_ax = axs[1,1]\n",
        "sns.lineplot(data=experiment_results,x='n_layers',y='train_loss',label='Train Loss',ax = loss_ax)\n",
        "sns.lineplot(data=experiment_results,x='n_layers',y='test_loss',label='Test Loss',ax = loss_ax)\n",
        "loss_ax.set_ylabel('Loss')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XghWEvHaRSc3"
      },
      "source": [
        "#### ¿Que comportamiento observa en función de la cantidad de capas (profundidad) de las redes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJTU8YNXae8"
      },
      "source": [
        "conclusion_performance_vs_deep = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVEeHhuaXvy7"
      },
      "source": [
        "#### ¿Cual considera que es la \"mejor\" cantidad de capas ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffHKu4zEY12L"
      },
      "source": [
        "mejor_cantidad_capas = ?\n",
        "print('El mejor modelo es el que tiene {} capas'.format(mejor_cantidad_capas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXcRIzuhRRtc"
      },
      "source": [
        "# elegimos el modelo que tiene la mejor cantidad de capas\n",
        "best_model_index = n_layers.index(mejor_cantidad_capas)\n",
        "best_model = models[best_model_index]\n",
        "best_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjK1I53YysZI"
      },
      "source": [
        "## PARTE III : Conclusiones finales\n",
        "\n",
        "Habiendo realizado este laboratorio complete las consignas a continuación. Note que no hay respuestas correctas, esta sección pretende servir para hacer una auto-reflexión y revisión de los conceptos vistos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHEUb9NvzKax"
      },
      "source": [
        "#### (1) Menciona 3 cosas que aprendiste con este laboratorio \n",
        "\n",
        "_Complete en esta celda sin limite de palabras ni formato_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0kdGM_6zhMs"
      },
      "source": [
        "#### (2) Menciona 3 cosas que aún no entendés del todo y cual es tu intuición sobre ellas \n",
        "\n",
        "_Complete en esta celda sin limite de palabras ni formato_\n",
        "\n",
        "\n",
        "_Si querés te invitamos a investigar más o preguntar para aclarar estos conceptos_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcwasnIQz1I1"
      },
      "source": [
        "#### (3) Menciona 3 ideas / cosas que harías a partír de aquí para mejorar los resultados\n",
        "\n",
        "_Complete en esta celda sin limite de palabras ni formato_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqvq3h9hFAmI"
      },
      "source": [
        "## Appendice : Usando la red neuronal para predecir resultados de test\n",
        "Ahora que tenemos un modelo base para trabajar, vamos a tratar de utilizarlo en la práctica para hacer algunas predicciones y ver como se comporta.\n",
        "\n",
        "En primer lugar obtenemos una muestra de ejemplos positivos para ver como los evalua este modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r96OsaBSdjbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "8cfa0e8a-05a6-4e0e-c478-0a98e98630eb"
      },
      "source": [
        "positive_test = test_df.loc[test_df[target_variable] == 'positive']\n",
        "print(positive_test.shape)\n",
        "positive_test.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(53, 81)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alanine transaminase</th>\n",
              "      <th>Monocytes</th>\n",
              "      <th>Ferritin</th>\n",
              "      <th>Myelocytes</th>\n",
              "      <th>Sodium</th>\n",
              "      <th>Neutrophils</th>\n",
              "      <th>y</th>\n",
              "      <th>Mean platelet volume</th>\n",
              "      <th>Aspartate transaminase</th>\n",
              "      <th>Proteina C reativa mg/dL</th>\n",
              "      <th>pH (venous blood gas analysis)</th>\n",
              "      <th>Urine - Crystals</th>\n",
              "      <th>Urine - Protein</th>\n",
              "      <th>Alkaline phosphatase</th>\n",
              "      <th>Urine - Aspect</th>\n",
              "      <th>Urine - pH</th>\n",
              "      <th>Hemoglobin</th>\n",
              "      <th>Urine - Hyaline cylinders</th>\n",
              "      <th>ctO2 (arterial blood gas analysis)</th>\n",
              "      <th>Myeloblasts</th>\n",
              "      <th>International normalized ratio (INR)</th>\n",
              "      <th>Mean corpuscular hemoglobin concentration (MCHC)</th>\n",
              "      <th>Serum Glucose</th>\n",
              "      <th>Platelets</th>\n",
              "      <th>Hematocrit</th>\n",
              "      <th>Direct Bilirubin</th>\n",
              "      <th>Urine - Urobilinogen</th>\n",
              "      <th>Eosinophils</th>\n",
              "      <th>Strepto A</th>\n",
              "      <th>Hb saturation (arterial blood gases)</th>\n",
              "      <th>Urine - Red blood cells</th>\n",
              "      <th>Urine - Leukocytes</th>\n",
              "      <th>Leukocytes</th>\n",
              "      <th>SARS-Cov-2 exam result</th>\n",
              "      <th>Base excess (venous blood gas analysis)</th>\n",
              "      <th>Phosphor</th>\n",
              "      <th>HCO3 (venous blood gas analysis)</th>\n",
              "      <th>Gamma-glutamyltransferase</th>\n",
              "      <th>Potassium</th>\n",
              "      <th>Urine - Esterase</th>\n",
              "      <th>...</th>\n",
              "      <th>Urine - Yeasts</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Urine - Density</th>\n",
              "      <th>Creatinine</th>\n",
              "      <th>HCO3 (arterial blood gas analysis)</th>\n",
              "      <th>Mean corpuscular volume (MCV)</th>\n",
              "      <th>Arteiral Fio2</th>\n",
              "      <th>Lactic Dehydrogenase</th>\n",
              "      <th>Albumin</th>\n",
              "      <th>Total Bilirubin</th>\n",
              "      <th>pO2 (venous blood gas analysis)</th>\n",
              "      <th>Lymphocytes</th>\n",
              "      <th>Urine - Ketone Bodies</th>\n",
              "      <th>Rods #</th>\n",
              "      <th>Mean corpuscular hemoglobin (MCH)</th>\n",
              "      <th>Basophils</th>\n",
              "      <th>Lipase dosage</th>\n",
              "      <th>Arterial Lactic Acid</th>\n",
              "      <th>Segmented</th>\n",
              "      <th>Total CO2 (arterial blood gas analysis)</th>\n",
              "      <th>Urine - Hemoglobin</th>\n",
              "      <th>Urine - Color</th>\n",
              "      <th>Relationship (Patient/Normal)</th>\n",
              "      <th>Urine - Bile pigments</th>\n",
              "      <th>Total CO2 (venous blood gas analysis)</th>\n",
              "      <th>Urea</th>\n",
              "      <th>Indirect Bilirubin</th>\n",
              "      <th>Hb saturation (venous blood gas analysis)</th>\n",
              "      <th>pO2 (arterial blood gas analysis)</th>\n",
              "      <th>Metamyelocytes</th>\n",
              "      <th>Creatine phosphokinase (CPK)</th>\n",
              "      <th>Base excess (arterial blood gas analysis)</th>\n",
              "      <th>Red blood cell distribution width (RDW)</th>\n",
              "      <th>Red blood Cells</th>\n",
              "      <th>Promyelocytes</th>\n",
              "      <th>pCO2 (arterial blood gas analysis)</th>\n",
              "      <th>Ionized calcium</th>\n",
              "      <th>Urine - Granular cylinders</th>\n",
              "      <th>pH (arterial blood gas analysis)</th>\n",
              "      <th>Patient age quantile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5639</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5560</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2241</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3471</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2993</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.145443</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.537138</td>\n",
              "      <td>1</td>\n",
              "      <td>0.010677</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.105468</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.74204</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.517413</td>\n",
              "      <td>0.96895</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.203417</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.417414</td>\n",
              "      <td>positive</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.074163</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.292779</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.230447</td>\n",
              "      <td>-0.223767</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.524862</td>\n",
              "      <td>0.87774</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Alanine transaminase  ...  Patient age quantile\n",
              "5639                   NaN  ...                     3\n",
              "5560                   NaN  ...                     9\n",
              "2241                   NaN  ...                    14\n",
              "3471                   NaN  ...                     7\n",
              "2993                   NaN  ...                     4\n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJXkoGowWemX"
      },
      "source": [
        "A partir del dataframe de ejemplos positivos necesitamos obtener los datapoints en el formato esperado por el modelo. Para ello implementamos la función `get_datapoints_sample_model` .\n",
        "\n",
        "#### Complete el código en los signos de interrogación para implementar esa función"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkV9Z5rJhM94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "6921b29b-7bff-4cce-9c75-fd4437cb04f5"
      },
      "source": [
        "\n",
        "'''\n",
        "  Función para transformar un dataframe tomado de un dataset similar al usado para entrenar y generar un arreglo de datapoints para predecir\n",
        "'''\n",
        "def get_datapoints_sample_model(\n",
        "    df, # dataframe de entrada\n",
        "    scaler_numeric, # objeto Scaler usado para generar el trainset\n",
        "    numeric_vars, # columnas de variables numericas en el dataframe\n",
        "    imputer # imputer object\n",
        "    ):\n",
        "  \n",
        "  # obtenenmos las matrices resultado de transofrmar el dataframe a todo numérico\n",
        "  _numeric,_y = ?\n",
        "\n",
        "  # aplicamos el preprocesamiento anterior\n",
        "  _numeric = ?.transform(_numeric)\n",
        "  _numeric = ?.transform(_numeric)\n",
        "  list_input_matrix = [_numeric]\n",
        "  \n",
        "  \n",
        "  # bind all the X_ matrix in one\n",
        "  _datapoints = np.hstack(tuple(list_input_matrix ))\n",
        "  return _datapoints\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-c66cdf62cba1>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    _numeric,_y = ?\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qeCR782huU4"
      },
      "source": [
        "Usando la función anterior obtenemos la matriz de `datapoints` la cual es pasada a la función `predict_classes` del mejor modelo para predecir sobre ellos y obtener un valor `1` si el modelo cree que el ejemplo es COVID positivo y un valor `0` si cree que es negativo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f4v2JHHIedt"
      },
      "source": [
        "datapoints = get_datapoints_sample_model(positive_test,\n",
        "                                         scaler_numeric,\n",
        "                                         numeric_vars,\n",
        "                                         imputer)\n",
        "print(datapoints.shape)\n",
        "postive_datapoints_predictions = best_model.predict_classes(datapoints)[:,0]\n",
        "print('Resultados de predicciones sobre una muestra de casos positivos: \\n{}'.format(postive_datapoints_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OculZTenZngQ"
      },
      "source": [
        "#### ¿Cuantos de los ejemplos se predijeron correctamente como positivos?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9COx6YcYiNLK"
      },
      "source": [
        "n_true_positives = ?\n",
        "print('{} de los {} ejemplos positivos se predijeron correctamente como tales'.format(n_true_positives, datapoints.shape[0])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk9tBxl0nziO"
      },
      "source": [
        "De la matriz `datapoints` vamos a extraér solamente los casos que se predijeron correctamente para analizar los mismos que es donde el modelo es bueno. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TanwOrVgAzqu"
      },
      "source": [
        "true_poistives_index = np.where(postive_datapoints_predictions == 1)\n",
        "true_positives_datapoints = datapoints[true_poistives_index,:][0]\n",
        "print(true_positives_datapoints.shape)\n",
        "true_positives_datapoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc7i3K-ZjAHq"
      },
      "source": [
        "A continuación usaremos el modulo [SHAP](https://github.com/slundberg/shap) para entender mejor las predicciones del modelo y ver cuales son las variables que más influencian el valor de la neurona de salida. \n",
        "\n",
        "Usaremos el objeto `KernelExplainer` para obtener los _SHAP Values_ sobre la matriz de datapoints. Sin entrar en detalles, estos valores nos dicen en promedio cuanto movió, positiva o negativamente el valor de la neurona de salida de un valor base (predicción promedio sobre el trainset). [Lectura recomendada para más detalle](https://christophm.github.io/interpretable-ml-book/shapley.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi2h279kCyJb"
      },
      "source": [
        "!pip install shap\n",
        "import shap \n",
        "\n",
        "# use Kernel SHAP to explain test set predictions\n",
        "explainer = shap.KernelExplainer(best_model.predict, shap.sample(X_train,100))\n",
        "shap_values_tp = explainer.shap_values(true_positives_datapoints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz_f86pikxi0"
      },
      "source": [
        "Usando los `shap_values` podemos graficarlos en un [`summary_plot`](https://christophm.github.io/interpretable-ml-book/shap.html#shap-summary-plot) donde vemos a la izquierda las variables que más influencian las predicciones y en linea con el nombre de la variable se ubica un punto por cada ejemplo donde los puntos más hacia la derecha han movido la prediccion positivamente y aquellos más a la izquierda la han movido negativamente. Además los puntos tienen un color que corresponde con el valor que tuvo ese ejemplo en la variable en cuestión. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilLPGM4rDvcw"
      },
      "source": [
        "feature_names = numeric_vars\n",
        "shap.summary_plot(shap_values_tp[0], true_positives_datapoints,feature_names=feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pVIU2_2YnEX"
      },
      "source": [
        "Analizamos ahora como se comporta para algunos casos negativos repitiendo el proceso anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OugbGWDqKaZT"
      },
      "source": [
        "negative_test = test_df.loc[test_df[target_variable] == 'negative']\n",
        "print(negative_test.shape)\n",
        "negative_test.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEkegY6uSF5r"
      },
      "source": [
        "datapoints = get_datapoints_sample_model(negative_test,\n",
        "                                         scaler_numeric,\n",
        "                                         numeric_vars,\n",
        "                                         imputer)\n",
        "\n",
        "print(datapoints.shape)\n",
        "negative_datapoints_predictions = best_model.predict_classes(datapoints)[:,0]\n",
        "print('Resultados de predicciones sobre una muestra de casos negativos: \\n{}'.format(negative_datapoints_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb8H7u-uxZgy"
      },
      "source": [
        "#### ¿Cuantos de los ejemplos se predijeron correctamente como `negativos`?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n22zUyVZxZg0"
      },
      "source": [
        "n_true_negatives = ?\n",
        "print('{} de los {} ejemplos negativos se predijeron correctamente como tales'.format(n_true_negatives,datapoints.shape[0])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl3NzcVumndA"
      },
      "source": [
        "true_negatives_index = np.where(negative_datapoints_predictions == 0)\n",
        "true_negatives_datapoints = datapoints[true_negatives_index,:][0]\n",
        "print(true_negatives_datapoints.shape)\n",
        "true_negatives_datapoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUw5SMHLn4fZ"
      },
      "source": [
        "# use Kernel SHAP to explain test set predictions\n",
        "shap_values_tn = explainer.shap_values(true_negatives_datapoints)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C_8kJ_GpQAr"
      },
      "source": [
        "feature_names = numeric_vars\n",
        "shap.summary_plot(shap_values_tn[0], true_negatives_datapoints,feature_names=feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h54lT_lacnu"
      },
      "source": [
        "#### Considerando estas muestras, ¿cual es la `precision`, `recall` y `accuracy`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJkwOUFBx8-B"
      },
      "source": [
        "precision_predictions = ?\n",
        "\n",
        "print('La precisión estimada para esta muestra es de {} %'.format(precision_predictions))\n",
        "\n",
        "recall_predictions = ?\n",
        "\n",
        "print('La recall estimada para esta muestra es de {} %'.format(recall_predictions))\n",
        "\n",
        "accuracy_predictions = ?\n",
        "\n",
        "print('La accuracy estimada para esta muestra es de {} %'.format(accuracy_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}